{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989d16ed-a86a-468b-bf49-4a8211c6a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-13 03:33:55] \n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:55] BASKETBALL SCRAPER STARTED\n",
      "[2025-10-13 03:33:55] Time: 2025-10-13 03:33:55\n",
      "[2025-10-13 03:33:55] Mode: QUICK\n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:55] Found 6 active leagues\n",
      "[2025-10-13 03:33:55] \n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:55] PROCESSING LEAGUE: 1 - ×œ×™×’×” ×œ××•××™×ª\n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:56] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:33:56]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:33:56] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:33:56] [LEUMIT] STARTING SCRAPE: ×œ×™×’×” ×œ××•××™×ª\n",
      "[2025-10-13 03:33:56] [LEUMIT] Mode: QUICK\n",
      "[2025-10-13 03:33:56] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:33:56] [LEUMIT] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:33:57] [LEUMIT] Found 176 players\n",
      "[2025-10-13 03:33:58]    âš ï¸  No mapping found for: '×”×¤×•×¢×œ ×’×œ×™×œ ×¢×œ×™×•×Ÿ' in league 1\n",
      "[2025-10-13 03:33:58]    âš ï¸  No mapping found for: '××›×‘×™ ×¨××ª ×’×Ÿ ×§×‘×•×¦×ª ×›× ' in league 1\n",
      "[2025-10-13 03:33:58]    âš ï¸  No mapping found for: '×”×¤×•×¢×œ ×™×¨×•×©×œ×™× ×‘.×™×”×‘' in league 1\n",
      "[2025-10-13 03:33:58] [LEUMIT] âœ… Player details updated\n",
      "[2025-10-13 03:33:58] [LEUMIT]    Total: 176 | New: 0 | Updated: 0 | Skipped: 176\n",
      "[2025-10-13 03:33:58] [LEUMIT] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:01] [LEUMIT] âœ… Downloaded schedule: 240 games\n",
      "[2025-10-13 03:34:01] [LEUMIT]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:01] [LEUMIT]    âœ… Normalized 240 games\n",
      "[2025-10-13 03:34:01] [LEUMIT] âœ… Games schedule normalized and saved\n",
      "[2025-10-13 03:34:01] [LEUMIT]    Found 6 completed games\n",
      "[2025-10-13 03:34:01] [LEUMIT]    Scraping 6 new games\n",
      "[2025-10-13 03:34:01] [LEUMIT]    [1/6] Game 1_741605: ××œ×™×¦×•×¨ ×™×‘× ×” vs ××›×‘×™ ×—×™×¤×”\n",
      "[2025-10-13 03:34:03] [LEUMIT]       Final Score: ××œ×™×¦×•×¨ ×™×‘× ×” 87 - 76 ××›×‘×™ ×—×™×¤×”\n",
      "[2025-10-13 03:34:04] [LEUMIT]       DEBUG: final_scores = {'home_team': '××œ×™×¦×•×¨ ×™×‘× ×”', 'away_team': '××›×‘×™ ×—×™×¤×”', 'home_score': 87, 'away_score': 76}\n",
      "[2025-10-13 03:34:05] [LEUMIT]    [2/6] Game 1_741600: ×”×¤×•×¢×œ × ×”×¨×™×” vs ×.×¡ ×¨××”\"×©\n",
      "[2025-10-13 03:34:06] [LEUMIT]       DEBUG: final_scores = {}\n",
      "[2025-10-13 03:34:06] [LEUMIT]    âš ï¸  No stats found for game 1_741600\n",
      "[2025-10-13 03:34:07] [LEUMIT]    [3/6] Game 1_741602: ×”×¤×•×¢×œ ×—×™×¤×” vs ××œ×™×¦×•×¨ ×©×•××¨×•×Ÿ\n",
      "[2025-10-13 03:34:09] [LEUMIT]       Final Score: ×”×¤×•×¢×œ ×—×™×¤×” 79 - 74 ××œ×™×¦×•×¨ ×©×•××¨×•×Ÿ\n",
      "[2025-10-13 03:34:09] [LEUMIT]       DEBUG: final_scores = {'home_team': '×”×¤×•×¢×œ ×—×™×¤×”', 'away_team': '××œ×™×¦×•×¨ ×©×•××¨×•×Ÿ', 'home_score': 79, 'away_score': 74}\n",
      "[2025-10-13 03:34:10] [LEUMIT]    [4/6] Game 1_741606: ××›×‘×™ ××©×“×•×“ vs ×.×¡. ×¢×™×¨×•× ×™ ××©×§×œ×•×Ÿ\n",
      "[2025-10-13 03:34:12] [LEUMIT]       Final Score: ××›×‘×™ ××©×“×•×“ 92 - 88 ×.×¡. ×¢×™×¨×•× ×™ ××©×§×œ×•×Ÿ\n",
      "[2025-10-13 03:34:12] [LEUMIT]       DEBUG: final_scores = {'home_team': '××›×‘×™ ××©×“×•×“', 'away_team': '×.×¡. ×¢×™×¨×•× ×™ ××©×§×œ×•×Ÿ', 'home_score': 92, 'away_score': 88}\n",
      "[2025-10-13 03:34:13] [LEUMIT]    [5/6] Game 1_741607: ×”×¤×•×¢×œ ××’×“×œ ×”×¢××§/×™×–×¨×¢××œ vs ×.×¡ ×¦×¤×ª\n",
      "[2025-10-13 03:34:15] [LEUMIT]       Final Score: ×”×¤×•×¢×œ ××’×“×œ ×”×¢××§/×™×–×¨×¢××œ 90 - 80 ×.×¡ ×¦×¤×ª\n",
      "[2025-10-13 03:34:15] [LEUMIT]       DEBUG: final_scores = {'home_team': '×”×¤×•×¢×œ ××’×“×œ ×”×¢××§/×™×–×¨×¢××œ', 'away_team': '×.×¡ ×¦×¤×ª', 'home_score': 90, 'away_score': 80}\n",
      "[2025-10-13 03:34:17] [LEUMIT]    [6/6] Game 1_741601: ×.×› ×¢×•×˜×£ ×“×¨×•× vs ××›×‘×™ ×§×¨×™×ª ×’×ª\n",
      "[2025-10-13 03:34:21] [LEUMIT]       Final Score: ×.×› ×¢×•×˜×£ ×“×¨×•× 94 - 87 ××›×‘×™ ×§×¨×™×ª ×’×ª\n",
      "[2025-10-13 03:34:21] [LEUMIT]       DEBUG: final_scores = {'home_team': '×.×› ×¢×•×˜×£ ×“×¨×•×', 'away_team': '××›×‘×™ ×§×¨×™×ª ×’×ª', 'home_score': 94, 'away_score': 87}\n",
      "[2025-10-13 03:34:22] [LEUMIT]    âœ… Saved quarters: 40 records\n",
      "[2025-10-13 03:34:22] [LEUMIT]    âœ… Saved player stats: 96 records\n",
      "[2025-10-13 03:34:22] [LEUMIT]    âœ… Saved team stats: 10 records\n",
      "[2025-10-13 03:34:22] [LEUMIT]    DEBUG: About to update 5 scores\n",
      "[2025-10-13 03:34:22] [LEUMIT]    Updating game scores in schedule...\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: ××œ×™×¦×•×¨ ×™×‘× ×” 87-76 ××›×‘×™ ×—×™×¤×”\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: ×”×¤×•×¢×œ ×—×™×¤×” 79-74 ××œ×™×¦×•×¨ ×©×•××¨×•×Ÿ\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: ××›×‘×™ ××©×“×•×“ 92-88 ×.×¡. ×¢×™×¨×•× ×™ ××©×§×œ×•×Ÿ\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: ×”×¤×•×¢×œ ××’×“×œ ×”×¢××§/×™×–×¨×¢××œ 90-80 ×.×¡ ×¦×¤×ª\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: ×.×› ×¢×•×˜×£ ×“×¨×•× 94-87 ××›×‘×™ ×§×¨×™×ª ×’×ª\n",
      "[2025-10-13 03:34:22] [LEUMIT]    âœ… Updated 5 game scores in schedule\n",
      "[2025-10-13 03:34:22] [LEUMIT] âœ… Game stats updated: 6 new games\n",
      "[2025-10-13 03:34:22] [LEUMIT] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:22] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:22]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:22] [LEUMIT] âœ… Player averages: 96 players\n",
      "[2025-10-13 03:34:23] [LEUMIT] âœ… Team averages: 10 teams\n",
      "[2025-10-13 03:34:23] [LEUMIT] âœ… Opponent averages: 10 teams\n",
      "[2025-10-13 03:34:23] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:34:23] [LEUMIT] âœ… SCRAPE COMPLETED SUCCESSFULLY\n",
      "[2025-10-13 03:34:23] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:34:23] \n",
      "[2025-10-13 03:34:23] ================================================================================\n",
      "[2025-10-13 03:34:23] PROCESSING LEAGUE: 4 - × ×•×¢×¨ ×¢×œ ×¦×¤×•×Ÿ\n",
      "[2025-10-13 03:34:23] ================================================================================\n",
      "[2025-10-13 03:34:23] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:23]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:23] [U18-NORTH] ============================================================\n",
      "[2025-10-13 03:34:23] [U18-NORTH] STARTING SCRAPE: × ×•×¢×¨ ×¢×œ ×¦×¤×•×Ÿ\n",
      "[2025-10-13 03:34:23] [U18-NORTH] Mode: QUICK\n",
      "[2025-10-13 03:34:23] [U18-NORTH] ============================================================\n",
      "[2025-10-13 03:34:23] [U18-NORTH] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:24] [U18-NORTH] Found 152 players\n",
      "[2025-10-13 03:34:24] [U18-NORTH] âœ… Player details updated\n",
      "[2025-10-13 03:34:24] [U18-NORTH]    Total: 152 | New: 0 | Updated: 0 | Skipped: 152\n",
      "[2025-10-13 03:34:24] [U18-NORTH] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:27] [U18-NORTH] âœ… Downloaded schedule: 66 games\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    âœ… Normalized 66 games\n",
      "[2025-10-13 03:34:27] [U18-NORTH] âœ… Games schedule normalized and saved\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    Found 0 completed games\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    âœ… All games already scraped\n",
      "[2025-10-13 03:34:27] [U18-NORTH] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:27] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:27]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:27] [U18-NORTH] âŒ No player stats found\n",
      "[2025-10-13 03:34:27] \n",
      "[2025-10-13 03:34:27] ================================================================================\n",
      "[2025-10-13 03:34:27] PROCESSING LEAGUE: 5 - × ×•×¢×¨ ×¢×œ ×“×¨×•×\n",
      "[2025-10-13 03:34:27] ================================================================================\n",
      "[2025-10-13 03:34:27] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:27]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] ============================================================\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] STARTING SCRAPE: × ×•×¢×¨ ×¢×œ ×“×¨×•×\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] Mode: QUICK\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] ============================================================\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:29] [U18-SOUTH] Found 136 players\n",
      "[2025-10-13 03:34:29]    âš ï¸  No mapping found for: '×”×¤×•×¢×œ ××©×›×•×œ' in league 5\n",
      "[2025-10-13 03:34:29]    âš ï¸  No mapping found for: '××›×‘×™ ×¤\"×ª ××¨×§×“×™' in league 5\n",
      "[2025-10-13 03:34:29] [U18-SOUTH] âœ… Player details updated\n",
      "[2025-10-13 03:34:29] [U18-SOUTH]    Total: 136 | New: 0 | Updated: 0 | Skipped: 136\n",
      "[2025-10-13 03:34:29] [U18-SOUTH] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] âœ… Downloaded schedule: 66 games\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    âœ… Normalized 66 games\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] âœ… Games schedule normalized and saved\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    Found 0 completed games\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    âœ… All games already scraped\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:32] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:32]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] âŒ No player stats found\n",
      "[2025-10-13 03:34:32] \n",
      "[2025-10-13 03:34:32] ================================================================================\n",
      "[2025-10-13 03:34:32] PROCESSING LEAGUE: 8 - ×œ×™×’×” ×œ××•××™×ª × ×©×™×\n",
      "[2025-10-13 03:34:32] ================================================================================\n",
      "[2025-10-13 03:34:32] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:32]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] STARTING SCRAPE: ×œ×™×’×” ×œ××•××™×ª × ×©×™×\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] Mode: QUICK\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN] Found 98 players\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN] âœ… Player details updated\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN]    Total: 98 | New: 0 | Updated: 0 | Skipped: 98\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] âœ… Downloaded schedule: 105 games\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    âœ… Normalized 105 games\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] âœ… Games schedule normalized and saved\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    Found 0 completed games\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    âœ… All games already scraped\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:39] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:39]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] âŒ No player stats found\n",
      "[2025-10-13 03:34:39] \n",
      "[2025-10-13 03:34:39] ================================================================================\n",
      "[2025-10-13 03:34:39] PROCESSING LEAGUE: 9 - × ×¢×¨×•×ª ×' ×¢×œ\n",
      "[2025-10-13 03:34:39] ================================================================================\n",
      "[2025-10-13 03:34:39] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:39]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] STARTING SCRAPE: × ×¢×¨×•×ª ×' ×¢×œ\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] Mode: QUICK\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:42] [U18-WOMEN] Found 99 players\n",
      "[2025-10-13 03:34:43] [U18-WOMEN] âœ… Player details updated\n",
      "[2025-10-13 03:34:43] [U18-WOMEN]    Total: 99 | New: 0 | Updated: 0 | Skipped: 99\n",
      "[2025-10-13 03:34:43] [U18-WOMEN] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] âœ… Downloaded schedule: 91 games\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    âœ… Normalized 91 games\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] âœ… Games schedule normalized and saved\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    Found 0 completed games\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    âœ… All games already scraped\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:46] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:46]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] âŒ No player stats found\n",
      "[2025-10-13 03:34:46] \n",
      "[2025-10-13 03:34:46] ================================================================================\n",
      "[2025-10-13 03:34:46] PROCESSING LEAGUE: 11 - ×œ×™×’×ª ×”×¢×œ ×œ× ×©×™×\n",
      "[2025-10-13 03:34:46] ================================================================================\n",
      "[2025-10-13 03:34:46] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:46]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] STARTING SCRAPE: ×œ×™×’×ª ×”×¢×œ ×œ× ×©×™×\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] Mode: QUICK\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:49] [WOMEN-PL] Found 122 players\n",
      "[2025-10-13 03:34:49]    âš ï¸  No mapping found for: '××œ×™×¦×•×¨ ×ª×œ ××‘×™×‘' in league 11\n",
      "[2025-10-13 03:34:49] [WOMEN-PL] âœ… Player details updated\n",
      "[2025-10-13 03:34:49] [WOMEN-PL]    Total: 122 | New: 0 | Updated: 0 | Skipped: 122\n",
      "[2025-10-13 03:34:49] [WOMEN-PL] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] âœ… Downloaded schedule: 90 games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    âœ… Normalized 90 games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] âœ… Games schedule normalized and saved\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    Found 8 completed games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    Already scraped: 8 games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    âœ… All games already scraped\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:52] âœ… Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:52]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] âœ… Player averages: 87 players\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] âœ… Team averages: 10 teams\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] âœ… Opponent averages: 10 teams\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] âœ… SCRAPE COMPLETED SUCCESSFULLY\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:53] \n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] UPDATING GLOBAL FILES\n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] Updating global leagues.csv...\n",
      "[2025-10-13 03:34:53] âœ… Global leagues.csv updated: 11 leagues\n",
      "[2025-10-13 03:34:53] Checking global teams.csv...\n",
      "[2025-10-13 03:34:53] âœ… Global teams.csv verified: 81 teams\n",
      "[2025-10-13 03:34:53] Updating global players.csv...\n",
      "[2025-10-13 03:34:53] âœ… Global players.csv updated: 783 players\n",
      "[2025-10-13 03:34:53] \n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] SCRAPING SUMMARY\n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] âœ… Successful: 2 leagues\n",
      "[2025-10-13 03:34:53]    âœ“ League 1: ×œ×™×’×” ×œ××•××™×ª\n",
      "[2025-10-13 03:34:53]    âœ“ League 11: ×œ×™×’×ª ×”×¢×œ ×œ× ×©×™×\n",
      "[2025-10-13 03:34:53] âŒ Failed: 4 leagues\n",
      "[2025-10-13 03:34:53]    âœ— League 4: × ×•×¢×¨ ×¢×œ ×¦×¤×•×Ÿ\n",
      "[2025-10-13 03:34:53]    âœ— League 5: × ×•×¢×¨ ×¢×œ ×“×¨×•×\n",
      "[2025-10-13 03:34:53]    âœ— League 8: ×œ×™×’×” ×œ××•××™×ª × ×©×™×\n",
      "[2025-10-13 03:34:53]    âœ— League 9: × ×¢×¨×•×ª ×' ×¢×œ\n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] \n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] BASKETBALL SCRAPER FINISHED\n",
      "[2025-10-13 03:34:53] Time: 2025-10-13 03:34:53\n",
      "[2025-10-13 03:34:53] ================================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['main.py']\n",
    "from main import main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd8431-5e94-405a-9e5c-428464ec79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Print a filtered tree: folders + CSV files only, plus a flat CSV list with sizes.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Settings ----------\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5          # ×¢×•××§ ××§×¡×™××œ×™ ×œ×¡×¨×™×§×”\n",
    "SHOW_HIDDEN = False    # ×œ×”×¦×™×’ ×ª×™×§×™×•×ª/×§×‘×¦×™× ×©××ª×—×™×œ×™× ×‘× ×§×•×“×”?\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_hidden(p: Path) -> bool:\n",
    "    return any(part.startswith(\".\") for part in p.parts)\n",
    "\n",
    "def within_depth(root: Path, p: Path, max_depth: int) -> bool:\n",
    "    return len(p.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def dir_children(root: Path, d: Path):\n",
    "    \"\"\"Return (dirs, csvs) immediate children (not recursive), filtered.\"\"\"\n",
    "    dirs, csvs = [], []\n",
    "    for child in sorted(d.iterdir()):\n",
    "        if not SHOW_HIDDEN and is_hidden(child):\n",
    "            continue\n",
    "        if child.is_dir():\n",
    "            dirs.append(child)\n",
    "        elif child.is_file() and child.suffix.lower() == \".csv\":\n",
    "            csvs.append(child)\n",
    "    return dirs, csvs\n",
    "\n",
    "def contains_relevant_descendants(root: Path, d: Path, max_depth: int) -> bool:\n",
    "    \"\"\"Does this directory (recursively) contain any dir/csv within max_depth?\"\"\"\n",
    "    for p in d.rglob(\"*\"):\n",
    "        # stop at max depth\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p):\n",
    "            continue\n",
    "        if p.is_dir() or (p.is_file() and p.suffix.lower() == \".csv\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def print_tree(root: Path, max_depth: int):\n",
    "    print(f\"ROOT: {root}\\n\")\n",
    "    # We print the root name then recurse\n",
    "    print(f\"ğŸ“ {root.name}\")\n",
    "    def recurse(d: Path, prefix: str, depth: int):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        dirs, csvs = dir_children(root, d)\n",
    "\n",
    "        # Filter out directories that do not contain any relevant descendants within depth\n",
    "        pruned_dirs = []\n",
    "        for sd in dirs:\n",
    "            if within_depth(root, sd, max_depth) and (contains_relevant_descendants(root, sd, max_depth) or list(dir_children(root, sd)[1])):\n",
    "                pruned_dirs.append(sd)\n",
    "\n",
    "        # Build combined list: dirs first, then CSVs\n",
    "        items = pruned_dirs + csvs\n",
    "        for i, item in enumerate(items):\n",
    "            is_last = (i == len(items) - 1)\n",
    "            branch = \"â””â”€ \" if is_last else \"â”œâ”€ \"\n",
    "            if item.is_dir():\n",
    "                print(prefix + branch + \"ğŸ“ \" + item.name)\n",
    "                new_prefix = prefix + (\"   \" if is_last else \"â”‚  \")\n",
    "                recurse(item, new_prefix, depth + 1)\n",
    "            else:\n",
    "                print(prefix + branch + \"ğŸ“„ \" + item.name)\n",
    "    recurse(root, \"\", 0)\n",
    "\n",
    "def human_size(num_bytes: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if num_bytes < 1024.0:\n",
    "            return f\"{num_bytes:.0f} {unit}\"\n",
    "        num_bytes /= 1024.0\n",
    "    return f\"{num_bytes:.0f} PB\"\n",
    "\n",
    "def list_csvs(root: Path, max_depth: int):\n",
    "    csvs = []\n",
    "    for p in sorted(root.rglob(\"*.csv\")):\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p.relative_to(root)):\n",
    "            continue\n",
    "        if p.is_file():\n",
    "            try:\n",
    "                size = human_size(p.stat().st_size)\n",
    "            except Exception:\n",
    "                size = \"?\"\n",
    "            csvs.append((p, size))\n",
    "    return csvs\n",
    "\n",
    "# ---------- Run ----------\n",
    "print(\"=== DIRECTORY TREE (folders + CSV only) ===\")\n",
    "print_tree(ROOT, MAX_DEPTH)\n",
    "\n",
    "print(\"\\n=== CSV FILES (flat list) ===\")\n",
    "csv_list = list_csvs(ROOT, MAX_DEPTH)\n",
    "if not csv_list:\n",
    "    print(\"(no CSV files found)\")\n",
    "else:\n",
    "    for p, size in csv_list:\n",
    "        rel = p.relative_to(ROOT)\n",
    "        print(f\"- {rel}  â€¢  {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ef0d3-b0fc-4f69-bf27-3e1404521fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ××¦×™×’ ×œ×›×œ ×§×•×‘×¥ CSV ××ª ×”× ×ª×™×‘, ×›×•×ª×¨×•×ª ×•×©×•×¨×” ××—×ª ×¨××©×•× ×” ×‘×¤×•×¨××˜ CSV\n",
    "# ××“×œ×’ ×¢×œ ×ª×™×§×™×•×ª ××•×¡×ª×¨×•×ª ×•-.ipynb_checkpoints, ×¢× ××¤×¨×™×“ ×‘×¨×•×¨ ×‘×™×Ÿ ×§×‘×¦×™×\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "def is_hidden_or_checkpoint(path: Path) -> bool:\n",
    "    \"\"\"×‘×•×“×§ ×”×× ×§×•×‘×¥ ××• ×ª×™×§×™×™×” ××•×¡×ª×¨×™× ××• ×‘×ª×•×š .ipynb_checkpoints\"\"\"\n",
    "    return any(\n",
    "        part.startswith(\".\") or \"ipynb_checkpoints\" in part\n",
    "        for part in path.parts\n",
    "    )\n",
    "\n",
    "def within_depth(root: Path, path: Path, max_depth: int) -> bool:\n",
    "    \"\"\"×‘×•×“×§ ×”×× ×”×§×•×‘×¥ ×‘×¢×•××§ ××•×ª×¨\"\"\"\n",
    "    return len(path.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def print_csv_previews(root: Path, max_depth: int):\n",
    "    csv_files = sorted(root.rglob(\"*.csv\"))\n",
    "    for csv_path in csv_files:\n",
    "        if is_hidden_or_checkpoint(csv_path.relative_to(root)):\n",
    "            continue\n",
    "        if not within_depth(root, csv_path, max_depth):\n",
    "            continue\n",
    "\n",
    "        rel_path = str(csv_path.relative_to(root))\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=\"utf-8-sig\", nrows=1)\n",
    "        except Exception:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding=\"utf-8\", nrows=1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n{rel_path}\")\n",
    "                print(f\"Error reading file: {e}\")\n",
    "                print(\"=\" * 60)\n",
    "                continue\n",
    "\n",
    "        print(f\"\\n{rel_path}\")  # ×›×•×ª×¨×ª ×¢× × ×ª×™×‘ ×”×§×•×‘×¥\n",
    "        print(\",\".join(df.columns))  # ×©×•×¨×ª ×›×•×ª×¨×•×ª\n",
    "        if not df.empty:\n",
    "            print(\",\".join(map(str, df.iloc[0].tolist())))  # ×©×•×¨×” ××—×ª ××”× ×ª×•× ×™×\n",
    "        print(\"=\" * 60)  # ××¤×¨×™×“ ×‘×™×Ÿ ×§×‘×¦×™×\n",
    "\n",
    "    print(\"\\nâœ… ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”.\")\n",
    "\n",
    "# ×”×¤×¢×œ×”\n",
    "print_csv_previews(ROOT, MAX_DEPTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3b1a8-1da3-4e21-a9f5-5d01beb98c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c709c0-3108-403e-bfbe-598ca510c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Test Live Mapping\n",
    "=================\n",
    "×‘×“×™×§×” ×¢× ×”×§×•×“ ×”××¢×•×“×›×Ÿ ×‘×“×™×•×§\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "teams_file = \"data/teams.csv\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Loading with COMPOSITE KEY: (variation, league_id)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "print(f\"âœ“ Loaded {len(df)} rows\\n\")\n",
    "\n",
    "# ×–×™×”×•×™ ×¢××•×“×•×ª\n",
    "columns = df.columns.tolist()\n",
    "column_mapping = {\n",
    "    'team_id': ['Team_ID', 'team_id', 'TeamID'],\n",
    "    'league_id': ['League_ID', 'league_id', 'LeagueID'],\n",
    "    'club_name': ['Team_Name', 'club_name', 'team_name', 'name'],\n",
    "    'variations': ['name_variations', 'variations', 'Variations'],\n",
    "}\n",
    "\n",
    "actual_columns = {}\n",
    "for key, possible_names in column_mapping.items():\n",
    "    for name in possible_names:\n",
    "        if name in columns:\n",
    "            actual_columns[key] = name\n",
    "            break\n",
    "\n",
    "print(f\"Detected columns: {actual_columns}\\n\")\n",
    "\n",
    "# ×™×¦×™×¨×ª ××™×¤×•×™ ×¢× ××¤×ª×— ××•×¨×›×‘\n",
    "mapping = {}\n",
    "teams_count = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    team_id = row[actual_columns['team_id']] if 'team_id' in actual_columns else None\n",
    "    league_id = row[actual_columns['league_id']] if 'league_id' in actual_columns else None\n",
    "    club_name = row[actual_columns['club_name']] if 'club_name' in actual_columns else None\n",
    "    \n",
    "    if pd.isna(team_id) or pd.isna(league_id) or pd.isna(club_name):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        league_id_int = int(league_id)\n",
    "        if league_id_int == 0:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    teams_count += 1\n",
    "    \n",
    "    variations = row[actual_columns['variations']] if 'variations' in actual_columns else club_name\n",
    "    \n",
    "    club_name = html.unescape(str(club_name)).strip()\n",
    "    variations_str = html.unescape(str(variations)).strip()\n",
    "    \n",
    "    variation_list = []\n",
    "    for v in variations_str.split('|'):\n",
    "        v_clean = html.unescape(v.strip())\n",
    "        if v_clean:\n",
    "            variation_list.append(v_clean)\n",
    "    \n",
    "    team_info = {\n",
    "        'team_id': int(team_id),\n",
    "        'league_id': int(league_id_int),\n",
    "        'club_name': club_name,\n",
    "        'short_name': club_name,\n",
    "        'bg_color': '#000000',\n",
    "        'text_color': '#FFFFFF',\n",
    "        'all_variations': variation_list\n",
    "    }\n",
    "    \n",
    "    # â­ ××¤×ª×— ××•×¨×›×‘!\n",
    "    for variation in variation_list:\n",
    "        if variation:\n",
    "            key = (variation, int(league_id_int))\n",
    "            if key not in mapping:\n",
    "                mapping[key] = team_info\n",
    "\n",
    "print(f\"âœ“ Total teams: {teams_count}\")\n",
    "print(f\"âœ“ Total mapping entries: {len(mapping)}\\n\")\n",
    "\n",
    "# ×‘×“×™×§×ª ×œ×™×’×” 1\n",
    "league_1_keys = [k for k in mapping.keys() if k[1] == 1]\n",
    "print(f\"âœ“ League 1 has {len(league_1_keys)} variations\\n\")\n",
    "\n",
    "if league_1_keys:\n",
    "    print(\"First 10 variations from League 1:\")\n",
    "    for i, key in enumerate(league_1_keys[:10]):\n",
    "        var, lid = key\n",
    "        info = mapping[key]\n",
    "        print(f\"  {i+1}. ('{var}', {lid}) â†’ team_id={info['team_id']}, club_name='{info['club_name']}'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing specific lookups:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    ('××œ×™×¦×•×¨ ×™×‘× ×”', 1),\n",
    "    ('××›×‘×™ ×—×™×¤×” ×’×™× × ×ª×Ÿ', 1),\n",
    "    ('××›×‘×™ ××©×“×•×“', 1),\n",
    "    ('×”×¤×•×¢×œ ×—×™×¤×”', 1),\n",
    "    ('××›×‘×™ ×¨×—×•×‘×•×ª', 1),\n",
    "]\n",
    "\n",
    "for team_name, league_id in test_cases:\n",
    "    key = (team_name, league_id)\n",
    "    if key in mapping:\n",
    "        info = mapping[key]\n",
    "        print(f\"âœ“ ({team_name!r}, {league_id}) â†’ team_id={info['team_id']}\")\n",
    "    else:\n",
    "        print(f\"âœ— ({team_name!r}, {league_id}) NOT FOUND\")\n",
    "        \n",
    "        # ×—×™×¤×•×© ×‘××™×–×• ×œ×™×’×” ×–×” ×›×Ÿ × ××¦×\n",
    "        found_in = []\n",
    "        for k in mapping.keys():\n",
    "            if k[0] == team_name:\n",
    "                found_in.append(k[1])\n",
    "        if found_in:\n",
    "            print(f\"  Found in leagues: {found_in}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92043d-21a5-48bf-906f-95a1b5cf04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clear Python Cache - Jupyter Notebook\n",
    "======================================\n",
    "××—×™×§×ª ×›×œ ×§×‘×¦×™ ×”-cache ×©×œ Python\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_python_cache(root_dir='.'):\n",
    "    \"\"\"\n",
    "    ××—×™×§×ª ×›×œ ×§×‘×¦×™ ×”-cache ×©×œ Python\n",
    "    \n",
    "    Args:\n",
    "        root_dir: ×ª×™×§×™×™×” ×œ×”×ª×—×™×œ ××× ×” (×‘×¨×™×¨×ª ××—×“×œ: × ×•×›×—×™×ª)\n",
    "    \"\"\"\n",
    "    deleted_count = 0\n",
    "    \n",
    "    # ××—×™×§×ª ×ª×™×§×™×•×ª __pycache__\n",
    "    print(\"ğŸ” ××—×¤×© ×ª×™×§×™×•×ª __pycache__...\")\n",
    "    for pycache_dir in Path(root_dir).rglob('__pycache__'):\n",
    "        try:\n",
    "            shutil.rmtree(pycache_dir)\n",
    "            print(f\"   âœ… × ××—×§: {pycache_dir}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ×©×’×™××” ×‘××—×™×§×ª {pycache_dir}: {e}\")\n",
    "    \n",
    "    # ××—×™×§×ª ×§×‘×¦×™ .pyc\n",
    "    print(\"\\nğŸ” ××—×¤×© ×§×‘×¦×™ .pyc...\")\n",
    "    pyc_count = 0\n",
    "    for pyc_file in Path(root_dir).rglob('*.pyc'):\n",
    "        try:\n",
    "            pyc_file.unlink()\n",
    "            pyc_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ×©×’×™××” ×‘××—×™×§×ª {pyc_file}: {e}\")\n",
    "    \n",
    "    if pyc_count > 0:\n",
    "        print(f\"   âœ… × ××—×§×• {pyc_count} ×§×‘×¦×™ .pyc\")\n",
    "    \n",
    "    # ××—×™×§×ª ×§×‘×¦×™ .pyo\n",
    "    print(\"\\nğŸ” ××—×¤×© ×§×‘×¦×™ .pyo...\")\n",
    "    pyo_count = 0\n",
    "    for pyo_file in Path(root_dir).rglob('*.pyo'):\n",
    "        try:\n",
    "            pyo_file.unlink()\n",
    "            pyo_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ×©×’×™××” ×‘××—×™×§×ª {pyo_file}: {e}\")\n",
    "    \n",
    "    if pyo_count > 0:\n",
    "        print(f\"   âœ… × ××—×§×• {pyo_count} ×§×‘×¦×™ .pyo\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"âœ… ×¡×™×™×! × ××—×§×• {deleted_count} ×ª×™×§×™×•×ª __pycache__\")\n",
    "    print(f\"âœ… × ××—×§×• {pyc_count + pyo_count} ×§×‘×¦×™× ××§×•××¤×œ×™×\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# ×”×¨×¦×”\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"××—×™×§×ª Python Cache\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "clear_python_cache()\n",
    "\n",
    "print(\"\\nğŸ’¡ ×›×¢×ª ×”×¨×¥ ××ª ×”×§×•×“ ××—×“×©:\")\n",
    "print(\"   !python main.py --league 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e8f3a-59de-4b0c-aef4-f0a5f77884be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8416ff-9a70-4069-b04d-7d6e19735db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56f7f97-6cb2-480f-8651-8441c12eef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--league LEAGUE] [--all] [--test]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\navon\\AppData\\Roaming\\jupyter\\runtime\\kernel-d5a1eede-d991-49d8-9026-3edb8d691127.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Migration Script - CSV to Supabase\n",
    "===================================\n",
    "××¢×‘×™×¨ ××ª ×›×œ ×”× ×ª×•× ×™× ×”×§×™×™××™× ×-CSV ×œ-Supabase\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from supabase_uploader import SupabaseUploader\n",
    "from config import LEAGUES\n",
    "from utils import log_message\n",
    "\n",
    "class DataMigration:\n",
    "    \"\"\"××—×œ×§×” ×œ×”×¢×‘×¨×ª × ×ª×•× ×™×\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"××ª×—×•×œ\"\"\"\n",
    "        self.uploader = SupabaseUploader()\n",
    "        self.stats = {\n",
    "            'leagues': 0,\n",
    "            'teams': 0,\n",
    "            'players': 0,\n",
    "            'history': 0,\n",
    "            'games': 0,\n",
    "            'quarters': 0,\n",
    "            'player_stats': 0,\n",
    "            'team_stats': 0,\n",
    "            'player_averages': 0,\n",
    "            'team_averages': 0,\n",
    "            'opponent_averages': 0\n",
    "        }\n",
    "    \n",
    "    def migrate_all(self, league_ids=None):\n",
    "        \"\"\"\n",
    "        ×”×¢×‘×¨×ª ×›×œ ×”× ×ª×•× ×™×\n",
    "        \n",
    "        Args:\n",
    "            league_ids: ×¨×©×™××ª league_ids ×œ×”×¢×‘×¨×” (None = ×›×•×œ×)\n",
    "        \"\"\"\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"ğŸš€ STARTING MIGRATION TO SUPABASE\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        # Test connection\n",
    "        if not self.uploader.test_connection():\n",
    "            log_message(\"âŒ Cannot connect to Supabase!\")\n",
    "            return False\n",
    "        \n",
    "        # Get leagues to migrate\n",
    "        if league_ids is None:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if cfg.get('active', False)}\n",
    "        else:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if lid in league_ids}\n",
    "        \n",
    "        log_message(f\"ğŸ“Š Migrating {len(leagues_to_migrate)} leagues\")\n",
    "        log_message(\"\")\n",
    "        \n",
    "        # Migrate global data first\n",
    "        self._migrate_global_leagues()\n",
    "        self._migrate_global_teams()\n",
    "        self._migrate_global_players()\n",
    "        \n",
    "        # Migrate each league\n",
    "        for league_id, config in leagues_to_migrate.items():\n",
    "            log_message(\"=\"*60)\n",
    "            log_message(f\"[{config['code'].upper()}] Migrating: {config['name']}\")\n",
    "            log_message(\"=\"*60)\n",
    "            \n",
    "            self._migrate_league_data(league_id, config)\n",
    "        \n",
    "        # Print stats\n",
    "        self._print_stats()\n",
    "        \n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"âœ… MIGRATION COMPLETED!\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _migrate_global_leagues(self):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×˜×‘×œ×ª leagues\"\"\"\n",
    "        log_message(\"ğŸ“‹ Migrating leagues...\")\n",
    "        \n",
    "        leagues_file = 'data/leagues.csv'\n",
    "        if not os.path.exists(leagues_file):\n",
    "            log_message(\"âš ï¸  leagues.csv not found, creating from config\")\n",
    "            # Create from config\n",
    "            for league_id, config in LEAGUES.items():\n",
    "                if config.get('active', False):\n",
    "                    league_data = {\n",
    "                        'league_id': int(league_id),\n",
    "                        'name': config['name'],\n",
    "                        'name_en': config.get('name_en', ''),\n",
    "                        'country': config.get('country', 'Israel'),\n",
    "                        'season': config.get('season', ''),\n",
    "                        'url': config['url'],\n",
    "                        'is_active': True\n",
    "                    }\n",
    "                    self.uploader.upsert_league(league_data)\n",
    "                    self.stats['leagues'] += 1\n",
    "        else:\n",
    "            df = pd.read_csv(leagues_file, encoding='utf-8-sig')\n",
    "            for _, row in df.iterrows():\n",
    "                league_data = {\n",
    "                    'league_id': int(row['league_id']),\n",
    "                    'name': row['name'],\n",
    "                    'name_en': row.get('name_en', ''),\n",
    "                    'country': row.get('country', 'Israel'),\n",
    "                    'season': row.get('season', ''),\n",
    "                    'url': row.get('url', ''),\n",
    "                    'is_active': bool(row.get('is_active', False))\n",
    "                }\n",
    "                self.uploader.upsert_league(league_data)\n",
    "                self.stats['leagues'] += 1\n",
    "        \n",
    "        log_message(f\"âœ… Migrated {self.stats['leagues']} leagues\")\n",
    "    \n",
    "    def _migrate_global_teams(self):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×˜×‘×œ×ª teams\"\"\"\n",
    "        log_message(\"ğŸ€ Migrating teams...\")\n",
    "        \n",
    "        teams_file = 'data/teams.csv'\n",
    "        if not os.path.exists(teams_file):\n",
    "            log_message(\"âš ï¸  teams.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "        teams_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Skip invalid teams\n",
    "            if pd.isna(row['Team_ID']) or pd.isna(row['League_ID']):\n",
    "                continue\n",
    "            if int(row['League_ID']) == 0:\n",
    "                continue\n",
    "            \n",
    "            teams_data.append({\n",
    "                'team_id': int(row['Team_ID']),\n",
    "                'league_id': int(row['League_ID']),\n",
    "                'team_name': row['Team_Name'],\n",
    "                'short_name': row.get('short_name', row['Team_Name']),\n",
    "                'bg_color': row.get('bg_color', '#000000'),\n",
    "                'text_color': row.get('text_color', '#FFFFFF'),\n",
    "                'name_variations': row.get('name_variations', '')\n",
    "            })\n",
    "        \n",
    "        if teams_data:\n",
    "            self.uploader.upsert_teams(teams_data)\n",
    "            self.stats['teams'] = len(teams_data)\n",
    "        \n",
    "        log_message(f\"âœ… Migrated {self.stats['teams']} teams\")\n",
    "    \n",
    "    def _migrate_global_players(self):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×˜×‘×œ×ª players\"\"\"\n",
    "        log_message(\"ğŸ‘¤ Migrating players...\")\n",
    "        \n",
    "        players_file = 'data/players.csv'\n",
    "        if not os.path.exists(players_file):\n",
    "            log_message(\"âš ï¸  players.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(players_file, encoding='utf-8-sig')\n",
    "        players_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            if pd.isna(row['player_id']):\n",
    "                continue\n",
    "            \n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['name'],\n",
    "                'current_team_id': int(row['current_team_id']) if pd.notna(row['current_team_id']) else None,\n",
    "                'league_id': int(row['league_id']),\n",
    "                'date_of_birth': row['date_of_birth'] if pd.notna(row['date_of_birth']) else None,\n",
    "                'height': float(row['height']) if pd.notna(row['height']) else None,\n",
    "                'jersey_number': int(row['jersey_number']) if pd.notna(row['jersey_number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            self.stats['players'] = len(players_data)\n",
    "        \n",
    "        log_message(f\"âœ… Migrated {self.stats['players']} players\")\n",
    "    \n",
    "    def _migrate_league_data(self, league_id, config):\n",
    "        \"\"\"×”×¢×‘×¨×ª × ×ª×•× ×™ ×œ×™×’×” ×¡×¤×¦×™×¤×™×ª\"\"\"\n",
    "        league_code = config['code']\n",
    "        data_folder = config['data_folder']\n",
    "        games_folder = config['games_folder']\n",
    "        \n",
    "        # Player details (if not in global file)\n",
    "        self._migrate_player_details(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Player history\n",
    "        self._migrate_player_history(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Games\n",
    "        self._migrate_games(league_id, games_folder)\n",
    "        \n",
    "        # Game quarters\n",
    "        self._migrate_game_quarters(league_id, games_folder)\n",
    "        \n",
    "        # Game player stats\n",
    "        self._migrate_game_player_stats(league_id, games_folder)\n",
    "        \n",
    "        # Game team stats\n",
    "        self._migrate_game_team_stats(league_id, games_folder)\n",
    "        \n",
    "        # Averages\n",
    "        self._migrate_player_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_team_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_opponent_averages(league_id, league_code, data_folder)\n",
    "    \n",
    "    def _migrate_player_details(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¤×¨×˜×™ ×©×—×§× ×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_details.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“ Player details...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        players_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['Name'],\n",
    "                'current_team_id': int(row['team_id']) if pd.notna(row['team_id']) else None,\n",
    "                'league_id': int(league_id),\n",
    "                'date_of_birth': row['Date Of Birth'] if pd.notna(row['Date Of Birth']) else None,\n",
    "                'height': float(row['Height']) if pd.notna(row['Height']) else None,\n",
    "                'jersey_number': int(row['Number']) if pd.notna(row['Number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            log_message(f\"  âœ… {len(players_data)} players\")\n",
    "    \n",
    "    def _migrate_player_history(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×”×™×¡×˜×•×¨×™×™×ª ×©×—×§× ×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_history.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“š Player history...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        history_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            player_id = str(row['player_id'])\n",
    "            \n",
    "            # Extract seasons from columns\n",
    "            for col in df.columns:\n",
    "                if col not in ['player_id', 'Name', 'Current Team', 'team_id', 'league_id',\n",
    "                               'Date Of Birth', 'Height', 'Number']:\n",
    "                    season = col\n",
    "                    if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                        team_league = str(row[col])\n",
    "                        team_name = team_league.split('(')[0].strip() if '(' in team_league else team_league\n",
    "                        league_name = team_league.split('(')[1].replace(')', '').strip() if '(' in team_league else ''\n",
    "                        \n",
    "                        history_data.append({\n",
    "                            'player_id': player_id,\n",
    "                            'season': season,\n",
    "                            'team_name': team_name,\n",
    "                            'league_name': league_name,\n",
    "                            'league_id': int(league_id)\n",
    "                        })\n",
    "        \n",
    "        if history_data:\n",
    "            self.uploader.upsert_player_history(history_data)\n",
    "            self.stats['history'] += len(history_data)\n",
    "            log_message(f\"  âœ… {len(history_data)} history records\")\n",
    "    \n",
    "    def _migrate_games(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ××©×—×§×™×\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'games_schedule.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ† Games...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        games_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            games_data.append({\n",
    "                'game_id': str(row['gameid']),\n",
    "                'league_id': int(league_id),\n",
    "                'code': str(row['Code']) if pd.notna(row.get('Code')) else None,\n",
    "                'week_day': row.get('Week Day', ''),\n",
    "                'date': row['Date'] if pd.notna(row['Date']) else None,\n",
    "                'round': str(row['Round']) if pd.notna(row.get('Round')) else None,\n",
    "                'time': row.get('Time', ''),\n",
    "                'home_team': row['Home Team'],\n",
    "                'home_team_code': row.get('Home Team Code', ''),\n",
    "                'home_team_id': int(row['home_team_id']) if pd.notna(row.get('home_team_id')) else None,\n",
    "                'away_team': row['Away Team'],\n",
    "                'away_team_code': row.get('Away Team Code', ''),\n",
    "                'away_team_id': int(row['away_team_id']) if pd.notna(row.get('away_team_id')) else None,\n",
    "                'venue': row.get('Venue', ''),\n",
    "                'home_score': int(row['Home Score']) if pd.notna(row.get('Home Score')) and str(row['Home Score']).replace('.','').isdigit() else None,\n",
    "                'away_score': int(row['Away Score']) if pd.notna(row.get('Away Score')) and str(row['Away Score']).replace('.','').isdigit() else None,\n",
    "                'arena': row.get('Arena', ''),\n",
    "                'status': 'completed' if pd.notna(row.get('Home Score')) and str(row.get('Home Score')).strip() != '' else 'scheduled'\n",
    "            })\n",
    "        \n",
    "        if games_data:\n",
    "            self.uploader.upsert_games(games_data)\n",
    "            self.stats['games'] += len(games_data)\n",
    "            log_message(f\"  âœ… {len(games_data)} games\")\n",
    "    \n",
    "    def _migrate_game_quarters(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¨×‘×¢×™ ××©×—×§\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_quarters.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ”¢ Quarters...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        quarters_data = df.to_dict('records')\n",
    "        \n",
    "        if quarters_data:\n",
    "            self.uploader.upsert_game_quarters(quarters_data)\n",
    "            self.stats['quarters'] += len(quarters_data)\n",
    "            log_message(f\"  âœ… {len(quarters_data)} quarters\")\n",
    "    \n",
    "    def _migrate_game_player_stats(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×©×—×§×Ÿ ×‘××©×—×§\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_player_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“Š Player stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_player_stats(stats_data)\n",
    "            self.stats['player_stats'] += len(stats_data)\n",
    "            log_message(f\"  âœ… {len(stats_data)} player stats\")\n",
    "    \n",
    "    def _migrate_game_team_stats(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×§×‘×•×¦×” ×‘××©×—×§\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_team_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“ˆ Team stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_team_stats(stats_data)\n",
    "            self.stats['team_stats'] += len(stats_data)\n",
    "            log_message(f\"  âœ… {len(stats_data)} team stats\")\n",
    "    \n",
    "    def _migrate_player_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×××•×¦×¢×™ ×©×—×§× ×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“‰ Player averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_player_averages(avg_data)\n",
    "            self.stats['player_averages'] += len(avg_data)\n",
    "            log_message(f\"  âœ… {len(avg_data)} player averages\")\n",
    "    \n",
    "    def _migrate_team_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×××•×¦×¢×™ ×§×‘×•×¦×•×ª\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_team_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“Š Team averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_team_averages(avg_data)\n",
    "            self.stats['team_averages'] += len(avg_data)\n",
    "            log_message(f\"  âœ… {len(avg_data)} team averages\")\n",
    "    \n",
    "    def _migrate_opponent_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×××•×¦×¢×™ ×™×¨×™×‘×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_opponent_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ›¡ï¸ Opponent averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_opponent_averages(avg_data)\n",
    "            self.stats['opponent_averages'] += len(avg_data)\n",
    "            log_message(f\"  âœ… {len(avg_data)} opponent averages\")\n",
    "    \n",
    "    def _print_stats(self):\n",
    "        \"\"\"×”×“×¤×¡×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª\"\"\"\n",
    "        log_message(\"\")\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"ğŸ“Š MIGRATION STATISTICS\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        for key, value in self.stats.items():\n",
    "            if value > 0:\n",
    "                log_message(f\"  {key.replace('_', ' ').title()}: {value:,}\")\n",
    "        \n",
    "        total = sum(self.stats.values())\n",
    "        log_message(\"\")\n",
    "        log_message(f\"  TOTAL RECORDS: {total:,}\")\n",
    "        log_message(\"=\"*60)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Migrate CSV data to Supabase')\n",
    "    parser.add_argument('--league', type=str, help='Specific league ID to migrate')\n",
    "    parser.add_argument('--all', action='store_true', help='Migrate all active leagues')\n",
    "    parser.add_argument('--test', action='store_true', help='Test connection only')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    migrator = DataMigration()\n",
    "    \n",
    "    if args.test:\n",
    "        # Test connection\n",
    "        log_message(\"ğŸ§ª Testing Supabase connection...\")\n",
    "        if migrator.uploader.test_connection():\n",
    "            log_message(\"âœ… Connection successful!\")\n",
    "        else:\n",
    "            log_message(\"âŒ Connection failed!\")\n",
    "    \n",
    "    elif args.all or args.league:\n",
    "        # Migrate data\n",
    "        league_ids = [args.league] if args.league else None\n",
    "        migrator.migrate_all(league_ids)\n",
    "    \n",
    "    else:\n",
    "        # Show help\n",
    "        parser.print_help()\n",
    "        print(\"\\nExamples:\")\n",
    "        print(\"  python migrate_to_supabase.py --test              # Test connection\")\n",
    "        print(\"  python migrate_to_supabase.py --all               # Migrate all leagues\")\n",
    "        print(\"  python migrate_to_supabase.py --league 1          # Migrate league 1 only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe7cb82-7e89-4803-96bb-f10f18190e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… URL: https://whaogopjlqzaricskrrq.supabase.co\n",
      "âœ… Key ××ª×—×™×œ ×‘: eyJhbGciOiJIUzI1NiIs...\n",
      "âœ… Connected!\n",
      "âœ… ×™×© 0 ×œ×™×’×•×ª\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import ×¡×¤×¦×™×¤×™\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Cell 2: ×˜×¢×Ÿ credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = os.getenv('SUPABASE_URL')\n",
    "key = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "# Cell 3: ×‘×“×•×§ ×©×”× ×§×™×™××™×\n",
    "if not url or not key:\n",
    "    print(\"âŒ ×—×¡×¨ URL ××• KEY!\")\n",
    "else:\n",
    "    print(f\"âœ… URL: {url}\")\n",
    "    print(f\"âœ… Key ××ª×—×™×œ ×‘: {key[:20]}...\")\n",
    "\n",
    "# Cell 4: ×—×™×‘×•×¨ ×¢× try-except\n",
    "try:\n",
    "    supabase: Client = create_client(url, key)\n",
    "    print(\"âœ… Connected!\")\n",
    "    \n",
    "    # ×‘×“×™×§×”\n",
    "    result = supabase.table('leagues').select(\"count\", count=\"exact\").execute()\n",
    "    print(f\"âœ… ×™×© {result.count} ×œ×™×’×•×ª\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
