{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989d16ed-a86a-468b-bf49-4a8211c6a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 12:02:41] \n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] BASKETBALL SCRAPER STARTED\n",
      "[2025-10-14 12:02:41] Time: 2025-10-14 12:02:41\n",
      "[2025-10-14 12:02:41] Mode: QUICK\n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] Found 1 active leagues\n",
      "[2025-10-14 12:02:41] \n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] PROCESSING LEAGUE: 10 - ×œ×™×’×ª Winner ×¡×œ\n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] âœ… Loaded global team mapping: 93 teams, 316 name variations\n",
      "[2025-10-14 12:02:41]    League 1: 16 teams, 49 variations\n",
      "[2025-10-14 12:02:41] [WINNER] Initialized WinnerScraper with 5 boards\n",
      "[2025-10-14 12:02:41] [WINNER] ============================================================\n",
      "[2025-10-14 12:02:41] [WINNER] STARTING SCRAPE: ×œ×™×’×ª Winner ×¡×œ\n",
      "[2025-10-14 12:02:41] [WINNER] Mode: QUICK\n",
      "[2025-10-14 12:02:41] [WINNER] ============================================================\n",
      "[2025-10-14 12:02:41] [WINNER] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-14 12:02:41] [WINNER] Loaded 17 existing players\n",
      "[2025-10-14 12:02:41] [WINNER] Processing team web_id: 1111\n",
      "[2025-10-14 12:02:43] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:02:43] [WINNER]   â†³ Scraping ×’'×•×¨×“×Ÿ×¤×œ×•×™×“\n",
      "[2025-10-14 12:02:46] [WINNER]   â†³ Scraping ×’×™×œ×‘× ×™\n",
      "[2025-10-14 12:02:50] [WINNER]   â†³ Scraping ×¡×××’'×˜×™×œ\n",
      "[2025-10-14 12:02:54] [WINNER]   â†³ Scraping ××•×¨×©×™×œ×¨\n",
      "[2025-10-14 12:02:57] [WINNER]   â†³ Scraping ×–'×•×¨×“××Ÿ× ×™×§×•×œ×™×¡\n",
      "[2025-10-14 12:03:02] [WINNER]   â†³ Scraping ×¢××™×ª××œ×•×Ÿ\n",
      "[2025-10-14 12:03:06] [WINNER]   â†³ Scraping ×¢×™×“×Ÿ×—×›××•×Ÿ\n",
      "[2025-10-14 12:03:10] [WINNER]   â†³ Scraping ×™×•× ×ª×Ÿ××˜×™××¡\n",
      "[2025-10-14 12:03:14] [WINNER]   â†³ Scraping ×“×¨×™×•×Ÿ××˜×§×™× ×¡\n",
      "[2025-10-14 12:03:18] [WINNER]   â†³ Scraping ×¨×•× ××œ×“×•×¡×’×•\n",
      "[2025-10-14 12:03:22] [WINNER]   â†³ Scraping ××“×××¨×™××œ\n",
      "[2025-10-14 12:03:26] [WINNER]   â†³ Scraping ×’'×•×¨×“×Ÿ×›×”×Ÿ\n",
      "[2025-10-14 12:03:30] [WINNER]   â†³ Scraping ×“×¨×•×§×¨×•×¤×•×¨×“\n",
      "[2025-10-14 12:03:34] [WINNER]   â†³ Scraping ××™×™×–×™×”××™×™×–× ×“×•×¨×£\n",
      "[2025-10-14 12:03:38] [WINNER]   â†³ Scraping ×××™×¨×“× ×•×Ÿ\n",
      "[2025-10-14 12:03:42] [WINNER]   â†³ Scraping ×¨×•×™×™×¦×§×Ÿ\n",
      "[2025-10-14 12:03:46] [WINNER]   â†³ Scraping ××•×¨×’×˜×”\n",
      "[2025-10-14 12:03:50] [WINNER] Processing team web_id: 1110\n",
      "[2025-10-14 12:03:53] [WINNER]   Found 19 players\n",
      "[2025-10-14 12:03:53] [WINNER]   â†³ Scraping ×’'×•× ×ª×Ÿ××•×˜×œ×™\n",
      "[2025-10-14 12:03:57] [WINNER]   â†³ Scraping ×›×¨×™×¡×’'×•× ×¡\n",
      "[2025-10-14 12:04:01] [WINNER]   â†³ Scraping ×× ×˜×•× ×™×•×‘×œ×™×™×§× ×™\n",
      "[2025-10-14 12:04:05] [WINNER]   â†³ Scraping ××œ×™×™×–'×”×‘×¨××™×™× ×˜\n",
      "[2025-10-14 12:04:09] [WINNER]   â†³ Scraping ×¡× ×“×™×›×”×Ÿ\n",
      "[2025-10-14 12:04:13] [WINNER]   â†³ Scraping ××™×ª×™×©×’×‘\n",
      "[2025-10-14 12:04:18] [WINNER]   â†³ Scraping ×’×™××¤×œ×˜×™×Ÿ\n",
      "[2025-10-14 12:04:22] [WINNER]   â†³ Scraping ×‘×¨×˜×™××•×¨\n",
      "[2025-10-14 12:04:26] [WINNER]   â†³ Scraping ×˜×™×™×œ×¨×× ×™×¡\n",
      "[2025-10-14 12:04:30] [WINNER]   â†³ Scraping ×¢×•×–×‘×œ×™×™×–×¨\n",
      "[2025-10-14 12:04:34] [WINNER]   â†³ Scraping ×§×•×œ×™×Ÿ××œ×§×•×œ×\n",
      "[2025-10-14 12:04:38] [WINNER]   â†³ Scraping ×˜××™××•×“×™××¡×™\n",
      "[2025-10-14 12:04:42] [WINNER]   â†³ Scraping ×•××¡×™×œ×™×”××™×¦×™×¥'\n",
      "[2025-10-14 12:04:46] [WINNER]   â†³ Scraping ××™×©×•×•×™×™× ×¨×™×™×˜\n",
      "[2025-10-14 12:04:50] [WINNER]   â†³ Scraping ×“×Ÿ××•×˜×•×¨×•\n",
      "[2025-10-14 12:04:54] [WINNER]   â†³ Scraping ×™×××“×¨\n",
      "[2025-10-14 12:04:57] [WINNER]   â†³ Scraping ×ª×•××¨×’×™× ×ª\n",
      "[2025-10-14 12:05:02] [WINNER]   â†³ Scraping ×‘×¨×•× ×•×§××‘×•×§×œ×•\n",
      "[2025-10-14 12:05:06] [WINNER]   â†³ Scraping ×™×¤×ª×—×–×™×•\n",
      "[2025-10-14 12:05:10] [WINNER] Processing team web_id: 1112\n",
      "[2025-10-14 12:05:13] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:05:13] [WINNER]   â†³ Scraping ×’'×¡×˜×™×Ÿ×¡××™×ª'\n",
      "[2025-10-14 12:05:17] [WINNER]   â†³ Scraping ×’'××¨×“×”××¨×¤×¨\n",
      "[2025-10-14 12:05:21] [WINNER]   â†³ Scraping ×§××“×™×Ÿ×§×¨×™× ×’×˜×•×Ÿ\n",
      "[2025-10-14 12:05:26] [WINNER]   â†³ Scraping ×¨× ×™×‘×œ×’×”\n",
      "[2025-10-14 12:05:29] [WINNER]   â†³ Scraping ××•×¡×˜×™×Ÿ×•×•×™×™×œ×™\n",
      "[2025-10-14 12:05:33] [WINNER]   â†³ Scraping ×¨×•×¢×™×¤×¨×™×¦×§×™\n",
      "[2025-10-14 12:05:37] [WINNER]   â†³ Scraping ×’×‘×¨×™××œ×¦××¦××©×•×™×œ×™\n",
      "[2025-10-14 12:05:41] [WINNER]   â†³ Scraping ×§×©×™×•×¡×•×•×™× ×¡×˜×•×Ÿ\n",
      "[2025-10-14 12:05:45] [WINNER]   â†³ Scraping ×™×•×‘×œ×‘×Ÿ ×¢×˜×¨\n",
      "[2025-10-14 12:05:49] [WINNER]   â†³ Scraping ×¨×•×¢×™×”×•×‘×¨\n",
      "[2025-10-14 12:05:54] [WINNER]   â†³ Scraping × ×™×¨×¤×¨×™\n",
      "[2025-10-14 12:05:57] [WINNER]   â†³ Scraping × ××¨×•×“×œ×•×™\n",
      "[2025-10-14 12:06:02] [WINNER]   â†³ Scraping ×™×¨×™×Ÿ×—×¡×•×Ÿ\n",
      "[2025-10-14 12:06:05] [WINNER]   â†³ Scraping ×× ×ª×•× ×™×œ×××‘\n",
      "[2025-10-14 12:06:10] [WINNER]   â†³ Scraping ×“××™×˜×¨×•×¡×§×¤×™× ×¦×‘\n",
      "[2025-10-14 12:06:13] [WINNER]   â†³ Scraping ×™×•×‘×œ×–×•×¡××Ÿ\n",
      "[2025-10-14 12:06:17] [WINNER]   â†³ Scraping ×’'×•×–××™×”Ö¾×’'×•×¨×“×Ÿ×’'×™×™××¡\n",
      "[2025-10-14 12:06:21] [WINNER] Processing team web_id: 1109\n",
      "[2025-10-14 12:06:24] [WINNER]   Found 16 players\n",
      "[2025-10-14 12:06:24] [WINNER]   â†³ Scraping ×’'×™×™×œ×Ÿ×”×•×¨×“\n",
      "[2025-10-14 12:06:28] [WINNER]   â†³ Scraping ×’'×™××™×§×œ××¨×§\n",
      "[2025-10-14 12:06:32] [WINNER]   â†³ Scraping ××¨×¡×™×•×¡× ×˜×•×¡\n",
      "[2025-10-14 12:06:35] [WINNER]   â†³ Scraping ×’×•×¨×œ×‘×™×\n",
      "[2025-10-14 12:06:40] [WINNER]   â†³ Scraping ××•×¨×•×©×˜×¨×™×¤×•× ×•×‘×™×¥'\n",
      "[2025-10-14 12:06:43] [WINNER]   â†³ Scraping ××•×¨×Ÿ×¡×”×¨\n",
      "[2025-10-14 12:06:47] [WINNER]   â†³ Scraping ×œ×•× ×™×•×•×§×¨\n",
      "[2025-10-14 12:06:51] [WINNER]   â†³ Scraping ×¨×•××Ÿ×¡×•×¨×§×™×Ÿ\n",
      "[2025-10-14 12:06:55] [WINNER]   â†³ Scraping ××•×©×™×™×‘×¨×™×¡×˜\n",
      "[2025-10-14 12:06:59] [WINNER]   â†³ Scraping ×•×•×™×œ×¨×™×™××Ÿ\n",
      "[2025-10-14 12:07:08] [WINNER]   â†³ Scraping ×’'×•×Ÿ×“×™×‘×¨×ª×•×œ×•××™××•\n",
      "[2025-10-14 12:07:12] [WINNER]   â†³ Scraping ×ª××™×¨×’×•×œ×“\n",
      "[2025-10-14 12:07:16] [WINNER]   â†³ Scraping ×§×œ×™×¤×•×¨×“××•××•×¨×™\n",
      "[2025-10-14 12:07:20] [WINNER]   â†³ Scraping ×’'×£×“××•×˜×™×Ÿ ×’'×•× ×™×•×¨\n",
      "[2025-10-14 12:07:23] [WINNER]   â†³ Scraping ×˜×™.×’'×™×™×œ×™×£\n",
      "[2025-10-14 12:07:27] [WINNER]   â†³ Scraping ×ª××™×¨×‘×œ××˜\n",
      "[2025-10-14 12:07:32] [WINNER] Processing team web_id: 1119\n",
      "[2025-10-14 12:07:35] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:07:35] [WINNER]   â†³ Scraping ×¡×™×¨×™×œ×œ× ×’'×™×•×•×™×™×Ÿ\n",
      "[2025-10-14 12:07:39] [WINNER]   â†³ Scraping ×’'×™×™×œ×Ÿ×‘×œ×™×™×§×¡\n",
      "[2025-10-14 12:07:43] [WINNER]   â†³ Scraping ×‘×Ÿ××œ×˜×©×•×œ×¨\n",
      "[2025-10-14 12:07:47] [WINNER]   â†³ Scraping ×“×™×× ×“×¨×”×’×•×œ×¡×˜×•×Ÿ\n",
      "[2025-10-14 12:07:51] [WINNER]   â†³ Scraping ××•×¨×™×’×•×¨×“×•×Ÿ\n",
      "[2025-10-14 12:07:55] [WINNER]   â†³ Scraping ×©×—×¨×¢××™×¨\n",
      "[2025-10-14 12:07:59] [WINNER]   â†³ Scraping ××™×™×–×™×”×‘×¨××•×Ÿ\n",
      "[2025-10-14 12:08:03] [WINNER]   â†³ Scraping ×©×—×¨×“×•×¨×•×Ÿ\n",
      "[2025-10-14 12:08:07] [WINNER]   â†³ Scraping ×™×”×œ××œ××“\n",
      "[2025-10-14 12:08:11] [WINNER]   â†³ Scraping ×¨×•×Ÿ×‘×Ÿ ×“×¨\n",
      "[2025-10-14 12:08:15] [WINNER]   â†³ Scraping ×‘×¨×™×™×œ×™××œ×‘×¨×˜\n",
      "[2025-10-14 12:08:19] [WINNER]   â†³ Scraping ×¨×–××“×\n",
      "[2025-10-14 12:08:23] [WINNER]   â†³ Scraping ×™×•××‘××œ××™×©×œ×™\n",
      "[2025-10-14 12:08:27] [WINNER]   â†³ Scraping ××•×¨×—×Ÿ\n",
      "[2025-10-14 12:08:31] [WINNER]   â†³ Scraping ××™×ª×™×¤×œ×“××Ÿ\n",
      "[2025-10-14 12:08:35] [WINNER]   â†³ Scraping × ×•×¢×××‘×™×‘×™\n",
      "[2025-10-14 12:08:39] [WINNER]   â†³ Scraping ×¡×××œ×’'×™×§×™\n",
      "[2025-10-14 12:08:43] [WINNER] Processing team web_id: 1124\n",
      "[2025-10-14 12:08:46] [WINNER]   Found 13 players\n",
      "[2025-10-14 12:08:46] [WINNER]   â†³ Scraping ×•×•×™×œ×™×•×•×¨×§××Ÿ\n",
      "[2025-10-14 12:08:50] [WINNER]   â†³ Scraping ×’'×™×™×¡×˜××•× ×¡× ×“\n",
      "[2025-10-14 12:08:54] [WINNER]   â†³ Scraping ×—×¡×Ÿ×“×™××¨×”\n",
      "[2025-10-14 12:08:58] [WINNER]   â†³ Scraping ×—×Ÿ×’×™×\n",
      "[2025-10-14 12:09:01] [WINNER]   â†³ Scraping ×œ× ××¨×“×¤×¨×™××Ÿ\n",
      "[2025-10-14 12:09:05] [WINNER]   â†³ Scraping ××¤×™×˜×™×•××§×™×Ÿ\n",
      "[2025-10-14 12:09:09] [WINNER]   â†³ Scraping ×—×•××§×™×Ÿ×©×•×›××Ÿ\n",
      "[2025-10-14 12:09:14] [WINNER]   â†³ Scraping × ×™×™×˜×œ××©×‘×¡×§×™\n",
      "[2025-10-14 12:09:18] [WINNER]   â†³ Scraping ×™×•×ª××—× ×•×›×™\n",
      "[2025-10-14 12:09:23] [WINNER]   â†³ Scraping ××œ×›×¡×œ×“×¨\n",
      "[2025-10-14 12:09:26] [WINNER]   â†³ Scraping ×˜×™×™×¨×™×¡×¨×“×¤×•×¨×“\n",
      "[2025-10-14 12:09:30] [WINNER]   â†³ Scraping ××•×¨×Ÿ×‘×•×§×•×‘×–×”\n",
      "[2025-10-14 12:09:34] [WINNER]   â†³ Scraping ××¨×™××œ×¢×¦××•×Ÿ\n",
      "[2025-10-14 12:09:38] [WINNER] Processing team web_id: 1122\n",
      "[2025-10-14 12:09:41] [WINNER]   Found 15 players\n",
      "[2025-10-14 12:09:41] [WINNER]   â†³ Scraping ×§×™×™×œ×¨××“×•××¨×“×¡\n",
      "[2025-10-14 12:09:45] [WINNER]   â†³ Scraping ×™×•×‘×œ×›×•×›×‘×™\n",
      "[2025-10-14 12:09:49] [WINNER]   â†³ Scraping ×§×™×™×“×Ÿ×©×“×¨×™×§\n",
      "[2025-10-14 12:09:58] [WINNER]   â†³ Scraping ×–×™×•×•×™×™×¡××Ÿ\n",
      "[2025-10-14 12:10:03] [WINNER]   â†³ Scraping ×¨×¤×™×× ×§×•\n",
      "[2025-10-14 12:10:07] [WINNER]   â†³ Scraping ××œ×™×™×–'×”×¦'×™×™×œ×“×¡\n",
      "[2025-10-14 12:10:11] [WINNER]   â†³ Scraping ××¨×•×Ÿ×•×•×™×œ×¨\n",
      "[2025-10-14 12:10:15] [WINNER]   â†³ Scraping ×™×•× ×ª×Ÿ××œ×•×œ\n",
      "[2025-10-14 12:10:19] [WINNER]   â†³ Scraping ×¨×•×¢×™× ×¦×™×”\n",
      "[2025-10-14 12:10:23] [WINNER]   â†³ Scraping ×§××¨×•×Ÿ×”× ×¨×™\n",
      "[2025-10-14 12:10:27] [WINNER]   â†³ Scraping ×’×™×œ× ×•×™×•×‘×™×¥'\n",
      "[2025-10-14 12:10:31] [WINNER]   â†³ Scraping ×œ×•×§××¡×’×•×œ×“× ×‘×¨×’\n",
      "[2025-10-14 12:10:35] [WINNER]   â†³ Scraping ×¢×“×™×¨×™× ×¡×§×™\n",
      "[2025-10-14 12:10:39] [WINNER]   â†³ Scraping × ×™×‘××©×’×‘\n",
      "[2025-10-14 12:10:44] [WINNER]   â†³ Scraping ×©×—×¨××¨×–\n",
      "[2025-10-14 12:10:47] [WINNER] Processing team web_id: 2109\n",
      "[2025-10-14 12:10:50] [WINNER]   Found 19 players\n",
      "[2025-10-14 12:10:50] [WINNER]   â†³ Scraping ×¡×××’'×”×›×¨×™×¡×˜×•×Ÿ\n",
      "[2025-10-14 12:10:54] [WINNER]   â†³ Scraping ×˜×¨×™×™×§×œ×•×•×™×Ÿ\n",
      "[2025-10-14 12:10:58] [WINNER]   â†³ Scraping ×©××§×‘×™×•×§× ×Ÿ\n",
      "[2025-10-14 12:11:02] [WINNER]   â†³ Scraping ×§×œ×™××œ×¡×¤×™×¨\n",
      "[2025-10-14 12:11:06] [WINNER]   â†³ Scraping ×“×•×¨×™×¡×”×¨\n",
      "[2025-10-14 12:11:10] [WINNER]   â†³ Scraping ××‘×™×‘×”× ×§×™×Ÿ\n",
      "[2025-10-14 12:11:14] [WINNER]   â†³ Scraping × ×™×§×™×¨×¤×¤×•×¨×˜\n",
      "[2025-10-14 12:11:18] [WINNER]   â†³ Scraping ×’×œ×©×˜×¨× ×‘×¨×’\n",
      "[2025-10-14 12:11:22] [WINNER]   â†³ Scraping ××¨×™××œ×¡×œ×¢\n",
      "[2025-10-14 12:11:26] [WINNER]   â†³ Scraping ×¢×™×“×•×©×‘×ª\n",
      "[2025-10-14 12:11:30] [WINNER]   â†³ Scraping ×‘×•×¨×™×¡×‘×•×’×•×¡×œ×‘×¡×§×™\n",
      "[2025-10-14 12:11:34] [WINNER]   â†³ Scraping ××§×¡×™××¤×™×œ×•× × ×§×•\n",
      "[2025-10-14 12:11:38] [WINNER]   â†³ Scraping × ×™×¡×™××ª×•×¨×’'××Ÿ\n",
      "[2025-10-14 12:11:42] [WINNER]   â†³ Scraping ×§×™×™×œ×‘×‘×•×Ÿ\n",
      "[2025-10-14 12:11:46] [WINNER]   â†³ Scraping ××•×˜×™×¡×¤×¨×™×™×–'×¨\n",
      "[2025-10-14 12:11:50] [WINNER]   â†³ Scraping × ×™×§×•×§×¨×‘××¦'×•\n",
      "[2025-10-14 12:11:54] [WINNER]   â†³ Scraping ×©×’×™×‘×“×‘×™×¨\n",
      "[2025-10-14 12:11:58] [WINNER]   â†³ Scraping ××™×ª×Ÿ×™× ××™\n",
      "[2025-10-14 12:12:01] [WINNER]   â†³ Scraping ××•×¨×§×•×¨× ×œ×™×•×¡\n",
      "[2025-10-14 12:12:06] [WINNER] Processing team web_id: 1120\n",
      "[2025-10-14 12:12:08] [WINNER]   Found 16 players\n",
      "[2025-10-14 12:12:08] [WINNER]   â†³ Scraping × ×•×•×”×‘×Ÿ ×©××Ÿ\n",
      "[2025-10-14 12:12:12] [WINNER]   â†³ Scraping ×‘× ×™××™×Ÿ×–×‘×™× ×¡×§×™\n",
      "[2025-10-14 12:12:16] [WINNER]   â†³ Scraping ××§×™×œ×™×•×™×™× ×™× ×’\n",
      "[2025-10-14 12:12:20] [WINNER]   â†³ Scraping ×¢×•×¤×¨×™×©×˜×¨×™×ª\n",
      "[2025-10-14 12:12:24] [WINNER]   â†³ Scraping ×˜××¨×™×Ÿ×˜×•×“\n",
      "[2025-10-14 12:12:28] [WINNER]   â†³ Scraping ××¨×™××œ××™×™×–×™×§\n",
      "[2025-10-14 12:12:32] [WINNER]   â†³ Scraping × ×˜×¢×¡×’×œ\n",
      "[2025-10-14 12:12:36] [WINNER]   â†³ Scraping ×¡×™.×’'×™×™××œ×‘×™\n",
      "[2025-10-14 12:12:40] [WINNER]   â†³ Scraping ×¢××™×ª×’×¨×©×•×Ÿ\n",
      "[2025-10-14 12:12:44] [WINNER]   â†³ Scraping ×‘×¨× ×“×•×Ÿ×× ×’'×œ\n",
      "[2025-10-14 12:12:48] [WINNER]   â†³ Scraping ×¡×××™×•×¨×™×•\n",
      "[2025-10-14 12:12:52] [WINNER]   â†³ Scraping ×™×•××‘×•×™×˜×œ×\n",
      "[2025-10-14 12:12:56] [WINNER]   â†³ Scraping ××™×™×§×œ×¤×•×¡×˜×¨ ×’'×•× ×™×•×¨\n",
      "[2025-10-14 12:13:00] [WINNER]   â†³ Scraping ××ª×Ÿ×¤×•×’×œ\n",
      "[2025-10-14 12:13:04] [WINNER]   â†³ Scraping ×§×•×“×™×“××¤×¡\n",
      "[2025-10-14 12:13:08] [WINNER]   â†³ Scraping ×’'×™×™×œ×Ÿ×˜×™×™×˜\n",
      "[2025-10-14 12:13:12] [WINNER] Processing team web_id: 1116\n",
      "[2025-10-14 12:13:14] [WINNER]   Found 16 players\n",
      "[2025-10-14 12:13:14] [WINNER]   â†³ Scraping ×¢×ª×™×¨×˜×œ××•×¨\n",
      "[2025-10-14 12:13:19] [WINNER]   â†³ Scraping ×“××™×•×Ÿ×œ×™\n",
      "[2025-10-14 12:13:23] [WINNER]   â†³ Scraping ×§×™×™×¡×™×©×¤×¨×“\n",
      "[2025-10-14 12:13:27] [WINNER]   â†³ Scraping ×˜×™.×’'×™×™×§×œ×™×™×Ÿ\n",
      "[2025-10-14 12:13:31] [WINNER]   â†³ Scraping ×©×’×™××§××—×’'×™\n",
      "[2025-10-14 12:13:35] [WINNER]   â†³ Scraping ×˜×™×™×¨×™×§×¡××™×ª'\n",
      "[2025-10-14 12:13:38] [WINNER]   â†³ Scraping ××™×ª×™××•×©×§×•×‘×™×¥\n",
      "[2025-10-14 12:13:43] [WINNER]   â†³ Scraping ×œ×•×˜×Ÿ×××¡×œ×\n",
      "[2025-10-14 12:13:47] [WINNER]   â†³ Scraping ×¡×¤× ×¡×¨×•×™×™×¡\n",
      "[2025-10-14 12:13:51] [WINNER]   â†³ Scraping ×× ×“×¨×”×§×•×¨×‘×œ×•\n",
      "[2025-10-14 12:13:55] [WINNER]   â†³ Scraping ××•×¨×˜×¤×™×¨×•\n",
      "[2025-10-14 12:13:59] [WINNER]   â†³ Scraping ××“×¨××”×¨×•× ×™\n",
      "[2025-10-14 12:14:03] [WINNER]   â†³ Scraping ××™×™×–×™×”×•×•×™×™×˜×”×“\n",
      "[2025-10-14 12:14:06] [WINNER]   â†³ Scraping ××•×¤×§×©××¨\n",
      "[2025-10-14 12:14:10] [WINNER]   â†³ Scraping ××•×“×•×§×”××–×•×‘×•×§×™×™\n",
      "[2025-10-14 12:14:14] [WINNER]   â†³ Scraping ×“×–×™×¨×•×“×¨×™×’×–\n",
      "[2025-10-14 12:14:18] [WINNER] Processing team web_id: 1114\n",
      "[2025-10-14 12:14:21] [WINNER]   Found 14 players\n",
      "[2025-10-14 12:14:21] [WINNER]   â†³ Scraping ×ª×•××¨××¡×™×™×’\n",
      "[2025-10-14 12:14:25] [WINNER]   â†³ Scraping ×’'××™×”× ×™×œ\n",
      "[2025-10-14 12:14:29] [WINNER]   â†³ Scraping × ×•××”×œ×•×§\n",
      "[2025-10-14 12:14:33] [WINNER]   â†³ Scraping ××“××©×¨×‘×§×•×‘\n",
      "[2025-10-14 12:14:37] [WINNER]   â†³ Scraping ××™×ª×™×–×œ×•×˜×•×œ×•×‘\n",
      "[2025-10-14 12:14:41] [WINNER]   â†³ Scraping ×‘×•×“×™×™×•×\n",
      "[2025-10-14 12:14:45] [WINNER]   â†³ Scraping ××™×œ×™×“×•×œ×™× ×¡×§×™\n",
      "[2025-10-14 12:14:49] [WINNER]   â†³ Scraping ××™×ª×™××•×‘×¡×•×‘×™×¥\n",
      "[2025-10-14 12:14:52] [WINNER]   â†³ Scraping ×’×œ×‘×™×™×˜× ×¨\n",
      "[2025-10-14 12:14:56] [WINNER]   â†³ Scraping ×¨×•×Ÿ×“× ×“×™×§×¨\n",
      "[2025-10-14 12:15:00] [WINNER]   â†³ Scraping ×××œ×™×§×”×•×œ\n",
      "[2025-10-14 12:15:04] [WINNER]   â†³ Scraping ×“×¨×™×•×¡×”×× ×”\n",
      "[2025-10-14 12:15:08] [WINNER]   â†³ Scraping ×“×•×¨×¢× ×‘×™\n",
      "[2025-10-14 12:15:12] [WINNER]   â†³ Scraping ×™×•×‘×œ×”×•×›×©×˜×˜×¨\n",
      "[2025-10-14 12:15:16] [WINNER] Processing team web_id: 1123\n",
      "[2025-10-14 12:15:19] [WINNER]   Found 13 players\n",
      "[2025-10-14 12:15:19] [WINNER]   â†³ Scraping ×§×•×•×”×’×¨× ×˜\n",
      "[2025-10-14 12:15:23] [WINNER]   â†³ Scraping ×‘×Ÿ××™×™×–× ×”××¨×˜\n",
      "[2025-10-14 12:15:27] [WINNER]   â†³ Scraping ×××™×Ÿ×¡×˜×™×‘× ×¡\n",
      "[2025-10-14 12:15:31] [WINNER]   â†³ Scraping ×§××œ×™×œ××—××“\n",
      "[2025-10-14 12:15:36] [WINNER]   â†³ Scraping ××¨×›×•×¡×•×•×™×œ×™×××¡\n",
      "[2025-10-14 12:15:40] [WINNER]   â†³ Scraping ×’×•×œ×Ÿ×’×•×˜\n",
      "[2025-10-14 12:15:44] [WINNER]   â†³ Scraping ××•×¨×™×–×”×‘×™\n",
      "[2025-10-14 12:15:48] [WINNER]   â†³ Scraping ×¢×•××¨×’×•×˜×¡××Ÿ\n",
      "[2025-10-14 12:15:52] [WINNER]   â†³ Scraping ×™×¨×•×Ÿ×’×•×œ×“××Ÿ\n",
      "[2025-10-14 12:15:56] [WINNER]   â†³ Scraping ×’'×•×©×¤×¨×•×™× ×“\n",
      "[2025-10-14 12:16:00] [WINNER]   â†³ Scraping ×œ×™××•×¨×‘×¨××Ÿ\n",
      "[2025-10-14 12:16:04] [WINNER]   â†³ Scraping ×’'×™×™.×’'×™×™×§×¤×œ×Ÿ\n",
      "[2025-10-14 12:16:08] [WINNER]   â†³ Scraping ×“×™.×’'×™×™×‘×¨× ×¡\n",
      "[2025-10-14 12:16:12] [WINNER] Processing team web_id: 1113\n",
      "[2025-10-14 12:16:15] [WINNER]   Found 18 players\n",
      "[2025-10-14 12:16:15] [WINNER]   â†³ Scraping ×’'×•×¨×“×Ÿ×‘×•×Ÿ\n",
      "[2025-10-14 12:16:19] [WINNER]   â†³ Scraping ×œ×™×¢×“×©××•××œ\n",
      "[2025-10-14 12:16:23] [WINNER]   â†³ Scraping ×“××™×•×Ÿ×¨×•×¡×¨\n",
      "[2025-10-14 12:16:27] [WINNER]   â†³ Scraping ×“×¨×™×§×•×•×œ×˜×•×Ÿ ×’'×•× ×™×•×¨\n",
      "[2025-10-14 12:16:31] [WINNER]   â†³ Scraping ×˜×•×“×•×•×™×ª'×¨×¡\n",
      "[2025-10-14 12:16:34] [WINNER]   â†³ Scraping ××§×¡×‘×™×™×¨×× ×¤×•×¨×“\n",
      "[2025-10-14 12:16:38] [WINNER]   â†³ Scraping ×—×¡×Ÿ××¨×˜×™×Ÿ\n",
      "[2025-10-14 12:16:42] [WINNER]   â†³ Scraping ××™×™×œ×¡×•×œ×˜×Ÿ\n",
      "[2025-10-14 12:16:46] [WINNER]   â†³ Scraping ××™×™×§×œ××œ×™××¡×–×“×”\n",
      "[2025-10-14 12:16:50] [WINNER]   â†³ Scraping ×œ×™××•×¨×§×¨×¨×”\n",
      "[2025-10-14 12:16:53] [WINNER]   â†³ Scraping × ×¡×™×™×¨×‘×¨×•×§×¡\n",
      "[2025-10-14 12:16:57] [WINNER]   â†³ Scraping ×¨×•×¢×™×¨×•×–× ×‘×¨×’\n",
      "[2025-10-14 12:17:01] [WINNER]   â†³ Scraping ××•×¤×§× ×™×¡××–×•×Ÿ\n",
      "[2025-10-14 12:17:05] [WINNER]   â†³ Scraping ×¢×™×“×Ÿ×–×œ×× ×¡×•×Ÿ\n",
      "[2025-10-14 12:17:09] [WINNER]   â†³ Scraping × ×ª× ××œ××¨×¦×™\n",
      "[2025-10-14 12:17:13] [WINNER]   â†³ Scraping ×™××™×¨×§×¨×‘×™×¥\n",
      "[2025-10-14 12:17:17] [WINNER]   â†³ Scraping ×™×•×ª××× ×©×”\n",
      "[2025-10-14 12:17:21] [WINNER]   â†³ Scraping ×’'×•×¨×“×Ÿ××§×¨×™×™\n",
      "[2025-10-14 12:17:25] [WINNER] Processing team web_id: 1118\n",
      "[2025-10-14 12:17:28] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:17:28] âŒ CRITICAL ERROR in league '10': cannot safely cast non-equivalent object to int64\n",
      "[2025-10-14 12:17:28] Traceback (most recent call last):\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\integer.py\", line 53, in _safe_cast\n",
      "    return values.astype(dtype, casting=\"safe\", copy=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\main.py\", line 178, in scrape_league\n",
      "    success = scraper.run()\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\scrapers\\base_scraper.py\", line 85, in run\n",
      "    if not self._update_player_details():\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\scrapers\\winner.py\", line 164, in _update_player_details\n",
      "    save_to_csv(merged_df, details_file)\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\utils\\helpers.py\", line 43, in save_to_csv\n",
      "    df[col] = df[col].astype('Int64')\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\", line 6665, in astype\n",
      "    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 449, in astype\n",
      "    return self.apply(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 363, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 784, in astype\n",
      "    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 237, in astype_array_safe\n",
      "    new_values = astype_array(values, dtype, copy=copy)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 182, in astype_array\n",
      "    values = _astype_nansafe(values, dtype, copy=copy)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 80, in _astype_nansafe\n",
      "    return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 153, in _from_sequence\n",
      "    values, mask = cls._coerce_to_array(scalars, dtype=dtype, copy=copy)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\numeric.py\", line 272, in _coerce_to_array\n",
      "    values, mask, _, _ = _coerce_to_data_and_mask(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\numeric.py\", line 229, in _coerce_to_data_and_mask\n",
      "    values = dtype_cls._safe_cast(values, dtype, copy=False)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\integer.py\", line 59, in _safe_cast\n",
      "    raise TypeError(\n",
      "TypeError: cannot safely cast non-equivalent object to int64\n",
      "\n",
      "[2025-10-14 12:17:28] \n",
      "[2025-10-14 12:17:28] ================================================================================\n",
      "[2025-10-14 12:17:28] UPDATING GLOBAL FILES\n",
      "[2025-10-14 12:17:28] ================================================================================\n",
      "[2025-10-14 12:17:28] Updating global leagues.csv...\n",
      "[2025-10-14 12:17:28] âœ… Global leagues.csv updated: 11 leagues\n",
      "[2025-10-14 12:17:28] Checking global teams.csv...\n",
      "[2025-10-14 12:17:28] âœ… Global teams.csv verified: 93 teams\n",
      "[2025-10-14 12:17:28] Updating global players.csv...\n",
      "[2025-10-14 12:17:29] âœ… Global players.csv updated: 800 players\n",
      "[2025-10-14 12:17:29] \n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] SCRAPING SUMMARY\n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] âœ… Successful: 0 leagues\n",
      "[2025-10-14 12:17:29] âŒ Failed: 1 leagues\n",
      "[2025-10-14 12:17:29]    âœ— League 10: ×œ×™×’×ª Winner ×¡×œ\n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] \n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] BASKETBALL SCRAPER FINISHED\n",
      "[2025-10-14 12:17:29] Time: 2025-10-14 12:17:29\n",
      "[2025-10-14 12:17:29] ================================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['main.py']\n",
    "from main import main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd8431-5e94-405a-9e5c-428464ec79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Print a filtered tree: folders + CSV files only, plus a flat CSV list with sizes.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Settings ----------\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5          # ×¢×•××§ ××§×¡×™××œ×™ ×œ×¡×¨×™×§×”\n",
    "SHOW_HIDDEN = False    # ×œ×”×¦×™×’ ×ª×™×§×™×•×ª/×§×‘×¦×™× ×©××ª×—×™×œ×™× ×‘× ×§×•×“×”?\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_hidden(p: Path) -> bool:\n",
    "    return any(part.startswith(\".\") for part in p.parts)\n",
    "\n",
    "def within_depth(root: Path, p: Path, max_depth: int) -> bool:\n",
    "    return len(p.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def dir_children(root: Path, d: Path):\n",
    "    \"\"\"Return (dirs, csvs) immediate children (not recursive), filtered.\"\"\"\n",
    "    dirs, csvs = [], []\n",
    "    for child in sorted(d.iterdir()):\n",
    "        if not SHOW_HIDDEN and is_hidden(child):\n",
    "            continue\n",
    "        if child.is_dir():\n",
    "            dirs.append(child)\n",
    "        elif child.is_file() and child.suffix.lower() == \".csv\":\n",
    "            csvs.append(child)\n",
    "    return dirs, csvs\n",
    "\n",
    "def contains_relevant_descendants(root: Path, d: Path, max_depth: int) -> bool:\n",
    "    \"\"\"Does this directory (recursively) contain any dir/csv within max_depth?\"\"\"\n",
    "    for p in d.rglob(\"*\"):\n",
    "        # stop at max depth\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p):\n",
    "            continue\n",
    "        if p.is_dir() or (p.is_file() and p.suffix.lower() == \".csv\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def print_tree(root: Path, max_depth: int):\n",
    "    print(f\"ROOT: {root}\\n\")\n",
    "    # We print the root name then recurse\n",
    "    print(f\"ğŸ“ {root.name}\")\n",
    "    def recurse(d: Path, prefix: str, depth: int):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        dirs, csvs = dir_children(root, d)\n",
    "\n",
    "        # Filter out directories that do not contain any relevant descendants within depth\n",
    "        pruned_dirs = []\n",
    "        for sd in dirs:\n",
    "            if within_depth(root, sd, max_depth) and (contains_relevant_descendants(root, sd, max_depth) or list(dir_children(root, sd)[1])):\n",
    "                pruned_dirs.append(sd)\n",
    "\n",
    "        # Build combined list: dirs first, then CSVs\n",
    "        items = pruned_dirs + csvs\n",
    "        for i, item in enumerate(items):\n",
    "            is_last = (i == len(items) - 1)\n",
    "            branch = \"â””â”€ \" if is_last else \"â”œâ”€ \"\n",
    "            if item.is_dir():\n",
    "                print(prefix + branch + \"ğŸ“ \" + item.name)\n",
    "                new_prefix = prefix + (\"   \" if is_last else \"â”‚  \")\n",
    "                recurse(item, new_prefix, depth + 1)\n",
    "            else:\n",
    "                print(prefix + branch + \"ğŸ“„ \" + item.name)\n",
    "    recurse(root, \"\", 0)\n",
    "\n",
    "def human_size(num_bytes: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if num_bytes < 1024.0:\n",
    "            return f\"{num_bytes:.0f} {unit}\"\n",
    "        num_bytes /= 1024.0\n",
    "    return f\"{num_bytes:.0f} PB\"\n",
    "\n",
    "def list_csvs(root: Path, max_depth: int):\n",
    "    csvs = []\n",
    "    for p in sorted(root.rglob(\"*.csv\")):\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p.relative_to(root)):\n",
    "            continue\n",
    "        if p.is_file():\n",
    "            try:\n",
    "                size = human_size(p.stat().st_size)\n",
    "            except Exception:\n",
    "                size = \"?\"\n",
    "            csvs.append((p, size))\n",
    "    return csvs\n",
    "\n",
    "# ---------- Run ----------\n",
    "print(\"=== DIRECTORY TREE (folders + CSV only) ===\")\n",
    "print_tree(ROOT, MAX_DEPTH)\n",
    "\n",
    "print(\"\\n=== CSV FILES (flat list) ===\")\n",
    "csv_list = list_csvs(ROOT, MAX_DEPTH)\n",
    "if not csv_list:\n",
    "    print(\"(no CSV files found)\")\n",
    "else:\n",
    "    for p, size in csv_list:\n",
    "        rel = p.relative_to(ROOT)\n",
    "        print(f\"- {rel}  â€¢  {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ef0d3-b0fc-4f69-bf27-3e1404521fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ××¦×™×’ ×œ×›×œ ×§×•×‘×¥ CSV ××ª ×”× ×ª×™×‘, ×›×•×ª×¨×•×ª ×•×©×•×¨×” ××—×ª ×¨××©×•× ×” ×‘×¤×•×¨××˜ CSV\n",
    "# ××“×œ×’ ×¢×œ ×ª×™×§×™×•×ª ××•×¡×ª×¨×•×ª ×•-.ipynb_checkpoints, ×¢× ××¤×¨×™×“ ×‘×¨×•×¨ ×‘×™×Ÿ ×§×‘×¦×™×\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "def is_hidden_or_checkpoint(path: Path) -> bool:\n",
    "    \"\"\"×‘×•×“×§ ×”×× ×§×•×‘×¥ ××• ×ª×™×§×™×™×” ××•×¡×ª×¨×™× ××• ×‘×ª×•×š .ipynb_checkpoints\"\"\"\n",
    "    return any(\n",
    "        part.startswith(\".\") or \"ipynb_checkpoints\" in part\n",
    "        for part in path.parts\n",
    "    )\n",
    "\n",
    "def within_depth(root: Path, path: Path, max_depth: int) -> bool:\n",
    "    \"\"\"×‘×•×“×§ ×”×× ×”×§×•×‘×¥ ×‘×¢×•××§ ××•×ª×¨\"\"\"\n",
    "    return len(path.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def print_csv_previews(root: Path, max_depth: int):\n",
    "    csv_files = sorted(root.rglob(\"*.csv\"))\n",
    "    for csv_path in csv_files:\n",
    "        if is_hidden_or_checkpoint(csv_path.relative_to(root)):\n",
    "            continue\n",
    "        if not within_depth(root, csv_path, max_depth):\n",
    "            continue\n",
    "\n",
    "        rel_path = str(csv_path.relative_to(root))\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=\"utf-8-sig\", nrows=1)\n",
    "        except Exception:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding=\"utf-8\", nrows=1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n{rel_path}\")\n",
    "                print(f\"Error reading file: {e}\")\n",
    "                print(\"=\" * 60)\n",
    "                continue\n",
    "\n",
    "        print(f\"\\n{rel_path}\")  # ×›×•×ª×¨×ª ×¢× × ×ª×™×‘ ×”×§×•×‘×¥\n",
    "        print(\",\".join(df.columns))  # ×©×•×¨×ª ×›×•×ª×¨×•×ª\n",
    "        if not df.empty:\n",
    "            print(\",\".join(map(str, df.iloc[0].tolist())))  # ×©×•×¨×” ××—×ª ××”× ×ª×•× ×™×\n",
    "        print(\"=\" * 60)  # ××¤×¨×™×“ ×‘×™×Ÿ ×§×‘×¦×™×\n",
    "\n",
    "    print(\"\\nâœ… ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”.\")\n",
    "\n",
    "# ×”×¤×¢×œ×”\n",
    "print_csv_previews(ROOT, MAX_DEPTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3b1a8-1da3-4e21-a9f5-5d01beb98c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c709c0-3108-403e-bfbe-598ca510c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Test Live Mapping\n",
    "=================\n",
    "×‘×“×™×§×” ×¢× ×”×§×•×“ ×”××¢×•×“×›×Ÿ ×‘×“×™×•×§\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "teams_file = \"data/teams.csv\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Loading with COMPOSITE KEY: (variation, league_id)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "print(f\"âœ“ Loaded {len(df)} rows\\n\")\n",
    "\n",
    "# ×–×™×”×•×™ ×¢××•×“×•×ª\n",
    "columns = df.columns.tolist()\n",
    "column_mapping = {\n",
    "    'team_id': ['Team_ID', 'team_id', 'TeamID'],\n",
    "    'league_id': ['League_ID', 'league_id', 'LeagueID'],\n",
    "    'club_name': ['Team_Name', 'club_name', 'team_name', 'name'],\n",
    "    'variations': ['name_variations', 'variations', 'Variations'],\n",
    "}\n",
    "\n",
    "actual_columns = {}\n",
    "for key, possible_names in column_mapping.items():\n",
    "    for name in possible_names:\n",
    "        if name in columns:\n",
    "            actual_columns[key] = name\n",
    "            break\n",
    "\n",
    "print(f\"Detected columns: {actual_columns}\\n\")\n",
    "\n",
    "# ×™×¦×™×¨×ª ××™×¤×•×™ ×¢× ××¤×ª×— ××•×¨×›×‘\n",
    "mapping = {}\n",
    "teams_count = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    team_id = row[actual_columns['team_id']] if 'team_id' in actual_columns else None\n",
    "    league_id = row[actual_columns['league_id']] if 'league_id' in actual_columns else None\n",
    "    club_name = row[actual_columns['club_name']] if 'club_name' in actual_columns else None\n",
    "    \n",
    "    if pd.isna(team_id) or pd.isna(league_id) or pd.isna(club_name):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        league_id_int = int(league_id)\n",
    "        if league_id_int == 0:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    teams_count += 1\n",
    "    \n",
    "    variations = row[actual_columns['variations']] if 'variations' in actual_columns else club_name\n",
    "    \n",
    "    club_name = html.unescape(str(club_name)).strip()\n",
    "    variations_str = html.unescape(str(variations)).strip()\n",
    "    \n",
    "    variation_list = []\n",
    "    for v in variations_str.split('|'):\n",
    "        v_clean = html.unescape(v.strip())\n",
    "        if v_clean:\n",
    "            variation_list.append(v_clean)\n",
    "    \n",
    "    team_info = {\n",
    "        'team_id': int(team_id),\n",
    "        'league_id': int(league_id_int),\n",
    "        'club_name': club_name,\n",
    "        'short_name': club_name,\n",
    "        'bg_color': '#000000',\n",
    "        'text_color': '#FFFFFF',\n",
    "        'all_variations': variation_list\n",
    "    }\n",
    "    \n",
    "    # â­ ××¤×ª×— ××•×¨×›×‘!\n",
    "    for variation in variation_list:\n",
    "        if variation:\n",
    "            key = (variation, int(league_id_int))\n",
    "            if key not in mapping:\n",
    "                mapping[key] = team_info\n",
    "\n",
    "print(f\"âœ“ Total teams: {teams_count}\")\n",
    "print(f\"âœ“ Total mapping entries: {len(mapping)}\\n\")\n",
    "\n",
    "# ×‘×“×™×§×ª ×œ×™×’×” 1\n",
    "league_1_keys = [k for k in mapping.keys() if k[1] == 1]\n",
    "print(f\"âœ“ League 1 has {len(league_1_keys)} variations\\n\")\n",
    "\n",
    "if league_1_keys:\n",
    "    print(\"First 10 variations from League 1:\")\n",
    "    for i, key in enumerate(league_1_keys[:10]):\n",
    "        var, lid = key\n",
    "        info = mapping[key]\n",
    "        print(f\"  {i+1}. ('{var}', {lid}) â†’ team_id={info['team_id']}, club_name='{info['club_name']}'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing specific lookups:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    ('××œ×™×¦×•×¨ ×™×‘× ×”', 1),\n",
    "    ('××›×‘×™ ×—×™×¤×” ×’×™× × ×ª×Ÿ', 1),\n",
    "    ('××›×‘×™ ××©×“×•×“', 1),\n",
    "    ('×”×¤×•×¢×œ ×—×™×¤×”', 1),\n",
    "    ('××›×‘×™ ×¨×—×•×‘×•×ª', 1),\n",
    "]\n",
    "\n",
    "for team_name, league_id in test_cases:\n",
    "    key = (team_name, league_id)\n",
    "    if key in mapping:\n",
    "        info = mapping[key]\n",
    "        print(f\"âœ“ ({team_name!r}, {league_id}) â†’ team_id={info['team_id']}\")\n",
    "    else:\n",
    "        print(f\"âœ— ({team_name!r}, {league_id}) NOT FOUND\")\n",
    "        \n",
    "        # ×—×™×¤×•×© ×‘××™×–×• ×œ×™×’×” ×–×” ×›×Ÿ × ××¦×\n",
    "        found_in = []\n",
    "        for k in mapping.keys():\n",
    "            if k[0] == team_name:\n",
    "                found_in.append(k[1])\n",
    "        if found_in:\n",
    "            print(f\"  Found in leagues: {found_in}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92043d-21a5-48bf-906f-95a1b5cf04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clear Python Cache - Jupyter Notebook\n",
    "======================================\n",
    "××—×™×§×ª ×›×œ ×§×‘×¦×™ ×”-cache ×©×œ Python\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_python_cache(root_dir='.'):\n",
    "    \"\"\"\n",
    "    ××—×™×§×ª ×›×œ ×§×‘×¦×™ ×”-cache ×©×œ Python\n",
    "    \n",
    "    Args:\n",
    "        root_dir: ×ª×™×§×™×™×” ×œ×”×ª×—×™×œ ××× ×” (×‘×¨×™×¨×ª ××—×“×œ: × ×•×›×—×™×ª)\n",
    "    \"\"\"\n",
    "    deleted_count = 0\n",
    "    \n",
    "    # ××—×™×§×ª ×ª×™×§×™×•×ª __pycache__\n",
    "    print(\"ğŸ” ××—×¤×© ×ª×™×§×™×•×ª __pycache__...\")\n",
    "    for pycache_dir in Path(root_dir).rglob('__pycache__'):\n",
    "        try:\n",
    "            shutil.rmtree(pycache_dir)\n",
    "            print(f\"   âœ… × ××—×§: {pycache_dir}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ×©×’×™××” ×‘××—×™×§×ª {pycache_dir}: {e}\")\n",
    "    \n",
    "    # ××—×™×§×ª ×§×‘×¦×™ .pyc\n",
    "    print(\"\\nğŸ” ××—×¤×© ×§×‘×¦×™ .pyc...\")\n",
    "    pyc_count = 0\n",
    "    for pyc_file in Path(root_dir).rglob('*.pyc'):\n",
    "        try:\n",
    "            pyc_file.unlink()\n",
    "            pyc_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ×©×’×™××” ×‘××—×™×§×ª {pyc_file}: {e}\")\n",
    "    \n",
    "    if pyc_count > 0:\n",
    "        print(f\"   âœ… × ××—×§×• {pyc_count} ×§×‘×¦×™ .pyc\")\n",
    "    \n",
    "    # ××—×™×§×ª ×§×‘×¦×™ .pyo\n",
    "    print(\"\\nğŸ” ××—×¤×© ×§×‘×¦×™ .pyo...\")\n",
    "    pyo_count = 0\n",
    "    for pyo_file in Path(root_dir).rglob('*.pyo'):\n",
    "        try:\n",
    "            pyo_file.unlink()\n",
    "            pyo_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ×©×’×™××” ×‘××—×™×§×ª {pyo_file}: {e}\")\n",
    "    \n",
    "    if pyo_count > 0:\n",
    "        print(f\"   âœ… × ××—×§×• {pyo_count} ×§×‘×¦×™ .pyo\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"âœ… ×¡×™×™×! × ××—×§×• {deleted_count} ×ª×™×§×™×•×ª __pycache__\")\n",
    "    print(f\"âœ… × ××—×§×• {pyc_count + pyo_count} ×§×‘×¦×™× ××§×•××¤×œ×™×\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# ×”×¨×¦×”\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"××—×™×§×ª Python Cache\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "clear_python_cache()\n",
    "\n",
    "print(\"\\nğŸ’¡ ×›×¢×ª ×”×¨×¥ ××ª ×”×§×•×“ ××—×“×©:\")\n",
    "print(\"   !python main.py --league 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e8f3a-59de-4b0c-aef4-f0a5f77884be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8416ff-9a70-4069-b04d-7d6e19735db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f7f97-6cb2-480f-8651-8441c12eef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Migration Script - CSV to Supabase\n",
    "===================================\n",
    "××¢×‘×™×¨ ××ª ×›×œ ×”× ×ª×•× ×™× ×”×§×™×™××™× ×-CSV ×œ-Supabase\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from supabase_uploader import SupabaseUploader\n",
    "from config import LEAGUES\n",
    "from utils import log_message\n",
    "\n",
    "class DataMigration:\n",
    "    \"\"\"××—×œ×§×” ×œ×”×¢×‘×¨×ª × ×ª×•× ×™×\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"××ª×—×•×œ\"\"\"\n",
    "        self.uploader = SupabaseUploader()\n",
    "        self.stats = {\n",
    "            'leagues': 0,\n",
    "            'teams': 0,\n",
    "            'players': 0,\n",
    "            'history': 0,\n",
    "            'games': 0,\n",
    "            'quarters': 0,\n",
    "            'player_stats': 0,\n",
    "            'team_stats': 0,\n",
    "            'player_averages': 0,\n",
    "            'team_averages': 0,\n",
    "            'opponent_averages': 0\n",
    "        }\n",
    "    \n",
    "    def migrate_all(self, league_ids=None):\n",
    "        \"\"\"\n",
    "        ×”×¢×‘×¨×ª ×›×œ ×”× ×ª×•× ×™×\n",
    "        \n",
    "        Args:\n",
    "            league_ids: ×¨×©×™××ª league_ids ×œ×”×¢×‘×¨×” (None = ×›×•×œ×)\n",
    "        \"\"\"\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"ğŸš€ STARTING MIGRATION TO SUPABASE\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        # Test connection\n",
    "        if not self.uploader.test_connection():\n",
    "            log_message(\"âŒ Cannot connect to Supabase!\")\n",
    "            return False\n",
    "        \n",
    "        # Get leagues to migrate\n",
    "        if league_ids is None:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if cfg.get('active', False)}\n",
    "        else:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if lid in league_ids}\n",
    "        \n",
    "        log_message(f\"ğŸ“Š Migrating {len(leagues_to_migrate)} leagues\")\n",
    "        log_message(\"\")\n",
    "        \n",
    "        # Migrate global data first\n",
    "        self._migrate_global_leagues()\n",
    "        self._migrate_global_teams()\n",
    "        self._migrate_global_players()\n",
    "        \n",
    "        # Migrate each league\n",
    "        for league_id, config in leagues_to_migrate.items():\n",
    "            log_message(\"=\"*60)\n",
    "            log_message(f\"[{config['code'].upper()}] Migrating: {config['name']}\")\n",
    "            log_message(\"=\"*60)\n",
    "            \n",
    "            self._migrate_league_data(league_id, config)\n",
    "        \n",
    "        # Print stats\n",
    "        self._print_stats()\n",
    "        \n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"âœ… MIGRATION COMPLETED!\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _migrate_global_leagues(self):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×˜×‘×œ×ª leagues\"\"\"\n",
    "        log_message(\"ğŸ“‹ Migrating leagues...\")\n",
    "        \n",
    "        leagues_file = 'data/leagues.csv'\n",
    "        if not os.path.exists(leagues_file):\n",
    "            log_message(\"âš ï¸  leagues.csv not found, creating from config\")\n",
    "            # Create from config\n",
    "            for league_id, config in LEAGUES.items():\n",
    "                if config.get('active', False):\n",
    "                    league_data = {\n",
    "                        'league_id': int(league_id),\n",
    "                        'name': config['name'],\n",
    "                        'name_en': config.get('name_en', ''),\n",
    "                        'country': config.get('country', 'Israel'),\n",
    "                        'season': config.get('season', ''),\n",
    "                        'url': config['url'],\n",
    "                        'is_active': True\n",
    "                    }\n",
    "                    self.uploader.upsert_league(league_data)\n",
    "                    self.stats['leagues'] += 1\n",
    "        else:\n",
    "            df = pd.read_csv(leagues_file, encoding='utf-8-sig')\n",
    "            for _, row in df.iterrows():\n",
    "                league_data = {\n",
    "                    'league_id': int(row['league_id']),\n",
    "                    'name': row['name'],\n",
    "                    'name_en': row.get('name_en', ''),\n",
    "                    'country': row.get('country', 'Israel'),\n",
    "                    'season': row.get('season', ''),\n",
    "                    'url': row.get('url', ''),\n",
    "                    'is_active': bool(row.get('is_active', False))\n",
    "                }\n",
    "                self.uploader.upsert_league(league_data)\n",
    "                self.stats['leagues'] += 1\n",
    "        \n",
    "        log_message(f\"âœ… Migrated {self.stats['leagues']} leagues\")\n",
    "    \n",
    "    def _migrate_global_teams(self):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×˜×‘×œ×ª teams\"\"\"\n",
    "        log_message(\"ğŸ€ Migrating teams...\")\n",
    "        \n",
    "        teams_file = 'data/teams.csv'\n",
    "        if not os.path.exists(teams_file):\n",
    "            log_message(\"âš ï¸  teams.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "        teams_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Skip invalid teams\n",
    "            if pd.isna(row['Team_ID']) or pd.isna(row['League_ID']):\n",
    "                continue\n",
    "            if int(row['League_ID']) == 0:\n",
    "                continue\n",
    "            \n",
    "            teams_data.append({\n",
    "                'team_id': int(row['Team_ID']),\n",
    "                'league_id': int(row['League_ID']),\n",
    "                'team_name': row['Team_Name'],\n",
    "                'short_name': row.get('short_name', row['Team_Name']),\n",
    "                'bg_color': row.get('bg_color', '#000000'),\n",
    "                'text_color': row.get('text_color', '#FFFFFF'),\n",
    "                'name_variations': row.get('name_variations', '')\n",
    "            })\n",
    "        \n",
    "        if teams_data:\n",
    "            self.uploader.upsert_teams(teams_data)\n",
    "            self.stats['teams'] = len(teams_data)\n",
    "        \n",
    "        log_message(f\"âœ… Migrated {self.stats['teams']} teams\")\n",
    "    \n",
    "    def _migrate_global_players(self):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×˜×‘×œ×ª players\"\"\"\n",
    "        log_message(\"ğŸ‘¤ Migrating players...\")\n",
    "        \n",
    "        players_file = 'data/players.csv'\n",
    "        if not os.path.exists(players_file):\n",
    "            log_message(\"âš ï¸  players.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(players_file, encoding='utf-8-sig')\n",
    "        players_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            if pd.isna(row['player_id']):\n",
    "                continue\n",
    "            \n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['name'],\n",
    "                'current_team_id': int(row['current_team_id']) if pd.notna(row['current_team_id']) else None,\n",
    "                'league_id': int(row['league_id']),\n",
    "                'date_of_birth': row['date_of_birth'] if pd.notna(row['date_of_birth']) else None,\n",
    "                'height': float(row['height']) if pd.notna(row['height']) else None,\n",
    "                'jersey_number': int(row['jersey_number']) if pd.notna(row['jersey_number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            self.stats['players'] = len(players_data)\n",
    "        \n",
    "        log_message(f\"âœ… Migrated {self.stats['players']} players\")\n",
    "    \n",
    "    def _migrate_league_data(self, league_id, config):\n",
    "        \"\"\"×”×¢×‘×¨×ª × ×ª×•× ×™ ×œ×™×’×” ×¡×¤×¦×™×¤×™×ª\"\"\"\n",
    "        league_code = config['code']\n",
    "        data_folder = config['data_folder']\n",
    "        games_folder = config['games_folder']\n",
    "        \n",
    "        # Player details (if not in global file)\n",
    "        self._migrate_player_details(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Player history\n",
    "        self._migrate_player_history(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Games\n",
    "        self._migrate_games(league_id, games_folder)\n",
    "        \n",
    "        # Game quarters\n",
    "        self._migrate_game_quarters(league_id, games_folder)\n",
    "        \n",
    "        # Game player stats\n",
    "        self._migrate_game_player_stats(league_id, games_folder)\n",
    "        \n",
    "        # Game team stats\n",
    "        self._migrate_game_team_stats(league_id, games_folder)\n",
    "        \n",
    "        # Averages\n",
    "        self._migrate_player_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_team_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_opponent_averages(league_id, league_code, data_folder)\n",
    "    \n",
    "    def _migrate_player_details(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¤×¨×˜×™ ×©×—×§× ×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_details.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“ Player details...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        players_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['Name'],\n",
    "                'current_team_id': int(row['team_id']) if pd.notna(row['team_id']) else None,\n",
    "                'league_id': int(league_id),\n",
    "                'date_of_birth': row['Date Of Birth'] if pd.notna(row['Date Of Birth']) else None,\n",
    "                'height': float(row['Height']) if pd.notna(row['Height']) else None,\n",
    "                'jersey_number': int(row['Number']) if pd.notna(row['Number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            log_message(f\"  âœ… {len(players_data)} players\")\n",
    "    \n",
    "    def _migrate_player_history(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×”×™×¡×˜×•×¨×™×™×ª ×©×—×§× ×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_history.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“š Player history...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        history_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            player_id = str(row['player_id'])\n",
    "            \n",
    "            # Extract seasons from columns\n",
    "            for col in df.columns:\n",
    "                if col not in ['player_id', 'Name', 'Current Team', 'team_id', 'league_id',\n",
    "                               'Date Of Birth', 'Height', 'Number']:\n",
    "                    season = col\n",
    "                    if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                        team_league = str(row[col])\n",
    "                        team_name = team_league.split('(')[0].strip() if '(' in team_league else team_league\n",
    "                        league_name = team_league.split('(')[1].replace(')', '').strip() if '(' in team_league else ''\n",
    "                        \n",
    "                        history_data.append({\n",
    "                            'player_id': player_id,\n",
    "                            'season': season,\n",
    "                            'team_name': team_name,\n",
    "                            'league_name': league_name,\n",
    "                            'league_id': int(league_id)\n",
    "                        })\n",
    "        \n",
    "        if history_data:\n",
    "            self.uploader.upsert_player_history(history_data)\n",
    "            self.stats['history'] += len(history_data)\n",
    "            log_message(f\"  âœ… {len(history_data)} history records\")\n",
    "    \n",
    "    def _migrate_games(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ××©×—×§×™×\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'games_schedule.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ† Games...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        games_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            games_data.append({\n",
    "                'game_id': str(row['gameid']),\n",
    "                'league_id': int(league_id),\n",
    "                'code': str(row['Code']) if pd.notna(row.get('Code')) else None,\n",
    "                'week_day': row.get('Week Day', ''),\n",
    "                'date': row['Date'] if pd.notna(row['Date']) else None,\n",
    "                'round': str(row['Round']) if pd.notna(row.get('Round')) else None,\n",
    "                'time': row.get('Time', ''),\n",
    "                'home_team': row['Home Team'],\n",
    "                'home_team_code': row.get('Home Team Code', ''),\n",
    "                'home_team_id': int(row['home_team_id']) if pd.notna(row.get('home_team_id')) else None,\n",
    "                'away_team': row['Away Team'],\n",
    "                'away_team_code': row.get('Away Team Code', ''),\n",
    "                'away_team_id': int(row['away_team_id']) if pd.notna(row.get('away_team_id')) else None,\n",
    "                'venue': row.get('Venue', ''),\n",
    "                'home_score': int(row['Home Score']) if pd.notna(row.get('Home Score')) and str(row['Home Score']).replace('.','').isdigit() else None,\n",
    "                'away_score': int(row['Away Score']) if pd.notna(row.get('Away Score')) and str(row['Away Score']).replace('.','').isdigit() else None,\n",
    "                'arena': row.get('Arena', ''),\n",
    "                'status': 'completed' if pd.notna(row.get('Home Score')) and str(row.get('Home Score')).strip() != '' else 'scheduled'\n",
    "            })\n",
    "        \n",
    "        if games_data:\n",
    "            self.uploader.upsert_games(games_data)\n",
    "            self.stats['games'] += len(games_data)\n",
    "            log_message(f\"  âœ… {len(games_data)} games\")\n",
    "    \n",
    "    def _migrate_game_quarters(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¨×‘×¢×™ ××©×—×§\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_quarters.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ”¢ Quarters...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        quarters_data = df.to_dict('records')\n",
    "        \n",
    "        if quarters_data:\n",
    "            self.uploader.upsert_game_quarters(quarters_data)\n",
    "            self.stats['quarters'] += len(quarters_data)\n",
    "            log_message(f\"  âœ… {len(quarters_data)} quarters\")\n",
    "    \n",
    "    def _migrate_game_player_stats(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×©×—×§×Ÿ ×‘××©×—×§\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_player_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“Š Player stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_player_stats(stats_data)\n",
    "            self.stats['player_stats'] += len(stats_data)\n",
    "            log_message(f\"  âœ… {len(stats_data)} player stats\")\n",
    "    \n",
    "    def _migrate_game_team_stats(self, league_id, games_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×§×‘×•×¦×” ×‘××©×—×§\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_team_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“ˆ Team stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_team_stats(stats_data)\n",
    "            self.stats['team_stats'] += len(stats_data)\n",
    "            log_message(f\"  âœ… {len(stats_data)} team stats\")\n",
    "    \n",
    "    def _migrate_player_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×××•×¦×¢×™ ×©×—×§× ×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“‰ Player averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_player_averages(avg_data)\n",
    "            self.stats['player_averages'] += len(avg_data)\n",
    "            log_message(f\"  âœ… {len(avg_data)} player averages\")\n",
    "    \n",
    "    def _migrate_team_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×××•×¦×¢×™ ×§×‘×•×¦×•×ª\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_team_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ“Š Team averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_team_averages(avg_data)\n",
    "            self.stats['team_averages'] += len(avg_data)\n",
    "            log_message(f\"  âœ… {len(avg_data)} team averages\")\n",
    "    \n",
    "    def _migrate_opponent_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"×”×¢×‘×¨×ª ×××•×¦×¢×™ ×™×¨×™×‘×™×\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_opponent_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  ğŸ›¡ï¸ Opponent averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_opponent_averages(avg_data)\n",
    "            self.stats['opponent_averages'] += len(avg_data)\n",
    "            log_message(f\"  âœ… {len(avg_data)} opponent averages\")\n",
    "    \n",
    "    def _print_stats(self):\n",
    "        \"\"\"×”×“×¤×¡×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª\"\"\"\n",
    "        log_message(\"\")\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"ğŸ“Š MIGRATION STATISTICS\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        for key, value in self.stats.items():\n",
    "            if value > 0:\n",
    "                log_message(f\"  {key.replace('_', ' ').title()}: {value:,}\")\n",
    "        \n",
    "        total = sum(self.stats.values())\n",
    "        log_message(\"\")\n",
    "        log_message(f\"  TOTAL RECORDS: {total:,}\")\n",
    "        log_message(\"=\"*60)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Migrate CSV data to Supabase')\n",
    "    parser.add_argument('--league', type=str, help='Specific league ID to migrate')\n",
    "    parser.add_argument('--all', action='store_true', help='Migrate all active leagues')\n",
    "    parser.add_argument('--test', action='store_true', help='Test connection only')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    migrator = DataMigration()\n",
    "    \n",
    "    if args.test:\n",
    "        # Test connection\n",
    "        log_message(\"ğŸ§ª Testing Supabase connection...\")\n",
    "        if migrator.uploader.test_connection():\n",
    "            log_message(\"âœ… Connection successful!\")\n",
    "        else:\n",
    "            log_message(\"âŒ Connection failed!\")\n",
    "    \n",
    "    elif args.all or args.league:\n",
    "        # Migrate data\n",
    "        league_ids = [args.league] if args.league else None\n",
    "        migrator.migrate_all(league_ids)\n",
    "    \n",
    "    else:\n",
    "        # Show help\n",
    "        parser.print_help()\n",
    "        print(\"\\nExamples:\")\n",
    "        print(\"  python migrate_to_supabase.py --test              # Test connection\")\n",
    "        print(\"  python migrate_to_supabase.py --all               # Migrate all leagues\")\n",
    "        print(\"  python migrate_to_supabase.py --league 1          # Migrate league 1 only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7cb82-7e89-4803-96bb-f10f18190e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import ×¡×¤×¦×™×¤×™\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Cell 2: ×˜×¢×Ÿ credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = os.getenv('SUPABASE_URL')\n",
    "key = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "# Cell 3: ×‘×“×•×§ ×©×”× ×§×™×™××™×\n",
    "if not url or not key:\n",
    "    print(\"âŒ ×—×¡×¨ URL ××• KEY!\")\n",
    "else:\n",
    "    print(f\"âœ… URL: {url}\")\n",
    "    print(f\"âœ… Key ××ª×—×™×œ ×‘: {key[:20]}...\")\n",
    "\n",
    "# Cell 4: ×—×™×‘×•×¨ ×¢× try-except\n",
    "try:\n",
    "    supabase: Client = create_client(url, key)\n",
    "    print(\"âœ… Connected!\")\n",
    "    \n",
    "    # ×‘×“×™×§×”\n",
    "    result = supabase.table('leagues').select(\"count\", count=\"exact\").execute()\n",
    "    print(f\"âœ… ×™×© {result.count} ×œ×™×’×•×ª\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
