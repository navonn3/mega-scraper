{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989d16ed-a86a-468b-bf49-4a8211c6a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 12:02:41] \n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] BASKETBALL SCRAPER STARTED\n",
      "[2025-10-14 12:02:41] Time: 2025-10-14 12:02:41\n",
      "[2025-10-14 12:02:41] Mode: QUICK\n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] Found 1 active leagues\n",
      "[2025-10-14 12:02:41] \n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] PROCESSING LEAGUE: 10 - ליגת Winner סל\n",
      "[2025-10-14 12:02:41] ================================================================================\n",
      "[2025-10-14 12:02:41] ✅ Loaded global team mapping: 93 teams, 316 name variations\n",
      "[2025-10-14 12:02:41]    League 1: 16 teams, 49 variations\n",
      "[2025-10-14 12:02:41] [WINNER] Initialized WinnerScraper with 5 boards\n",
      "[2025-10-14 12:02:41] [WINNER] ============================================================\n",
      "[2025-10-14 12:02:41] [WINNER] STARTING SCRAPE: ליגת Winner סל\n",
      "[2025-10-14 12:02:41] [WINNER] Mode: QUICK\n",
      "[2025-10-14 12:02:41] [WINNER] ============================================================\n",
      "[2025-10-14 12:02:41] [WINNER] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-14 12:02:41] [WINNER] Loaded 17 existing players\n",
      "[2025-10-14 12:02:41] [WINNER] Processing team web_id: 1111\n",
      "[2025-10-14 12:02:43] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:02:43] [WINNER]   ↳ Scraping ג'ורדןפלויד\n",
      "[2025-10-14 12:02:46] [WINNER]   ↳ Scraping גילבני\n",
      "[2025-10-14 12:02:50] [WINNER]   ↳ Scraping סמאג'טיל\n",
      "[2025-10-14 12:02:54] [WINNER]   ↳ Scraping אורשילר\n",
      "[2025-10-14 12:02:57] [WINNER]   ↳ Scraping ז'ורדאןניקוליס\n",
      "[2025-10-14 12:03:02] [WINNER]   ↳ Scraping עמיתאלון\n",
      "[2025-10-14 12:03:06] [WINNER]   ↳ Scraping עידןחכמון\n",
      "[2025-10-14 12:03:10] [WINNER]   ↳ Scraping יונתןאטיאס\n",
      "[2025-10-14 12:03:14] [WINNER]   ↳ Scraping דריוןאטקינס\n",
      "[2025-10-14 12:03:18] [WINNER]   ↳ Scraping רונאלדוסגו\n",
      "[2025-10-14 12:03:22] [WINNER]   ↳ Scraping אדםאריאל\n",
      "[2025-10-14 12:03:26] [WINNER]   ↳ Scraping ג'ורדןכהן\n",
      "[2025-10-14 12:03:30] [WINNER]   ↳ Scraping דרוקרופורד\n",
      "[2025-10-14 12:03:34] [WINNER]   ↳ Scraping אייזיהאייזנדורף\n",
      "[2025-10-14 12:03:38] [WINNER]   ↳ Scraping אמירדנון\n",
      "[2025-10-14 12:03:42] [WINNER]   ↳ Scraping רוייצקן\n",
      "[2025-10-14 12:03:46] [WINNER]   ↳ Scraping אורגטה\n",
      "[2025-10-14 12:03:50] [WINNER] Processing team web_id: 1110\n",
      "[2025-10-14 12:03:53] [WINNER]   Found 19 players\n",
      "[2025-10-14 12:03:53] [WINNER]   ↳ Scraping ג'ונתןמוטלי\n",
      "[2025-10-14 12:03:57] [WINNER]   ↳ Scraping כריסג'ונס\n",
      "[2025-10-14 12:04:01] [WINNER]   ↳ Scraping אנטוניובלייקני\n",
      "[2025-10-14 12:04:05] [WINNER]   ↳ Scraping אלייז'הבראיינט\n",
      "[2025-10-14 12:04:09] [WINNER]   ↳ Scraping סנדיכהן\n",
      "[2025-10-14 12:04:13] [WINNER]   ↳ Scraping איתישגב\n",
      "[2025-10-14 12:04:18] [WINNER]   ↳ Scraping גיאפלטין\n",
      "[2025-10-14 12:04:22] [WINNER]   ↳ Scraping ברטימור\n",
      "[2025-10-14 12:04:26] [WINNER]   ↳ Scraping טיילראניס\n",
      "[2025-10-14 12:04:30] [WINNER]   ↳ Scraping עוזבלייזר\n",
      "[2025-10-14 12:04:34] [WINNER]   ↳ Scraping קוליןמלקולם\n",
      "[2025-10-14 12:04:38] [WINNER]   ↳ Scraping טאיאודיאסי\n",
      "[2025-10-14 12:04:42] [WINNER]   ↳ Scraping ואסיליהמיציץ'\n",
      "[2025-10-14 12:04:46] [WINNER]   ↳ Scraping אישוויינרייט\n",
      "[2025-10-14 12:04:50] [WINNER]   ↳ Scraping דןאוטורו\n",
      "[2025-10-14 12:04:54] [WINNER]   ↳ Scraping יםמדר\n",
      "[2025-10-14 12:04:57] [WINNER]   ↳ Scraping תומרגינת\n",
      "[2025-10-14 12:05:02] [WINNER]   ↳ Scraping ברונוקאבוקלו\n",
      "[2025-10-14 12:05:06] [WINNER]   ↳ Scraping יפתחזיו\n",
      "[2025-10-14 12:05:10] [WINNER] Processing team web_id: 1112\n",
      "[2025-10-14 12:05:13] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:05:13] [WINNER]   ↳ Scraping ג'סטיןסמית'\n",
      "[2025-10-14 12:05:17] [WINNER]   ↳ Scraping ג'ארדהארפר\n",
      "[2025-10-14 12:05:21] [WINNER]   ↳ Scraping קאדיןקרינגטון\n",
      "[2025-10-14 12:05:26] [WINNER]   ↳ Scraping רניבלגה\n",
      "[2025-10-14 12:05:29] [WINNER]   ↳ Scraping אוסטיןוויילי\n",
      "[2025-10-14 12:05:33] [WINNER]   ↳ Scraping רועיפריצקי\n",
      "[2025-10-14 12:05:37] [WINNER]   ↳ Scraping גבריאלצאצאשוילי\n",
      "[2025-10-14 12:05:41] [WINNER]   ↳ Scraping קשיוסווינסטון\n",
      "[2025-10-14 12:05:45] [WINNER]   ↳ Scraping יובלבן עטר\n",
      "[2025-10-14 12:05:49] [WINNER]   ↳ Scraping רועיהובר\n",
      "[2025-10-14 12:05:54] [WINNER]   ↳ Scraping נירפרי\n",
      "[2025-10-14 12:05:57] [WINNER]   ↳ Scraping נמרודלוי\n",
      "[2025-10-14 12:06:02] [WINNER]   ↳ Scraping יריןחסון\n",
      "[2025-10-14 12:06:05] [WINNER]   ↳ Scraping אנתונילאמב\n",
      "[2025-10-14 12:06:10] [WINNER]   ↳ Scraping דמיטרוסקפינצב\n",
      "[2025-10-14 12:06:13] [WINNER]   ↳ Scraping יובלזוסמן\n",
      "[2025-10-14 12:06:17] [WINNER]   ↳ Scraping ג'וזאיה־ג'ורדןג'יימס\n",
      "[2025-10-14 12:06:21] [WINNER] Processing team web_id: 1109\n",
      "[2025-10-14 12:06:24] [WINNER]   Found 16 players\n",
      "[2025-10-14 12:06:24] [WINNER]   ↳ Scraping ג'יילןהורד\n",
      "[2025-10-14 12:06:28] [WINNER]   ↳ Scraping ג'ימיקלארק\n",
      "[2025-10-14 12:06:32] [WINNER]   ↳ Scraping מרסיוסנטוס\n",
      "[2025-10-14 12:06:35] [WINNER]   ↳ Scraping גורלביא\n",
      "[2025-10-14 12:06:40] [WINNER]   ↳ Scraping אורושטריפונוביץ'\n",
      "[2025-10-14 12:06:43] [WINNER]   ↳ Scraping אורןסהר\n",
      "[2025-10-14 12:06:47] [WINNER]   ↳ Scraping לוניווקר\n",
      "[2025-10-14 12:06:51] [WINNER]   ↳ Scraping רומןסורקין\n",
      "[2025-10-14 12:06:55] [WINNER]   ↳ Scraping אושייבריסט\n",
      "[2025-10-14 12:06:59] [WINNER]   ↳ Scraping ווילריימן\n",
      "[2025-10-14 12:07:08] [WINNER]   ↳ Scraping ג'וןדיברתולומיאו\n",
      "[2025-10-14 12:07:12] [WINNER]   ↳ Scraping תמירגולד\n",
      "[2025-10-14 12:07:16] [WINNER]   ↳ Scraping קליפורדאומורי\n",
      "[2025-10-14 12:07:20] [WINNER]   ↳ Scraping ג'ףדאוטין ג'וניור\n",
      "[2025-10-14 12:07:23] [WINNER]   ↳ Scraping טי.ג'ייליף\n",
      "[2025-10-14 12:07:27] [WINNER]   ↳ Scraping תמירבלאט\n",
      "[2025-10-14 12:07:32] [WINNER] Processing team web_id: 1119\n",
      "[2025-10-14 12:07:35] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:07:35] [WINNER]   ↳ Scraping סיריללנג'יוויין\n",
      "[2025-10-14 12:07:39] [WINNER]   ↳ Scraping ג'יילןבלייקס\n",
      "[2025-10-14 12:07:43] [WINNER]   ↳ Scraping בןאלטשולר\n",
      "[2025-10-14 12:07:47] [WINNER]   ↳ Scraping דיאנדרהגולסטון\n",
      "[2025-10-14 12:07:51] [WINNER]   ↳ Scraping אוריגורדון\n",
      "[2025-10-14 12:07:55] [WINNER]   ↳ Scraping שחרעמיר\n",
      "[2025-10-14 12:07:59] [WINNER]   ↳ Scraping אייזיהבראון\n",
      "[2025-10-14 12:08:03] [WINNER]   ↳ Scraping שחרדורון\n",
      "[2025-10-14 12:08:07] [WINNER]   ↳ Scraping יהלמלמד\n",
      "[2025-10-14 12:08:11] [WINNER]   ↳ Scraping רוןבן דר\n",
      "[2025-10-14 12:08:15] [WINNER]   ↳ Scraping ברייליאלברט\n",
      "[2025-10-14 12:08:19] [WINNER]   ↳ Scraping רזאדם\n",
      "[2025-10-14 12:08:23] [WINNER]   ↳ Scraping יואבאלמישלי\n",
      "[2025-10-14 12:08:27] [WINNER]   ↳ Scraping אורחן\n",
      "[2025-10-14 12:08:31] [WINNER]   ↳ Scraping איתיפלדמן\n",
      "[2025-10-14 12:08:35] [WINNER]   ↳ Scraping נועםאביבי\n",
      "[2025-10-14 12:08:39] [WINNER]   ↳ Scraping סםאלג'יקי\n",
      "[2025-10-14 12:08:43] [WINNER] Processing team web_id: 1124\n",
      "[2025-10-14 12:08:46] [WINNER]   Found 13 players\n",
      "[2025-10-14 12:08:46] [WINNER]   ↳ Scraping וויליוורקמן\n",
      "[2025-10-14 12:08:50] [WINNER]   ↳ Scraping ג'ייסטאונסנד\n",
      "[2025-10-14 12:08:54] [WINNER]   ↳ Scraping חסןדיארה\n",
      "[2025-10-14 12:08:58] [WINNER]   ↳ Scraping חןגיא\n",
      "[2025-10-14 12:09:01] [WINNER]   ↳ Scraping לנארדפרימן\n",
      "[2025-10-14 12:09:05] [WINNER]   ↳ Scraping אפיטיומקין\n",
      "[2025-10-14 12:09:09] [WINNER]   ↳ Scraping חואקיןשוכמן\n",
      "[2025-10-14 12:09:14] [WINNER]   ↳ Scraping נייטלאשבסקי\n",
      "[2025-10-14 12:09:18] [WINNER]   ↳ Scraping יותםחנוכי\n",
      "[2025-10-14 12:09:23] [WINNER]   ↳ Scraping אלכסלדר\n",
      "[2025-10-14 12:09:26] [WINNER]   ↳ Scraping טייריסרדפורד\n",
      "[2025-10-14 12:09:30] [WINNER]   ↳ Scraping אורןבוקובזה\n",
      "[2025-10-14 12:09:34] [WINNER]   ↳ Scraping אריאלעצמון\n",
      "[2025-10-14 12:09:38] [WINNER] Processing team web_id: 1122\n",
      "[2025-10-14 12:09:41] [WINNER]   Found 15 players\n",
      "[2025-10-14 12:09:41] [WINNER]   ↳ Scraping קיילראדוארדס\n",
      "[2025-10-14 12:09:45] [WINNER]   ↳ Scraping יובלכוכבי\n",
      "[2025-10-14 12:09:49] [WINNER]   ↳ Scraping קיידןשדריק\n",
      "[2025-10-14 12:09:58] [WINNER]   ↳ Scraping זיווייסמן\n",
      "[2025-10-14 12:10:03] [WINNER]   ↳ Scraping רפימנקו\n",
      "[2025-10-14 12:10:07] [WINNER]   ↳ Scraping אלייז'הצ'יילדס\n",
      "[2025-10-14 12:10:11] [WINNER]   ↳ Scraping ארוןווילר\n",
      "[2025-10-14 12:10:15] [WINNER]   ↳ Scraping יונתןמלול\n",
      "[2025-10-14 12:10:19] [WINNER]   ↳ Scraping רועינציה\n",
      "[2025-10-14 12:10:23] [WINNER]   ↳ Scraping קמרוןהנרי\n",
      "[2025-10-14 12:10:27] [WINNER]   ↳ Scraping גילנויוביץ'\n",
      "[2025-10-14 12:10:31] [WINNER]   ↳ Scraping לוקאסגולדנברג\n",
      "[2025-10-14 12:10:35] [WINNER]   ↳ Scraping עדירינסקי\n",
      "[2025-10-14 12:10:39] [WINNER]   ↳ Scraping ניבמשגב\n",
      "[2025-10-14 12:10:44] [WINNER]   ↳ Scraping שחרארז\n",
      "[2025-10-14 12:10:47] [WINNER] Processing team web_id: 2109\n",
      "[2025-10-14 12:10:50] [WINNER]   Found 19 players\n",
      "[2025-10-14 12:10:50] [WINNER]   ↳ Scraping סמאג'הכריסטון\n",
      "[2025-10-14 12:10:54] [WINNER]   ↳ Scraping טרייקלווין\n",
      "[2025-10-14 12:10:58] [WINNER]   ↳ Scraping שאקביוקנן\n",
      "[2025-10-14 12:11:02] [WINNER]   ↳ Scraping קליאלספיר\n",
      "[2025-10-14 12:11:06] [WINNER]   ↳ Scraping דוריסהר\n",
      "[2025-10-14 12:11:10] [WINNER]   ↳ Scraping אביבהנקין\n",
      "[2025-10-14 12:11:14] [WINNER]   ↳ Scraping ניקירפפורט\n",
      "[2025-10-14 12:11:18] [WINNER]   ↳ Scraping גלשטרנברג\n",
      "[2025-10-14 12:11:22] [WINNER]   ↳ Scraping אריאלסלע\n",
      "[2025-10-14 12:11:26] [WINNER]   ↳ Scraping עידושבת\n",
      "[2025-10-14 12:11:30] [WINNER]   ↳ Scraping בוריסבוגוסלבסקי\n",
      "[2025-10-14 12:11:34] [WINNER]   ↳ Scraping מקסיםפילוננקו\n",
      "[2025-10-14 12:11:38] [WINNER]   ↳ Scraping ניסיםתורג'מן\n",
      "[2025-10-14 12:11:42] [WINNER]   ↳ Scraping קיילבבון\n",
      "[2025-10-14 12:11:46] [WINNER]   ↳ Scraping אוטיספרייז'ר\n",
      "[2025-10-14 12:11:50] [WINNER]   ↳ Scraping ניקוקרבאצ'ו\n",
      "[2025-10-14 12:11:54] [WINNER]   ↳ Scraping שגיבדביר\n",
      "[2025-10-14 12:11:58] [WINNER]   ↳ Scraping איתןינאי\n",
      "[2025-10-14 12:12:01] [WINNER]   ↳ Scraping אורקורנליוס\n",
      "[2025-10-14 12:12:06] [WINNER] Processing team web_id: 1120\n",
      "[2025-10-14 12:12:08] [WINNER]   Found 16 players\n",
      "[2025-10-14 12:12:08] [WINNER]   ↳ Scraping נווהבן שמן\n",
      "[2025-10-14 12:12:12] [WINNER]   ↳ Scraping בנימיןזבינסקי\n",
      "[2025-10-14 12:12:16] [WINNER]   ↳ Scraping אקיליויינינג\n",
      "[2025-10-14 12:12:20] [WINNER]   ↳ Scraping עופרישטרית\n",
      "[2025-10-14 12:12:24] [WINNER]   ↳ Scraping טאריןטוד\n",
      "[2025-10-14 12:12:28] [WINNER]   ↳ Scraping אריאלאייזיק\n",
      "[2025-10-14 12:12:32] [WINNER]   ↳ Scraping נטעסגל\n",
      "[2025-10-14 12:12:36] [WINNER]   ↳ Scraping סי.ג'ייאלבי\n",
      "[2025-10-14 12:12:40] [WINNER]   ↳ Scraping עמיתגרשון\n",
      "[2025-10-14 12:12:44] [WINNER]   ↳ Scraping ברנדוןאנג'ל\n",
      "[2025-10-14 12:12:48] [WINNER]   ↳ Scraping סםאיוריו\n",
      "[2025-10-14 12:12:52] [WINNER]   ↳ Scraping יואבויטלם\n",
      "[2025-10-14 12:12:56] [WINNER]   ↳ Scraping מייקלפוסטר ג'וניור\n",
      "[2025-10-14 12:13:00] [WINNER]   ↳ Scraping מתןפוגל\n",
      "[2025-10-14 12:13:04] [WINNER]   ↳ Scraping קודידמפס\n",
      "[2025-10-14 12:13:08] [WINNER]   ↳ Scraping ג'יילןטייט\n",
      "[2025-10-14 12:13:12] [WINNER] Processing team web_id: 1116\n",
      "[2025-10-14 12:13:14] [WINNER]   Found 16 players\n",
      "[2025-10-14 12:13:14] [WINNER]   ↳ Scraping עתירטלמור\n",
      "[2025-10-14 12:13:19] [WINNER]   ↳ Scraping דמיוןלי\n",
      "[2025-10-14 12:13:23] [WINNER]   ↳ Scraping קייסישפרד\n",
      "[2025-10-14 12:13:27] [WINNER]   ↳ Scraping טי.ג'ייקליין\n",
      "[2025-10-14 12:13:31] [WINNER]   ↳ Scraping שגיאקמחג'י\n",
      "[2025-10-14 12:13:35] [WINNER]   ↳ Scraping טייריקסמית'\n",
      "[2025-10-14 12:13:38] [WINNER]   ↳ Scraping איתימושקוביץ\n",
      "[2025-10-14 12:13:43] [WINNER]   ↳ Scraping לוטןאמסלם\n",
      "[2025-10-14 12:13:47] [WINNER]   ↳ Scraping ספנסרוייס\n",
      "[2025-10-14 12:13:51] [WINNER]   ↳ Scraping אנדרהקורבלו\n",
      "[2025-10-14 12:13:55] [WINNER]   ↳ Scraping אורטפירו\n",
      "[2025-10-14 12:13:59] [WINNER]   ↳ Scraping אדראהרוני\n",
      "[2025-10-14 12:14:03] [WINNER]   ↳ Scraping אייזיהווייטהד\n",
      "[2025-10-14 12:14:06] [WINNER]   ↳ Scraping אופקשמר\n",
      "[2025-10-14 12:14:10] [WINNER]   ↳ Scraping אודוקהאזובוקיי\n",
      "[2025-10-14 12:14:14] [WINNER]   ↳ Scraping דזירודריגז\n",
      "[2025-10-14 12:14:18] [WINNER] Processing team web_id: 1114\n",
      "[2025-10-14 12:14:21] [WINNER]   Found 14 players\n",
      "[2025-10-14 12:14:21] [WINNER]   ↳ Scraping תומראסייג\n",
      "[2025-10-14 12:14:25] [WINNER]   ↳ Scraping ג'מיהניל\n",
      "[2025-10-14 12:14:29] [WINNER]   ↳ Scraping נואהלוק\n",
      "[2025-10-14 12:14:33] [WINNER]   ↳ Scraping אדםשרבקוב\n",
      "[2025-10-14 12:14:37] [WINNER]   ↳ Scraping איתיזלוטולוב\n",
      "[2025-10-14 12:14:41] [WINNER]   ↳ Scraping בודייום\n",
      "[2025-10-14 12:14:45] [WINNER]   ↳ Scraping אילידולינסקי\n",
      "[2025-10-14 12:14:49] [WINNER]   ↳ Scraping איתימובסוביץ\n",
      "[2025-10-14 12:14:52] [WINNER]   ↳ Scraping גלבייטנר\n",
      "[2025-10-14 12:14:56] [WINNER]   ↳ Scraping רוןדנדיקר\n",
      "[2025-10-14 12:15:00] [WINNER]   ↳ Scraping מאליקהול\n",
      "[2025-10-14 12:15:04] [WINNER]   ↳ Scraping דריוסהאנה\n",
      "[2025-10-14 12:15:08] [WINNER]   ↳ Scraping דורענבי\n",
      "[2025-10-14 12:15:12] [WINNER]   ↳ Scraping יובלהוכשטטר\n",
      "[2025-10-14 12:15:16] [WINNER] Processing team web_id: 1123\n",
      "[2025-10-14 12:15:19] [WINNER]   Found 13 players\n",
      "[2025-10-14 12:15:19] [WINNER]   ↳ Scraping קווהגרנט\n",
      "[2025-10-14 12:15:23] [WINNER]   ↳ Scraping בןאייזנהארט\n",
      "[2025-10-14 12:15:27] [WINNER]   ↳ Scraping אמיןסטיבנס\n",
      "[2025-10-14 12:15:31] [WINNER]   ↳ Scraping קאלילאחמד\n",
      "[2025-10-14 12:15:36] [WINNER]   ↳ Scraping מרכוסוויליאמס\n",
      "[2025-10-14 12:15:40] [WINNER]   ↳ Scraping גולןגוט\n",
      "[2025-10-14 12:15:44] [WINNER]   ↳ Scraping אוריזהבי\n",
      "[2025-10-14 12:15:48] [WINNER]   ↳ Scraping עומרגוטסמן\n",
      "[2025-10-14 12:15:52] [WINNER]   ↳ Scraping ירוןגולדמן\n",
      "[2025-10-14 12:15:56] [WINNER]   ↳ Scraping ג'ושפרוינד\n",
      "[2025-10-14 12:16:00] [WINNER]   ↳ Scraping ליאורברמן\n",
      "[2025-10-14 12:16:04] [WINNER]   ↳ Scraping ג'יי.ג'ייקפלן\n",
      "[2025-10-14 12:16:08] [WINNER]   ↳ Scraping די.ג'ייברנס\n",
      "[2025-10-14 12:16:12] [WINNER] Processing team web_id: 1113\n",
      "[2025-10-14 12:16:15] [WINNER]   Found 18 players\n",
      "[2025-10-14 12:16:15] [WINNER]   ↳ Scraping ג'ורדןבון\n",
      "[2025-10-14 12:16:19] [WINNER]   ↳ Scraping ליעדשמואל\n",
      "[2025-10-14 12:16:23] [WINNER]   ↳ Scraping דמיוןרוסר\n",
      "[2025-10-14 12:16:27] [WINNER]   ↳ Scraping דריקוולטון ג'וניור\n",
      "[2025-10-14 12:16:31] [WINNER]   ↳ Scraping טודווית'רס\n",
      "[2025-10-14 12:16:34] [WINNER]   ↳ Scraping אקסביירמנפורד\n",
      "[2025-10-14 12:16:38] [WINNER]   ↳ Scraping חסןמרטין\n",
      "[2025-10-14 12:16:42] [WINNER]   ↳ Scraping איילסולטן\n",
      "[2025-10-14 12:16:46] [WINNER]   ↳ Scraping מייקלאליאסזדה\n",
      "[2025-10-14 12:16:50] [WINNER]   ↳ Scraping ליאורקררה\n",
      "[2025-10-14 12:16:53] [WINNER]   ↳ Scraping נסיירברוקס\n",
      "[2025-10-14 12:16:57] [WINNER]   ↳ Scraping רועירוזנברג\n",
      "[2025-10-14 12:17:01] [WINNER]   ↳ Scraping אופקניסמזון\n",
      "[2025-10-14 12:17:05] [WINNER]   ↳ Scraping עידןזלמנסון\n",
      "[2025-10-14 12:17:09] [WINNER]   ↳ Scraping נתנאלארצי\n",
      "[2025-10-14 12:17:13] [WINNER]   ↳ Scraping יאירקרביץ\n",
      "[2025-10-14 12:17:17] [WINNER]   ↳ Scraping יותםמנשה\n",
      "[2025-10-14 12:17:21] [WINNER]   ↳ Scraping ג'ורדןמקריי\n",
      "[2025-10-14 12:17:25] [WINNER] Processing team web_id: 1118\n",
      "[2025-10-14 12:17:28] [WINNER]   Found 17 players\n",
      "[2025-10-14 12:17:28] ❌ CRITICAL ERROR in league '10': cannot safely cast non-equivalent object to int64\n",
      "[2025-10-14 12:17:28] Traceback (most recent call last):\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\integer.py\", line 53, in _safe_cast\n",
      "    return values.astype(dtype, casting=\"safe\", copy=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\main.py\", line 178, in scrape_league\n",
      "    success = scraper.run()\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\scrapers\\base_scraper.py\", line 85, in run\n",
      "    if not self._update_player_details():\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\scrapers\\winner.py\", line 164, in _update_player_details\n",
      "    save_to_csv(merged_df, details_file)\n",
      "  File \"C:\\Users\\navon\\OneDrive\\Documents\\Python\\BasketballApp\\utils\\helpers.py\", line 43, in save_to_csv\n",
      "    df[col] = df[col].astype('Int64')\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\", line 6665, in astype\n",
      "    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 449, in astype\n",
      "    return self.apply(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 363, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 784, in astype\n",
      "    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 237, in astype_array_safe\n",
      "    new_values = astype_array(values, dtype, copy=copy)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 182, in astype_array\n",
      "    values = _astype_nansafe(values, dtype, copy=copy)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 80, in _astype_nansafe\n",
      "    return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 153, in _from_sequence\n",
      "    values, mask = cls._coerce_to_array(scalars, dtype=dtype, copy=copy)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\numeric.py\", line 272, in _coerce_to_array\n",
      "    values, mask, _, _ = _coerce_to_data_and_mask(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\numeric.py\", line 229, in _coerce_to_data_and_mask\n",
      "    values = dtype_cls._safe_cast(values, dtype, copy=False)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\integer.py\", line 59, in _safe_cast\n",
      "    raise TypeError(\n",
      "TypeError: cannot safely cast non-equivalent object to int64\n",
      "\n",
      "[2025-10-14 12:17:28] \n",
      "[2025-10-14 12:17:28] ================================================================================\n",
      "[2025-10-14 12:17:28] UPDATING GLOBAL FILES\n",
      "[2025-10-14 12:17:28] ================================================================================\n",
      "[2025-10-14 12:17:28] Updating global leagues.csv...\n",
      "[2025-10-14 12:17:28] ✅ Global leagues.csv updated: 11 leagues\n",
      "[2025-10-14 12:17:28] Checking global teams.csv...\n",
      "[2025-10-14 12:17:28] ✅ Global teams.csv verified: 93 teams\n",
      "[2025-10-14 12:17:28] Updating global players.csv...\n",
      "[2025-10-14 12:17:29] ✅ Global players.csv updated: 800 players\n",
      "[2025-10-14 12:17:29] \n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] SCRAPING SUMMARY\n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] ✅ Successful: 0 leagues\n",
      "[2025-10-14 12:17:29] ❌ Failed: 1 leagues\n",
      "[2025-10-14 12:17:29]    ✗ League 10: ליגת Winner סל\n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] \n",
      "[2025-10-14 12:17:29] ================================================================================\n",
      "[2025-10-14 12:17:29] BASKETBALL SCRAPER FINISHED\n",
      "[2025-10-14 12:17:29] Time: 2025-10-14 12:17:29\n",
      "[2025-10-14 12:17:29] ================================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['main.py']\n",
    "from main import main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd8431-5e94-405a-9e5c-428464ec79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Print a filtered tree: folders + CSV files only, plus a flat CSV list with sizes.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Settings ----------\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5          # עומק מקסימלי לסריקה\n",
    "SHOW_HIDDEN = False    # להציג תיקיות/קבצים שמתחילים בנקודה?\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_hidden(p: Path) -> bool:\n",
    "    return any(part.startswith(\".\") for part in p.parts)\n",
    "\n",
    "def within_depth(root: Path, p: Path, max_depth: int) -> bool:\n",
    "    return len(p.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def dir_children(root: Path, d: Path):\n",
    "    \"\"\"Return (dirs, csvs) immediate children (not recursive), filtered.\"\"\"\n",
    "    dirs, csvs = [], []\n",
    "    for child in sorted(d.iterdir()):\n",
    "        if not SHOW_HIDDEN and is_hidden(child):\n",
    "            continue\n",
    "        if child.is_dir():\n",
    "            dirs.append(child)\n",
    "        elif child.is_file() and child.suffix.lower() == \".csv\":\n",
    "            csvs.append(child)\n",
    "    return dirs, csvs\n",
    "\n",
    "def contains_relevant_descendants(root: Path, d: Path, max_depth: int) -> bool:\n",
    "    \"\"\"Does this directory (recursively) contain any dir/csv within max_depth?\"\"\"\n",
    "    for p in d.rglob(\"*\"):\n",
    "        # stop at max depth\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p):\n",
    "            continue\n",
    "        if p.is_dir() or (p.is_file() and p.suffix.lower() == \".csv\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def print_tree(root: Path, max_depth: int):\n",
    "    print(f\"ROOT: {root}\\n\")\n",
    "    # We print the root name then recurse\n",
    "    print(f\"📁 {root.name}\")\n",
    "    def recurse(d: Path, prefix: str, depth: int):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        dirs, csvs = dir_children(root, d)\n",
    "\n",
    "        # Filter out directories that do not contain any relevant descendants within depth\n",
    "        pruned_dirs = []\n",
    "        for sd in dirs:\n",
    "            if within_depth(root, sd, max_depth) and (contains_relevant_descendants(root, sd, max_depth) or list(dir_children(root, sd)[1])):\n",
    "                pruned_dirs.append(sd)\n",
    "\n",
    "        # Build combined list: dirs first, then CSVs\n",
    "        items = pruned_dirs + csvs\n",
    "        for i, item in enumerate(items):\n",
    "            is_last = (i == len(items) - 1)\n",
    "            branch = \"└─ \" if is_last else \"├─ \"\n",
    "            if item.is_dir():\n",
    "                print(prefix + branch + \"📁 \" + item.name)\n",
    "                new_prefix = prefix + (\"   \" if is_last else \"│  \")\n",
    "                recurse(item, new_prefix, depth + 1)\n",
    "            else:\n",
    "                print(prefix + branch + \"📄 \" + item.name)\n",
    "    recurse(root, \"\", 0)\n",
    "\n",
    "def human_size(num_bytes: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if num_bytes < 1024.0:\n",
    "            return f\"{num_bytes:.0f} {unit}\"\n",
    "        num_bytes /= 1024.0\n",
    "    return f\"{num_bytes:.0f} PB\"\n",
    "\n",
    "def list_csvs(root: Path, max_depth: int):\n",
    "    csvs = []\n",
    "    for p in sorted(root.rglob(\"*.csv\")):\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p.relative_to(root)):\n",
    "            continue\n",
    "        if p.is_file():\n",
    "            try:\n",
    "                size = human_size(p.stat().st_size)\n",
    "            except Exception:\n",
    "                size = \"?\"\n",
    "            csvs.append((p, size))\n",
    "    return csvs\n",
    "\n",
    "# ---------- Run ----------\n",
    "print(\"=== DIRECTORY TREE (folders + CSV only) ===\")\n",
    "print_tree(ROOT, MAX_DEPTH)\n",
    "\n",
    "print(\"\\n=== CSV FILES (flat list) ===\")\n",
    "csv_list = list_csvs(ROOT, MAX_DEPTH)\n",
    "if not csv_list:\n",
    "    print(\"(no CSV files found)\")\n",
    "else:\n",
    "    for p, size in csv_list:\n",
    "        rel = p.relative_to(ROOT)\n",
    "        print(f\"- {rel}  •  {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ef0d3-b0fc-4f69-bf27-3e1404521fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# מציג לכל קובץ CSV את הנתיב, כותרות ושורה אחת ראשונה בפורמט CSV\n",
    "# מדלג על תיקיות מוסתרות ו-.ipynb_checkpoints, עם מפריד ברור בין קבצים\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "def is_hidden_or_checkpoint(path: Path) -> bool:\n",
    "    \"\"\"בודק האם קובץ או תיקייה מוסתרים או בתוך .ipynb_checkpoints\"\"\"\n",
    "    return any(\n",
    "        part.startswith(\".\") or \"ipynb_checkpoints\" in part\n",
    "        for part in path.parts\n",
    "    )\n",
    "\n",
    "def within_depth(root: Path, path: Path, max_depth: int) -> bool:\n",
    "    \"\"\"בודק האם הקובץ בעומק מותר\"\"\"\n",
    "    return len(path.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def print_csv_previews(root: Path, max_depth: int):\n",
    "    csv_files = sorted(root.rglob(\"*.csv\"))\n",
    "    for csv_path in csv_files:\n",
    "        if is_hidden_or_checkpoint(csv_path.relative_to(root)):\n",
    "            continue\n",
    "        if not within_depth(root, csv_path, max_depth):\n",
    "            continue\n",
    "\n",
    "        rel_path = str(csv_path.relative_to(root))\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=\"utf-8-sig\", nrows=1)\n",
    "        except Exception:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding=\"utf-8\", nrows=1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n{rel_path}\")\n",
    "                print(f\"Error reading file: {e}\")\n",
    "                print(\"=\" * 60)\n",
    "                continue\n",
    "\n",
    "        print(f\"\\n{rel_path}\")  # כותרת עם נתיב הקובץ\n",
    "        print(\",\".join(df.columns))  # שורת כותרות\n",
    "        if not df.empty:\n",
    "            print(\",\".join(map(str, df.iloc[0].tolist())))  # שורה אחת מהנתונים\n",
    "        print(\"=\" * 60)  # מפריד בין קבצים\n",
    "\n",
    "    print(\"\\n✅ הסתיים בהצלחה.\")\n",
    "\n",
    "# הפעלה\n",
    "print_csv_previews(ROOT, MAX_DEPTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3b1a8-1da3-4e21-a9f5-5d01beb98c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c709c0-3108-403e-bfbe-598ca510c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Test Live Mapping\n",
    "=================\n",
    "בדיקה עם הקוד המעודכן בדיוק\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "teams_file = \"data/teams.csv\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Loading with COMPOSITE KEY: (variation, league_id)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "print(f\"✓ Loaded {len(df)} rows\\n\")\n",
    "\n",
    "# זיהוי עמודות\n",
    "columns = df.columns.tolist()\n",
    "column_mapping = {\n",
    "    'team_id': ['Team_ID', 'team_id', 'TeamID'],\n",
    "    'league_id': ['League_ID', 'league_id', 'LeagueID'],\n",
    "    'club_name': ['Team_Name', 'club_name', 'team_name', 'name'],\n",
    "    'variations': ['name_variations', 'variations', 'Variations'],\n",
    "}\n",
    "\n",
    "actual_columns = {}\n",
    "for key, possible_names in column_mapping.items():\n",
    "    for name in possible_names:\n",
    "        if name in columns:\n",
    "            actual_columns[key] = name\n",
    "            break\n",
    "\n",
    "print(f\"Detected columns: {actual_columns}\\n\")\n",
    "\n",
    "# יצירת מיפוי עם מפתח מורכב\n",
    "mapping = {}\n",
    "teams_count = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    team_id = row[actual_columns['team_id']] if 'team_id' in actual_columns else None\n",
    "    league_id = row[actual_columns['league_id']] if 'league_id' in actual_columns else None\n",
    "    club_name = row[actual_columns['club_name']] if 'club_name' in actual_columns else None\n",
    "    \n",
    "    if pd.isna(team_id) or pd.isna(league_id) or pd.isna(club_name):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        league_id_int = int(league_id)\n",
    "        if league_id_int == 0:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    teams_count += 1\n",
    "    \n",
    "    variations = row[actual_columns['variations']] if 'variations' in actual_columns else club_name\n",
    "    \n",
    "    club_name = html.unescape(str(club_name)).strip()\n",
    "    variations_str = html.unescape(str(variations)).strip()\n",
    "    \n",
    "    variation_list = []\n",
    "    for v in variations_str.split('|'):\n",
    "        v_clean = html.unescape(v.strip())\n",
    "        if v_clean:\n",
    "            variation_list.append(v_clean)\n",
    "    \n",
    "    team_info = {\n",
    "        'team_id': int(team_id),\n",
    "        'league_id': int(league_id_int),\n",
    "        'club_name': club_name,\n",
    "        'short_name': club_name,\n",
    "        'bg_color': '#000000',\n",
    "        'text_color': '#FFFFFF',\n",
    "        'all_variations': variation_list\n",
    "    }\n",
    "    \n",
    "    # ⭐ מפתח מורכב!\n",
    "    for variation in variation_list:\n",
    "        if variation:\n",
    "            key = (variation, int(league_id_int))\n",
    "            if key not in mapping:\n",
    "                mapping[key] = team_info\n",
    "\n",
    "print(f\"✓ Total teams: {teams_count}\")\n",
    "print(f\"✓ Total mapping entries: {len(mapping)}\\n\")\n",
    "\n",
    "# בדיקת ליגה 1\n",
    "league_1_keys = [k for k in mapping.keys() if k[1] == 1]\n",
    "print(f\"✓ League 1 has {len(league_1_keys)} variations\\n\")\n",
    "\n",
    "if league_1_keys:\n",
    "    print(\"First 10 variations from League 1:\")\n",
    "    for i, key in enumerate(league_1_keys[:10]):\n",
    "        var, lid = key\n",
    "        info = mapping[key]\n",
    "        print(f\"  {i+1}. ('{var}', {lid}) → team_id={info['team_id']}, club_name='{info['club_name']}'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing specific lookups:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    ('אליצור יבנה', 1),\n",
    "    ('מכבי חיפה גיא נתן', 1),\n",
    "    ('מכבי אשדוד', 1),\n",
    "    ('הפועל חיפה', 1),\n",
    "    ('מכבי רחובות', 1),\n",
    "]\n",
    "\n",
    "for team_name, league_id in test_cases:\n",
    "    key = (team_name, league_id)\n",
    "    if key in mapping:\n",
    "        info = mapping[key]\n",
    "        print(f\"✓ ({team_name!r}, {league_id}) → team_id={info['team_id']}\")\n",
    "    else:\n",
    "        print(f\"✗ ({team_name!r}, {league_id}) NOT FOUND\")\n",
    "        \n",
    "        # חיפוש באיזו ליגה זה כן נמצא\n",
    "        found_in = []\n",
    "        for k in mapping.keys():\n",
    "            if k[0] == team_name:\n",
    "                found_in.append(k[1])\n",
    "        if found_in:\n",
    "            print(f\"  Found in leagues: {found_in}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92043d-21a5-48bf-906f-95a1b5cf04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clear Python Cache - Jupyter Notebook\n",
    "======================================\n",
    "מחיקת כל קבצי ה-cache של Python\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_python_cache(root_dir='.'):\n",
    "    \"\"\"\n",
    "    מחיקת כל קבצי ה-cache של Python\n",
    "    \n",
    "    Args:\n",
    "        root_dir: תיקייה להתחיל ממנה (ברירת מחדל: נוכחית)\n",
    "    \"\"\"\n",
    "    deleted_count = 0\n",
    "    \n",
    "    # מחיקת תיקיות __pycache__\n",
    "    print(\"🔍 מחפש תיקיות __pycache__...\")\n",
    "    for pycache_dir in Path(root_dir).rglob('__pycache__'):\n",
    "        try:\n",
    "            shutil.rmtree(pycache_dir)\n",
    "            print(f\"   ✅ נמחק: {pycache_dir}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ שגיאה במחיקת {pycache_dir}: {e}\")\n",
    "    \n",
    "    # מחיקת קבצי .pyc\n",
    "    print(\"\\n🔍 מחפש קבצי .pyc...\")\n",
    "    pyc_count = 0\n",
    "    for pyc_file in Path(root_dir).rglob('*.pyc'):\n",
    "        try:\n",
    "            pyc_file.unlink()\n",
    "            pyc_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ שגיאה במחיקת {pyc_file}: {e}\")\n",
    "    \n",
    "    if pyc_count > 0:\n",
    "        print(f\"   ✅ נמחקו {pyc_count} קבצי .pyc\")\n",
    "    \n",
    "    # מחיקת קבצי .pyo\n",
    "    print(\"\\n🔍 מחפש קבצי .pyo...\")\n",
    "    pyo_count = 0\n",
    "    for pyo_file in Path(root_dir).rglob('*.pyo'):\n",
    "        try:\n",
    "            pyo_file.unlink()\n",
    "            pyo_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ שגיאה במחיקת {pyo_file}: {e}\")\n",
    "    \n",
    "    if pyo_count > 0:\n",
    "        print(f\"   ✅ נמחקו {pyo_count} קבצי .pyo\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"✅ סיים! נמחקו {deleted_count} תיקיות __pycache__\")\n",
    "    print(f\"✅ נמחקו {pyc_count + pyo_count} קבצים מקומפלים\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# הרצה\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"מחיקת Python Cache\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "clear_python_cache()\n",
    "\n",
    "print(\"\\n💡 כעת הרץ את הקוד מחדש:\")\n",
    "print(\"   !python main.py --league 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e8f3a-59de-4b0c-aef4-f0a5f77884be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8416ff-9a70-4069-b04d-7d6e19735db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f7f97-6cb2-480f-8651-8441c12eef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Migration Script - CSV to Supabase\n",
    "===================================\n",
    "מעביר את כל הנתונים הקיימים מ-CSV ל-Supabase\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from supabase_uploader import SupabaseUploader\n",
    "from config import LEAGUES\n",
    "from utils import log_message\n",
    "\n",
    "class DataMigration:\n",
    "    \"\"\"מחלקה להעברת נתונים\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"אתחול\"\"\"\n",
    "        self.uploader = SupabaseUploader()\n",
    "        self.stats = {\n",
    "            'leagues': 0,\n",
    "            'teams': 0,\n",
    "            'players': 0,\n",
    "            'history': 0,\n",
    "            'games': 0,\n",
    "            'quarters': 0,\n",
    "            'player_stats': 0,\n",
    "            'team_stats': 0,\n",
    "            'player_averages': 0,\n",
    "            'team_averages': 0,\n",
    "            'opponent_averages': 0\n",
    "        }\n",
    "    \n",
    "    def migrate_all(self, league_ids=None):\n",
    "        \"\"\"\n",
    "        העברת כל הנתונים\n",
    "        \n",
    "        Args:\n",
    "            league_ids: רשימת league_ids להעברה (None = כולם)\n",
    "        \"\"\"\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"🚀 STARTING MIGRATION TO SUPABASE\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        # Test connection\n",
    "        if not self.uploader.test_connection():\n",
    "            log_message(\"❌ Cannot connect to Supabase!\")\n",
    "            return False\n",
    "        \n",
    "        # Get leagues to migrate\n",
    "        if league_ids is None:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if cfg.get('active', False)}\n",
    "        else:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if lid in league_ids}\n",
    "        \n",
    "        log_message(f\"📊 Migrating {len(leagues_to_migrate)} leagues\")\n",
    "        log_message(\"\")\n",
    "        \n",
    "        # Migrate global data first\n",
    "        self._migrate_global_leagues()\n",
    "        self._migrate_global_teams()\n",
    "        self._migrate_global_players()\n",
    "        \n",
    "        # Migrate each league\n",
    "        for league_id, config in leagues_to_migrate.items():\n",
    "            log_message(\"=\"*60)\n",
    "            log_message(f\"[{config['code'].upper()}] Migrating: {config['name']}\")\n",
    "            log_message(\"=\"*60)\n",
    "            \n",
    "            self._migrate_league_data(league_id, config)\n",
    "        \n",
    "        # Print stats\n",
    "        self._print_stats()\n",
    "        \n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"✅ MIGRATION COMPLETED!\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _migrate_global_leagues(self):\n",
    "        \"\"\"העברת טבלת leagues\"\"\"\n",
    "        log_message(\"📋 Migrating leagues...\")\n",
    "        \n",
    "        leagues_file = 'data/leagues.csv'\n",
    "        if not os.path.exists(leagues_file):\n",
    "            log_message(\"⚠️  leagues.csv not found, creating from config\")\n",
    "            # Create from config\n",
    "            for league_id, config in LEAGUES.items():\n",
    "                if config.get('active', False):\n",
    "                    league_data = {\n",
    "                        'league_id': int(league_id),\n",
    "                        'name': config['name'],\n",
    "                        'name_en': config.get('name_en', ''),\n",
    "                        'country': config.get('country', 'Israel'),\n",
    "                        'season': config.get('season', ''),\n",
    "                        'url': config['url'],\n",
    "                        'is_active': True\n",
    "                    }\n",
    "                    self.uploader.upsert_league(league_data)\n",
    "                    self.stats['leagues'] += 1\n",
    "        else:\n",
    "            df = pd.read_csv(leagues_file, encoding='utf-8-sig')\n",
    "            for _, row in df.iterrows():\n",
    "                league_data = {\n",
    "                    'league_id': int(row['league_id']),\n",
    "                    'name': row['name'],\n",
    "                    'name_en': row.get('name_en', ''),\n",
    "                    'country': row.get('country', 'Israel'),\n",
    "                    'season': row.get('season', ''),\n",
    "                    'url': row.get('url', ''),\n",
    "                    'is_active': bool(row.get('is_active', False))\n",
    "                }\n",
    "                self.uploader.upsert_league(league_data)\n",
    "                self.stats['leagues'] += 1\n",
    "        \n",
    "        log_message(f\"✅ Migrated {self.stats['leagues']} leagues\")\n",
    "    \n",
    "    def _migrate_global_teams(self):\n",
    "        \"\"\"העברת טבלת teams\"\"\"\n",
    "        log_message(\"🏀 Migrating teams...\")\n",
    "        \n",
    "        teams_file = 'data/teams.csv'\n",
    "        if not os.path.exists(teams_file):\n",
    "            log_message(\"⚠️  teams.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "        teams_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Skip invalid teams\n",
    "            if pd.isna(row['Team_ID']) or pd.isna(row['League_ID']):\n",
    "                continue\n",
    "            if int(row['League_ID']) == 0:\n",
    "                continue\n",
    "            \n",
    "            teams_data.append({\n",
    "                'team_id': int(row['Team_ID']),\n",
    "                'league_id': int(row['League_ID']),\n",
    "                'team_name': row['Team_Name'],\n",
    "                'short_name': row.get('short_name', row['Team_Name']),\n",
    "                'bg_color': row.get('bg_color', '#000000'),\n",
    "                'text_color': row.get('text_color', '#FFFFFF'),\n",
    "                'name_variations': row.get('name_variations', '')\n",
    "            })\n",
    "        \n",
    "        if teams_data:\n",
    "            self.uploader.upsert_teams(teams_data)\n",
    "            self.stats['teams'] = len(teams_data)\n",
    "        \n",
    "        log_message(f\"✅ Migrated {self.stats['teams']} teams\")\n",
    "    \n",
    "    def _migrate_global_players(self):\n",
    "        \"\"\"העברת טבלת players\"\"\"\n",
    "        log_message(\"👤 Migrating players...\")\n",
    "        \n",
    "        players_file = 'data/players.csv'\n",
    "        if not os.path.exists(players_file):\n",
    "            log_message(\"⚠️  players.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(players_file, encoding='utf-8-sig')\n",
    "        players_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            if pd.isna(row['player_id']):\n",
    "                continue\n",
    "            \n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['name'],\n",
    "                'current_team_id': int(row['current_team_id']) if pd.notna(row['current_team_id']) else None,\n",
    "                'league_id': int(row['league_id']),\n",
    "                'date_of_birth': row['date_of_birth'] if pd.notna(row['date_of_birth']) else None,\n",
    "                'height': float(row['height']) if pd.notna(row['height']) else None,\n",
    "                'jersey_number': int(row['jersey_number']) if pd.notna(row['jersey_number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            self.stats['players'] = len(players_data)\n",
    "        \n",
    "        log_message(f\"✅ Migrated {self.stats['players']} players\")\n",
    "    \n",
    "    def _migrate_league_data(self, league_id, config):\n",
    "        \"\"\"העברת נתוני ליגה ספציפית\"\"\"\n",
    "        league_code = config['code']\n",
    "        data_folder = config['data_folder']\n",
    "        games_folder = config['games_folder']\n",
    "        \n",
    "        # Player details (if not in global file)\n",
    "        self._migrate_player_details(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Player history\n",
    "        self._migrate_player_history(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Games\n",
    "        self._migrate_games(league_id, games_folder)\n",
    "        \n",
    "        # Game quarters\n",
    "        self._migrate_game_quarters(league_id, games_folder)\n",
    "        \n",
    "        # Game player stats\n",
    "        self._migrate_game_player_stats(league_id, games_folder)\n",
    "        \n",
    "        # Game team stats\n",
    "        self._migrate_game_team_stats(league_id, games_folder)\n",
    "        \n",
    "        # Averages\n",
    "        self._migrate_player_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_team_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_opponent_averages(league_id, league_code, data_folder)\n",
    "    \n",
    "    def _migrate_player_details(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת פרטי שחקנים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_details.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📝 Player details...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        players_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['Name'],\n",
    "                'current_team_id': int(row['team_id']) if pd.notna(row['team_id']) else None,\n",
    "                'league_id': int(league_id),\n",
    "                'date_of_birth': row['Date Of Birth'] if pd.notna(row['Date Of Birth']) else None,\n",
    "                'height': float(row['Height']) if pd.notna(row['Height']) else None,\n",
    "                'jersey_number': int(row['Number']) if pd.notna(row['Number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            log_message(f\"  ✅ {len(players_data)} players\")\n",
    "    \n",
    "    def _migrate_player_history(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת היסטוריית שחקנים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_history.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📚 Player history...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        history_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            player_id = str(row['player_id'])\n",
    "            \n",
    "            # Extract seasons from columns\n",
    "            for col in df.columns:\n",
    "                if col not in ['player_id', 'Name', 'Current Team', 'team_id', 'league_id',\n",
    "                               'Date Of Birth', 'Height', 'Number']:\n",
    "                    season = col\n",
    "                    if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                        team_league = str(row[col])\n",
    "                        team_name = team_league.split('(')[0].strip() if '(' in team_league else team_league\n",
    "                        league_name = team_league.split('(')[1].replace(')', '').strip() if '(' in team_league else ''\n",
    "                        \n",
    "                        history_data.append({\n",
    "                            'player_id': player_id,\n",
    "                            'season': season,\n",
    "                            'team_name': team_name,\n",
    "                            'league_name': league_name,\n",
    "                            'league_id': int(league_id)\n",
    "                        })\n",
    "        \n",
    "        if history_data:\n",
    "            self.uploader.upsert_player_history(history_data)\n",
    "            self.stats['history'] += len(history_data)\n",
    "            log_message(f\"  ✅ {len(history_data)} history records\")\n",
    "    \n",
    "    def _migrate_games(self, league_id, games_folder):\n",
    "        \"\"\"העברת משחקים\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'games_schedule.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  🏆 Games...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        games_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            games_data.append({\n",
    "                'game_id': str(row['gameid']),\n",
    "                'league_id': int(league_id),\n",
    "                'code': str(row['Code']) if pd.notna(row.get('Code')) else None,\n",
    "                'week_day': row.get('Week Day', ''),\n",
    "                'date': row['Date'] if pd.notna(row['Date']) else None,\n",
    "                'round': str(row['Round']) if pd.notna(row.get('Round')) else None,\n",
    "                'time': row.get('Time', ''),\n",
    "                'home_team': row['Home Team'],\n",
    "                'home_team_code': row.get('Home Team Code', ''),\n",
    "                'home_team_id': int(row['home_team_id']) if pd.notna(row.get('home_team_id')) else None,\n",
    "                'away_team': row['Away Team'],\n",
    "                'away_team_code': row.get('Away Team Code', ''),\n",
    "                'away_team_id': int(row['away_team_id']) if pd.notna(row.get('away_team_id')) else None,\n",
    "                'venue': row.get('Venue', ''),\n",
    "                'home_score': int(row['Home Score']) if pd.notna(row.get('Home Score')) and str(row['Home Score']).replace('.','').isdigit() else None,\n",
    "                'away_score': int(row['Away Score']) if pd.notna(row.get('Away Score')) and str(row['Away Score']).replace('.','').isdigit() else None,\n",
    "                'arena': row.get('Arena', ''),\n",
    "                'status': 'completed' if pd.notna(row.get('Home Score')) and str(row.get('Home Score')).strip() != '' else 'scheduled'\n",
    "            })\n",
    "        \n",
    "        if games_data:\n",
    "            self.uploader.upsert_games(games_data)\n",
    "            self.stats['games'] += len(games_data)\n",
    "            log_message(f\"  ✅ {len(games_data)} games\")\n",
    "    \n",
    "    def _migrate_game_quarters(self, league_id, games_folder):\n",
    "        \"\"\"העברת רבעי משחק\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_quarters.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  🔢 Quarters...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        quarters_data = df.to_dict('records')\n",
    "        \n",
    "        if quarters_data:\n",
    "            self.uploader.upsert_game_quarters(quarters_data)\n",
    "            self.stats['quarters'] += len(quarters_data)\n",
    "            log_message(f\"  ✅ {len(quarters_data)} quarters\")\n",
    "    \n",
    "    def _migrate_game_player_stats(self, league_id, games_folder):\n",
    "        \"\"\"העברת סטטיסטיקות שחקן במשחק\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_player_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📊 Player stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_player_stats(stats_data)\n",
    "            self.stats['player_stats'] += len(stats_data)\n",
    "            log_message(f\"  ✅ {len(stats_data)} player stats\")\n",
    "    \n",
    "    def _migrate_game_team_stats(self, league_id, games_folder):\n",
    "        \"\"\"העברת סטטיסטיקות קבוצה במשחק\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_team_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📈 Team stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_team_stats(stats_data)\n",
    "            self.stats['team_stats'] += len(stats_data)\n",
    "            log_message(f\"  ✅ {len(stats_data)} team stats\")\n",
    "    \n",
    "    def _migrate_player_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת ממוצעי שחקנים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📉 Player averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_player_averages(avg_data)\n",
    "            self.stats['player_averages'] += len(avg_data)\n",
    "            log_message(f\"  ✅ {len(avg_data)} player averages\")\n",
    "    \n",
    "    def _migrate_team_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת ממוצעי קבוצות\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_team_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📊 Team averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_team_averages(avg_data)\n",
    "            self.stats['team_averages'] += len(avg_data)\n",
    "            log_message(f\"  ✅ {len(avg_data)} team averages\")\n",
    "    \n",
    "    def _migrate_opponent_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת ממוצעי יריבים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_opponent_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  🛡️ Opponent averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_opponent_averages(avg_data)\n",
    "            self.stats['opponent_averages'] += len(avg_data)\n",
    "            log_message(f\"  ✅ {len(avg_data)} opponent averages\")\n",
    "    \n",
    "    def _print_stats(self):\n",
    "        \"\"\"הדפסת סטטיסטיקות\"\"\"\n",
    "        log_message(\"\")\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"📊 MIGRATION STATISTICS\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        for key, value in self.stats.items():\n",
    "            if value > 0:\n",
    "                log_message(f\"  {key.replace('_', ' ').title()}: {value:,}\")\n",
    "        \n",
    "        total = sum(self.stats.values())\n",
    "        log_message(\"\")\n",
    "        log_message(f\"  TOTAL RECORDS: {total:,}\")\n",
    "        log_message(\"=\"*60)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Migrate CSV data to Supabase')\n",
    "    parser.add_argument('--league', type=str, help='Specific league ID to migrate')\n",
    "    parser.add_argument('--all', action='store_true', help='Migrate all active leagues')\n",
    "    parser.add_argument('--test', action='store_true', help='Test connection only')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    migrator = DataMigration()\n",
    "    \n",
    "    if args.test:\n",
    "        # Test connection\n",
    "        log_message(\"🧪 Testing Supabase connection...\")\n",
    "        if migrator.uploader.test_connection():\n",
    "            log_message(\"✅ Connection successful!\")\n",
    "        else:\n",
    "            log_message(\"❌ Connection failed!\")\n",
    "    \n",
    "    elif args.all or args.league:\n",
    "        # Migrate data\n",
    "        league_ids = [args.league] if args.league else None\n",
    "        migrator.migrate_all(league_ids)\n",
    "    \n",
    "    else:\n",
    "        # Show help\n",
    "        parser.print_help()\n",
    "        print(\"\\nExamples:\")\n",
    "        print(\"  python migrate_to_supabase.py --test              # Test connection\")\n",
    "        print(\"  python migrate_to_supabase.py --all               # Migrate all leagues\")\n",
    "        print(\"  python migrate_to_supabase.py --league 1          # Migrate league 1 only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7cb82-7e89-4803-96bb-f10f18190e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import ספציפי\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Cell 2: טען credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = os.getenv('SUPABASE_URL')\n",
    "key = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "# Cell 3: בדוק שהם קיימים\n",
    "if not url or not key:\n",
    "    print(\"❌ חסר URL או KEY!\")\n",
    "else:\n",
    "    print(f\"✅ URL: {url}\")\n",
    "    print(f\"✅ Key מתחיל ב: {key[:20]}...\")\n",
    "\n",
    "# Cell 4: חיבור עם try-except\n",
    "try:\n",
    "    supabase: Client = create_client(url, key)\n",
    "    print(\"✅ Connected!\")\n",
    "    \n",
    "    # בדיקה\n",
    "    result = supabase.table('leagues').select(\"count\", count=\"exact\").execute()\n",
    "    print(f\"✅ יש {result.count} ליגות\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
