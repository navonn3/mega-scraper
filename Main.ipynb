{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989d16ed-a86a-468b-bf49-4a8211c6a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-13 03:33:55] \n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:55] BASKETBALL SCRAPER STARTED\n",
      "[2025-10-13 03:33:55] Time: 2025-10-13 03:33:55\n",
      "[2025-10-13 03:33:55] Mode: QUICK\n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:55] Found 6 active leagues\n",
      "[2025-10-13 03:33:55] \n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:55] PROCESSING LEAGUE: 1 - ליגה לאומית\n",
      "[2025-10-13 03:33:55] ================================================================================\n",
      "[2025-10-13 03:33:56] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:33:56]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:33:56] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:33:56] [LEUMIT] STARTING SCRAPE: ליגה לאומית\n",
      "[2025-10-13 03:33:56] [LEUMIT] Mode: QUICK\n",
      "[2025-10-13 03:33:56] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:33:56] [LEUMIT] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:33:57] [LEUMIT] Found 176 players\n",
      "[2025-10-13 03:33:58]    ⚠️  No mapping found for: 'הפועל גליל עליון' in league 1\n",
      "[2025-10-13 03:33:58]    ⚠️  No mapping found for: 'מכבי רמת גן קבוצת כנ' in league 1\n",
      "[2025-10-13 03:33:58]    ⚠️  No mapping found for: 'הפועל ירושלים ב.יהב' in league 1\n",
      "[2025-10-13 03:33:58] [LEUMIT] ✅ Player details updated\n",
      "[2025-10-13 03:33:58] [LEUMIT]    Total: 176 | New: 0 | Updated: 0 | Skipped: 176\n",
      "[2025-10-13 03:33:58] [LEUMIT] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:01] [LEUMIT] ✅ Downloaded schedule: 240 games\n",
      "[2025-10-13 03:34:01] [LEUMIT]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:01] [LEUMIT]    ✅ Normalized 240 games\n",
      "[2025-10-13 03:34:01] [LEUMIT] ✅ Games schedule normalized and saved\n",
      "[2025-10-13 03:34:01] [LEUMIT]    Found 6 completed games\n",
      "[2025-10-13 03:34:01] [LEUMIT]    Scraping 6 new games\n",
      "[2025-10-13 03:34:01] [LEUMIT]    [1/6] Game 1_741605: אליצור יבנה vs מכבי חיפה\n",
      "[2025-10-13 03:34:03] [LEUMIT]       Final Score: אליצור יבנה 87 - 76 מכבי חיפה\n",
      "[2025-10-13 03:34:04] [LEUMIT]       DEBUG: final_scores = {'home_team': 'אליצור יבנה', 'away_team': 'מכבי חיפה', 'home_score': 87, 'away_score': 76}\n",
      "[2025-10-13 03:34:05] [LEUMIT]    [2/6] Game 1_741600: הפועל נהריה vs א.ס רמה\"ש\n",
      "[2025-10-13 03:34:06] [LEUMIT]       DEBUG: final_scores = {}\n",
      "[2025-10-13 03:34:06] [LEUMIT]    ⚠️  No stats found for game 1_741600\n",
      "[2025-10-13 03:34:07] [LEUMIT]    [3/6] Game 1_741602: הפועל חיפה vs אליצור שומרון\n",
      "[2025-10-13 03:34:09] [LEUMIT]       Final Score: הפועל חיפה 79 - 74 אליצור שומרון\n",
      "[2025-10-13 03:34:09] [LEUMIT]       DEBUG: final_scores = {'home_team': 'הפועל חיפה', 'away_team': 'אליצור שומרון', 'home_score': 79, 'away_score': 74}\n",
      "[2025-10-13 03:34:10] [LEUMIT]    [4/6] Game 1_741606: מכבי אשדוד vs א.ס. עירוני אשקלון\n",
      "[2025-10-13 03:34:12] [LEUMIT]       Final Score: מכבי אשדוד 92 - 88 א.ס. עירוני אשקלון\n",
      "[2025-10-13 03:34:12] [LEUMIT]       DEBUG: final_scores = {'home_team': 'מכבי אשדוד', 'away_team': 'א.ס. עירוני אשקלון', 'home_score': 92, 'away_score': 88}\n",
      "[2025-10-13 03:34:13] [LEUMIT]    [5/6] Game 1_741607: הפועל מגדל העמק/יזרעאל vs מ.ס צפת\n",
      "[2025-10-13 03:34:15] [LEUMIT]       Final Score: הפועל מגדל העמק/יזרעאל 90 - 80 מ.ס צפת\n",
      "[2025-10-13 03:34:15] [LEUMIT]       DEBUG: final_scores = {'home_team': 'הפועל מגדל העמק/יזרעאל', 'away_team': 'מ.ס צפת', 'home_score': 90, 'away_score': 80}\n",
      "[2025-10-13 03:34:17] [LEUMIT]    [6/6] Game 1_741601: מ.כ עוטף דרום vs מכבי קרית גת\n",
      "[2025-10-13 03:34:21] [LEUMIT]       Final Score: מ.כ עוטף דרום 94 - 87 מכבי קרית גת\n",
      "[2025-10-13 03:34:21] [LEUMIT]       DEBUG: final_scores = {'home_team': 'מ.כ עוטף דרום', 'away_team': 'מכבי קרית גת', 'home_score': 94, 'away_score': 87}\n",
      "[2025-10-13 03:34:22] [LEUMIT]    ✅ Saved quarters: 40 records\n",
      "[2025-10-13 03:34:22] [LEUMIT]    ✅ Saved player stats: 96 records\n",
      "[2025-10-13 03:34:22] [LEUMIT]    ✅ Saved team stats: 10 records\n",
      "[2025-10-13 03:34:22] [LEUMIT]    DEBUG: About to update 5 scores\n",
      "[2025-10-13 03:34:22] [LEUMIT]    Updating game scores in schedule...\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: אליצור יבנה 87-76 מכבי חיפה\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: הפועל חיפה 79-74 אליצור שומרון\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: מכבי אשדוד 92-88 א.ס. עירוני אשקלון\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: הפועל מגדל העמק/יזרעאל 90-80 מ.ס צפת\n",
      "[2025-10-13 03:34:22] [LEUMIT]       Updated: מ.כ עוטף דרום 94-87 מכבי קרית גת\n",
      "[2025-10-13 03:34:22] [LEUMIT]    ✅ Updated 5 game scores in schedule\n",
      "[2025-10-13 03:34:22] [LEUMIT] ✅ Game stats updated: 6 new games\n",
      "[2025-10-13 03:34:22] [LEUMIT] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:22] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:22]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:22] [LEUMIT] ✅ Player averages: 96 players\n",
      "[2025-10-13 03:34:23] [LEUMIT] ✅ Team averages: 10 teams\n",
      "[2025-10-13 03:34:23] [LEUMIT] ✅ Opponent averages: 10 teams\n",
      "[2025-10-13 03:34:23] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:34:23] [LEUMIT] ✅ SCRAPE COMPLETED SUCCESSFULLY\n",
      "[2025-10-13 03:34:23] [LEUMIT] ============================================================\n",
      "[2025-10-13 03:34:23] \n",
      "[2025-10-13 03:34:23] ================================================================================\n",
      "[2025-10-13 03:34:23] PROCESSING LEAGUE: 4 - נוער על צפון\n",
      "[2025-10-13 03:34:23] ================================================================================\n",
      "[2025-10-13 03:34:23] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:23]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:23] [U18-NORTH] ============================================================\n",
      "[2025-10-13 03:34:23] [U18-NORTH] STARTING SCRAPE: נוער על צפון\n",
      "[2025-10-13 03:34:23] [U18-NORTH] Mode: QUICK\n",
      "[2025-10-13 03:34:23] [U18-NORTH] ============================================================\n",
      "[2025-10-13 03:34:23] [U18-NORTH] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:24] [U18-NORTH] Found 152 players\n",
      "[2025-10-13 03:34:24] [U18-NORTH] ✅ Player details updated\n",
      "[2025-10-13 03:34:24] [U18-NORTH]    Total: 152 | New: 0 | Updated: 0 | Skipped: 152\n",
      "[2025-10-13 03:34:24] [U18-NORTH] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:27] [U18-NORTH] ✅ Downloaded schedule: 66 games\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    ✅ Normalized 66 games\n",
      "[2025-10-13 03:34:27] [U18-NORTH] ✅ Games schedule normalized and saved\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    Found 0 completed games\n",
      "[2025-10-13 03:34:27] [U18-NORTH]    ✅ All games already scraped\n",
      "[2025-10-13 03:34:27] [U18-NORTH] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:27] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:27]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:27] [U18-NORTH] ❌ No player stats found\n",
      "[2025-10-13 03:34:27] \n",
      "[2025-10-13 03:34:27] ================================================================================\n",
      "[2025-10-13 03:34:27] PROCESSING LEAGUE: 5 - נוער על דרום\n",
      "[2025-10-13 03:34:27] ================================================================================\n",
      "[2025-10-13 03:34:27] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:27]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] ============================================================\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] STARTING SCRAPE: נוער על דרום\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] Mode: QUICK\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] ============================================================\n",
      "[2025-10-13 03:34:27] [U18-SOUTH] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:29] [U18-SOUTH] Found 136 players\n",
      "[2025-10-13 03:34:29]    ⚠️  No mapping found for: 'הפועל אשכול' in league 5\n",
      "[2025-10-13 03:34:29]    ⚠️  No mapping found for: 'מכבי פ\"ת ארקדי' in league 5\n",
      "[2025-10-13 03:34:29] [U18-SOUTH] ✅ Player details updated\n",
      "[2025-10-13 03:34:29] [U18-SOUTH]    Total: 136 | New: 0 | Updated: 0 | Skipped: 136\n",
      "[2025-10-13 03:34:29] [U18-SOUTH] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] ✅ Downloaded schedule: 66 games\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    ✅ Normalized 66 games\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] ✅ Games schedule normalized and saved\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    Found 0 completed games\n",
      "[2025-10-13 03:34:32] [U18-SOUTH]    ✅ All games already scraped\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:32] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:32]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:32] [U18-SOUTH] ❌ No player stats found\n",
      "[2025-10-13 03:34:32] \n",
      "[2025-10-13 03:34:32] ================================================================================\n",
      "[2025-10-13 03:34:32] PROCESSING LEAGUE: 8 - ליגה לאומית נשים\n",
      "[2025-10-13 03:34:32] ================================================================================\n",
      "[2025-10-13 03:34:32] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:32]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] STARTING SCRAPE: ליגה לאומית נשים\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] Mode: QUICK\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:32] [LEUMIT-WOMEN] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN] Found 98 players\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN] ✅ Player details updated\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN]    Total: 98 | New: 0 | Updated: 0 | Skipped: 98\n",
      "[2025-10-13 03:34:35] [LEUMIT-WOMEN] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] ✅ Downloaded schedule: 105 games\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    ✅ Normalized 105 games\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] ✅ Games schedule normalized and saved\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    Found 0 completed games\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN]    ✅ All games already scraped\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:39] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:39]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:39] [LEUMIT-WOMEN] ❌ No player stats found\n",
      "[2025-10-13 03:34:39] \n",
      "[2025-10-13 03:34:39] ================================================================================\n",
      "[2025-10-13 03:34:39] PROCESSING LEAGUE: 9 - נערות א' על\n",
      "[2025-10-13 03:34:39] ================================================================================\n",
      "[2025-10-13 03:34:39] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:39]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] STARTING SCRAPE: נערות א' על\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] Mode: QUICK\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] ============================================================\n",
      "[2025-10-13 03:34:39] [U18-WOMEN] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:42] [U18-WOMEN] Found 99 players\n",
      "[2025-10-13 03:34:43] [U18-WOMEN] ✅ Player details updated\n",
      "[2025-10-13 03:34:43] [U18-WOMEN]    Total: 99 | New: 0 | Updated: 0 | Skipped: 99\n",
      "[2025-10-13 03:34:43] [U18-WOMEN] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] ✅ Downloaded schedule: 91 games\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    ✅ Normalized 91 games\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] ✅ Games schedule normalized and saved\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    Found 0 completed games\n",
      "[2025-10-13 03:34:46] [U18-WOMEN]    ✅ All games already scraped\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:46] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:46]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:46] [U18-WOMEN] ❌ No player stats found\n",
      "[2025-10-13 03:34:46] \n",
      "[2025-10-13 03:34:46] ================================================================================\n",
      "[2025-10-13 03:34:46] PROCESSING LEAGUE: 11 - ליגת העל לנשים\n",
      "[2025-10-13 03:34:46] ================================================================================\n",
      "[2025-10-13 03:34:46] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:46]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] STARTING SCRAPE: ליגת העל לנשים\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] Mode: QUICK\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:46] [WOMEN-PL] STEP 1: UPDATING PLAYER DETAILS\n",
      "[2025-10-13 03:34:49] [WOMEN-PL] Found 122 players\n",
      "[2025-10-13 03:34:49]    ⚠️  No mapping found for: 'אליצור תל אביב' in league 11\n",
      "[2025-10-13 03:34:49] [WOMEN-PL] ✅ Player details updated\n",
      "[2025-10-13 03:34:49] [WOMEN-PL]    Total: 122 | New: 0 | Updated: 0 | Skipped: 122\n",
      "[2025-10-13 03:34:49] [WOMEN-PL] STEP 2: UPDATING GAME DETAILS\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] ✅ Downloaded schedule: 90 games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    Normalizing schedule teams...\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    ✅ Normalized 90 games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] ✅ Games schedule normalized and saved\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    Found 8 completed games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    Already scraped: 8 games\n",
      "[2025-10-13 03:34:52] [WOMEN-PL]    ✅ All games already scraped\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] STEP 3: CALCULATING AVERAGES\n",
      "[2025-10-13 03:34:52] ✅ Loaded global team mapping: 79 teams, 270 name variations\n",
      "[2025-10-13 03:34:52]    League 1: 16 teams, 49 variations\n",
      "[2025-10-13 03:34:52] [WOMEN-PL] ✅ Player averages: 87 players\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] ✅ Team averages: 10 teams\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] ✅ Opponent averages: 10 teams\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] ✅ SCRAPE COMPLETED SUCCESSFULLY\n",
      "[2025-10-13 03:34:53] [WOMEN-PL] ============================================================\n",
      "[2025-10-13 03:34:53] \n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] UPDATING GLOBAL FILES\n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] Updating global leagues.csv...\n",
      "[2025-10-13 03:34:53] ✅ Global leagues.csv updated: 11 leagues\n",
      "[2025-10-13 03:34:53] Checking global teams.csv...\n",
      "[2025-10-13 03:34:53] ✅ Global teams.csv verified: 81 teams\n",
      "[2025-10-13 03:34:53] Updating global players.csv...\n",
      "[2025-10-13 03:34:53] ✅ Global players.csv updated: 783 players\n",
      "[2025-10-13 03:34:53] \n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] SCRAPING SUMMARY\n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] ✅ Successful: 2 leagues\n",
      "[2025-10-13 03:34:53]    ✓ League 1: ליגה לאומית\n",
      "[2025-10-13 03:34:53]    ✓ League 11: ליגת העל לנשים\n",
      "[2025-10-13 03:34:53] ❌ Failed: 4 leagues\n",
      "[2025-10-13 03:34:53]    ✗ League 4: נוער על צפון\n",
      "[2025-10-13 03:34:53]    ✗ League 5: נוער על דרום\n",
      "[2025-10-13 03:34:53]    ✗ League 8: ליגה לאומית נשים\n",
      "[2025-10-13 03:34:53]    ✗ League 9: נערות א' על\n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] \n",
      "[2025-10-13 03:34:53] ================================================================================\n",
      "[2025-10-13 03:34:53] BASKETBALL SCRAPER FINISHED\n",
      "[2025-10-13 03:34:53] Time: 2025-10-13 03:34:53\n",
      "[2025-10-13 03:34:53] ================================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['main.py']\n",
    "from main import main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd8431-5e94-405a-9e5c-428464ec79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Print a filtered tree: folders + CSV files only, plus a flat CSV list with sizes.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Settings ----------\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5          # עומק מקסימלי לסריקה\n",
    "SHOW_HIDDEN = False    # להציג תיקיות/קבצים שמתחילים בנקודה?\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_hidden(p: Path) -> bool:\n",
    "    return any(part.startswith(\".\") for part in p.parts)\n",
    "\n",
    "def within_depth(root: Path, p: Path, max_depth: int) -> bool:\n",
    "    return len(p.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def dir_children(root: Path, d: Path):\n",
    "    \"\"\"Return (dirs, csvs) immediate children (not recursive), filtered.\"\"\"\n",
    "    dirs, csvs = [], []\n",
    "    for child in sorted(d.iterdir()):\n",
    "        if not SHOW_HIDDEN and is_hidden(child):\n",
    "            continue\n",
    "        if child.is_dir():\n",
    "            dirs.append(child)\n",
    "        elif child.is_file() and child.suffix.lower() == \".csv\":\n",
    "            csvs.append(child)\n",
    "    return dirs, csvs\n",
    "\n",
    "def contains_relevant_descendants(root: Path, d: Path, max_depth: int) -> bool:\n",
    "    \"\"\"Does this directory (recursively) contain any dir/csv within max_depth?\"\"\"\n",
    "    for p in d.rglob(\"*\"):\n",
    "        # stop at max depth\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p):\n",
    "            continue\n",
    "        if p.is_dir() or (p.is_file() and p.suffix.lower() == \".csv\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def print_tree(root: Path, max_depth: int):\n",
    "    print(f\"ROOT: {root}\\n\")\n",
    "    # We print the root name then recurse\n",
    "    print(f\"📁 {root.name}\")\n",
    "    def recurse(d: Path, prefix: str, depth: int):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        dirs, csvs = dir_children(root, d)\n",
    "\n",
    "        # Filter out directories that do not contain any relevant descendants within depth\n",
    "        pruned_dirs = []\n",
    "        for sd in dirs:\n",
    "            if within_depth(root, sd, max_depth) and (contains_relevant_descendants(root, sd, max_depth) or list(dir_children(root, sd)[1])):\n",
    "                pruned_dirs.append(sd)\n",
    "\n",
    "        # Build combined list: dirs first, then CSVs\n",
    "        items = pruned_dirs + csvs\n",
    "        for i, item in enumerate(items):\n",
    "            is_last = (i == len(items) - 1)\n",
    "            branch = \"└─ \" if is_last else \"├─ \"\n",
    "            if item.is_dir():\n",
    "                print(prefix + branch + \"📁 \" + item.name)\n",
    "                new_prefix = prefix + (\"   \" if is_last else \"│  \")\n",
    "                recurse(item, new_prefix, depth + 1)\n",
    "            else:\n",
    "                print(prefix + branch + \"📄 \" + item.name)\n",
    "    recurse(root, \"\", 0)\n",
    "\n",
    "def human_size(num_bytes: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if num_bytes < 1024.0:\n",
    "            return f\"{num_bytes:.0f} {unit}\"\n",
    "        num_bytes /= 1024.0\n",
    "    return f\"{num_bytes:.0f} PB\"\n",
    "\n",
    "def list_csvs(root: Path, max_depth: int):\n",
    "    csvs = []\n",
    "    for p in sorted(root.rglob(\"*.csv\")):\n",
    "        if not within_depth(root, p, max_depth):\n",
    "            continue\n",
    "        if not SHOW_HIDDEN and is_hidden(p.relative_to(root)):\n",
    "            continue\n",
    "        if p.is_file():\n",
    "            try:\n",
    "                size = human_size(p.stat().st_size)\n",
    "            except Exception:\n",
    "                size = \"?\"\n",
    "            csvs.append((p, size))\n",
    "    return csvs\n",
    "\n",
    "# ---------- Run ----------\n",
    "print(\"=== DIRECTORY TREE (folders + CSV only) ===\")\n",
    "print_tree(ROOT, MAX_DEPTH)\n",
    "\n",
    "print(\"\\n=== CSV FILES (flat list) ===\")\n",
    "csv_list = list_csvs(ROOT, MAX_DEPTH)\n",
    "if not csv_list:\n",
    "    print(\"(no CSV files found)\")\n",
    "else:\n",
    "    for p, size in csv_list:\n",
    "        rel = p.relative_to(ROOT)\n",
    "        print(f\"- {rel}  •  {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ef0d3-b0fc-4f69-bf27-3e1404521fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# מציג לכל קובץ CSV את הנתיב, כותרות ושורה אחת ראשונה בפורמט CSV\n",
    "# מדלג על תיקיות מוסתרות ו-.ipynb_checkpoints, עם מפריד ברור בין קבצים\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "def is_hidden_or_checkpoint(path: Path) -> bool:\n",
    "    \"\"\"בודק האם קובץ או תיקייה מוסתרים או בתוך .ipynb_checkpoints\"\"\"\n",
    "    return any(\n",
    "        part.startswith(\".\") or \"ipynb_checkpoints\" in part\n",
    "        for part in path.parts\n",
    "    )\n",
    "\n",
    "def within_depth(root: Path, path: Path, max_depth: int) -> bool:\n",
    "    \"\"\"בודק האם הקובץ בעומק מותר\"\"\"\n",
    "    return len(path.relative_to(root).parts) <= max_depth\n",
    "\n",
    "def print_csv_previews(root: Path, max_depth: int):\n",
    "    csv_files = sorted(root.rglob(\"*.csv\"))\n",
    "    for csv_path in csv_files:\n",
    "        if is_hidden_or_checkpoint(csv_path.relative_to(root)):\n",
    "            continue\n",
    "        if not within_depth(root, csv_path, max_depth):\n",
    "            continue\n",
    "\n",
    "        rel_path = str(csv_path.relative_to(root))\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=\"utf-8-sig\", nrows=1)\n",
    "        except Exception:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding=\"utf-8\", nrows=1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n{rel_path}\")\n",
    "                print(f\"Error reading file: {e}\")\n",
    "                print(\"=\" * 60)\n",
    "                continue\n",
    "\n",
    "        print(f\"\\n{rel_path}\")  # כותרת עם נתיב הקובץ\n",
    "        print(\",\".join(df.columns))  # שורת כותרות\n",
    "        if not df.empty:\n",
    "            print(\",\".join(map(str, df.iloc[0].tolist())))  # שורה אחת מהנתונים\n",
    "        print(\"=\" * 60)  # מפריד בין קבצים\n",
    "\n",
    "    print(\"\\n✅ הסתיים בהצלחה.\")\n",
    "\n",
    "# הפעלה\n",
    "print_csv_previews(ROOT, MAX_DEPTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3b1a8-1da3-4e21-a9f5-5d01beb98c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c709c0-3108-403e-bfbe-598ca510c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Test Live Mapping\n",
    "=================\n",
    "בדיקה עם הקוד המעודכן בדיוק\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "teams_file = \"data/teams.csv\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Loading with COMPOSITE KEY: (variation, league_id)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "print(f\"✓ Loaded {len(df)} rows\\n\")\n",
    "\n",
    "# זיהוי עמודות\n",
    "columns = df.columns.tolist()\n",
    "column_mapping = {\n",
    "    'team_id': ['Team_ID', 'team_id', 'TeamID'],\n",
    "    'league_id': ['League_ID', 'league_id', 'LeagueID'],\n",
    "    'club_name': ['Team_Name', 'club_name', 'team_name', 'name'],\n",
    "    'variations': ['name_variations', 'variations', 'Variations'],\n",
    "}\n",
    "\n",
    "actual_columns = {}\n",
    "for key, possible_names in column_mapping.items():\n",
    "    for name in possible_names:\n",
    "        if name in columns:\n",
    "            actual_columns[key] = name\n",
    "            break\n",
    "\n",
    "print(f\"Detected columns: {actual_columns}\\n\")\n",
    "\n",
    "# יצירת מיפוי עם מפתח מורכב\n",
    "mapping = {}\n",
    "teams_count = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    team_id = row[actual_columns['team_id']] if 'team_id' in actual_columns else None\n",
    "    league_id = row[actual_columns['league_id']] if 'league_id' in actual_columns else None\n",
    "    club_name = row[actual_columns['club_name']] if 'club_name' in actual_columns else None\n",
    "    \n",
    "    if pd.isna(team_id) or pd.isna(league_id) or pd.isna(club_name):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        league_id_int = int(league_id)\n",
    "        if league_id_int == 0:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    teams_count += 1\n",
    "    \n",
    "    variations = row[actual_columns['variations']] if 'variations' in actual_columns else club_name\n",
    "    \n",
    "    club_name = html.unescape(str(club_name)).strip()\n",
    "    variations_str = html.unescape(str(variations)).strip()\n",
    "    \n",
    "    variation_list = []\n",
    "    for v in variations_str.split('|'):\n",
    "        v_clean = html.unescape(v.strip())\n",
    "        if v_clean:\n",
    "            variation_list.append(v_clean)\n",
    "    \n",
    "    team_info = {\n",
    "        'team_id': int(team_id),\n",
    "        'league_id': int(league_id_int),\n",
    "        'club_name': club_name,\n",
    "        'short_name': club_name,\n",
    "        'bg_color': '#000000',\n",
    "        'text_color': '#FFFFFF',\n",
    "        'all_variations': variation_list\n",
    "    }\n",
    "    \n",
    "    # ⭐ מפתח מורכב!\n",
    "    for variation in variation_list:\n",
    "        if variation:\n",
    "            key = (variation, int(league_id_int))\n",
    "            if key not in mapping:\n",
    "                mapping[key] = team_info\n",
    "\n",
    "print(f\"✓ Total teams: {teams_count}\")\n",
    "print(f\"✓ Total mapping entries: {len(mapping)}\\n\")\n",
    "\n",
    "# בדיקת ליגה 1\n",
    "league_1_keys = [k for k in mapping.keys() if k[1] == 1]\n",
    "print(f\"✓ League 1 has {len(league_1_keys)} variations\\n\")\n",
    "\n",
    "if league_1_keys:\n",
    "    print(\"First 10 variations from League 1:\")\n",
    "    for i, key in enumerate(league_1_keys[:10]):\n",
    "        var, lid = key\n",
    "        info = mapping[key]\n",
    "        print(f\"  {i+1}. ('{var}', {lid}) → team_id={info['team_id']}, club_name='{info['club_name']}'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing specific lookups:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    ('אליצור יבנה', 1),\n",
    "    ('מכבי חיפה גיא נתן', 1),\n",
    "    ('מכבי אשדוד', 1),\n",
    "    ('הפועל חיפה', 1),\n",
    "    ('מכבי רחובות', 1),\n",
    "]\n",
    "\n",
    "for team_name, league_id in test_cases:\n",
    "    key = (team_name, league_id)\n",
    "    if key in mapping:\n",
    "        info = mapping[key]\n",
    "        print(f\"✓ ({team_name!r}, {league_id}) → team_id={info['team_id']}\")\n",
    "    else:\n",
    "        print(f\"✗ ({team_name!r}, {league_id}) NOT FOUND\")\n",
    "        \n",
    "        # חיפוש באיזו ליגה זה כן נמצא\n",
    "        found_in = []\n",
    "        for k in mapping.keys():\n",
    "            if k[0] == team_name:\n",
    "                found_in.append(k[1])\n",
    "        if found_in:\n",
    "            print(f\"  Found in leagues: {found_in}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92043d-21a5-48bf-906f-95a1b5cf04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clear Python Cache - Jupyter Notebook\n",
    "======================================\n",
    "מחיקת כל קבצי ה-cache של Python\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_python_cache(root_dir='.'):\n",
    "    \"\"\"\n",
    "    מחיקת כל קבצי ה-cache של Python\n",
    "    \n",
    "    Args:\n",
    "        root_dir: תיקייה להתחיל ממנה (ברירת מחדל: נוכחית)\n",
    "    \"\"\"\n",
    "    deleted_count = 0\n",
    "    \n",
    "    # מחיקת תיקיות __pycache__\n",
    "    print(\"🔍 מחפש תיקיות __pycache__...\")\n",
    "    for pycache_dir in Path(root_dir).rglob('__pycache__'):\n",
    "        try:\n",
    "            shutil.rmtree(pycache_dir)\n",
    "            print(f\"   ✅ נמחק: {pycache_dir}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ שגיאה במחיקת {pycache_dir}: {e}\")\n",
    "    \n",
    "    # מחיקת קבצי .pyc\n",
    "    print(\"\\n🔍 מחפש קבצי .pyc...\")\n",
    "    pyc_count = 0\n",
    "    for pyc_file in Path(root_dir).rglob('*.pyc'):\n",
    "        try:\n",
    "            pyc_file.unlink()\n",
    "            pyc_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ שגיאה במחיקת {pyc_file}: {e}\")\n",
    "    \n",
    "    if pyc_count > 0:\n",
    "        print(f\"   ✅ נמחקו {pyc_count} קבצי .pyc\")\n",
    "    \n",
    "    # מחיקת קבצי .pyo\n",
    "    print(\"\\n🔍 מחפש קבצי .pyo...\")\n",
    "    pyo_count = 0\n",
    "    for pyo_file in Path(root_dir).rglob('*.pyo'):\n",
    "        try:\n",
    "            pyo_file.unlink()\n",
    "            pyo_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ שגיאה במחיקת {pyo_file}: {e}\")\n",
    "    \n",
    "    if pyo_count > 0:\n",
    "        print(f\"   ✅ נמחקו {pyo_count} קבצי .pyo\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"✅ סיים! נמחקו {deleted_count} תיקיות __pycache__\")\n",
    "    print(f\"✅ נמחקו {pyc_count + pyo_count} קבצים מקומפלים\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# הרצה\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"מחיקת Python Cache\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "clear_python_cache()\n",
    "\n",
    "print(\"\\n💡 כעת הרץ את הקוד מחדש:\")\n",
    "print(\"   !python main.py --league 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e8f3a-59de-4b0c-aef4-f0a5f77884be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8416ff-9a70-4069-b04d-7d6e19735db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56f7f97-6cb2-480f-8651-8441c12eef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--league LEAGUE] [--all] [--test]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\navon\\AppData\\Roaming\\jupyter\\runtime\\kernel-d5a1eede-d991-49d8-9026-3edb8d691127.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Migration Script - CSV to Supabase\n",
    "===================================\n",
    "מעביר את כל הנתונים הקיימים מ-CSV ל-Supabase\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from supabase_uploader import SupabaseUploader\n",
    "from config import LEAGUES\n",
    "from utils import log_message\n",
    "\n",
    "class DataMigration:\n",
    "    \"\"\"מחלקה להעברת נתונים\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"אתחול\"\"\"\n",
    "        self.uploader = SupabaseUploader()\n",
    "        self.stats = {\n",
    "            'leagues': 0,\n",
    "            'teams': 0,\n",
    "            'players': 0,\n",
    "            'history': 0,\n",
    "            'games': 0,\n",
    "            'quarters': 0,\n",
    "            'player_stats': 0,\n",
    "            'team_stats': 0,\n",
    "            'player_averages': 0,\n",
    "            'team_averages': 0,\n",
    "            'opponent_averages': 0\n",
    "        }\n",
    "    \n",
    "    def migrate_all(self, league_ids=None):\n",
    "        \"\"\"\n",
    "        העברת כל הנתונים\n",
    "        \n",
    "        Args:\n",
    "            league_ids: רשימת league_ids להעברה (None = כולם)\n",
    "        \"\"\"\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"🚀 STARTING MIGRATION TO SUPABASE\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        # Test connection\n",
    "        if not self.uploader.test_connection():\n",
    "            log_message(\"❌ Cannot connect to Supabase!\")\n",
    "            return False\n",
    "        \n",
    "        # Get leagues to migrate\n",
    "        if league_ids is None:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if cfg.get('active', False)}\n",
    "        else:\n",
    "            leagues_to_migrate = {lid: cfg for lid, cfg in LEAGUES.items() if lid in league_ids}\n",
    "        \n",
    "        log_message(f\"📊 Migrating {len(leagues_to_migrate)} leagues\")\n",
    "        log_message(\"\")\n",
    "        \n",
    "        # Migrate global data first\n",
    "        self._migrate_global_leagues()\n",
    "        self._migrate_global_teams()\n",
    "        self._migrate_global_players()\n",
    "        \n",
    "        # Migrate each league\n",
    "        for league_id, config in leagues_to_migrate.items():\n",
    "            log_message(\"=\"*60)\n",
    "            log_message(f\"[{config['code'].upper()}] Migrating: {config['name']}\")\n",
    "            log_message(\"=\"*60)\n",
    "            \n",
    "            self._migrate_league_data(league_id, config)\n",
    "        \n",
    "        # Print stats\n",
    "        self._print_stats()\n",
    "        \n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"✅ MIGRATION COMPLETED!\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _migrate_global_leagues(self):\n",
    "        \"\"\"העברת טבלת leagues\"\"\"\n",
    "        log_message(\"📋 Migrating leagues...\")\n",
    "        \n",
    "        leagues_file = 'data/leagues.csv'\n",
    "        if not os.path.exists(leagues_file):\n",
    "            log_message(\"⚠️  leagues.csv not found, creating from config\")\n",
    "            # Create from config\n",
    "            for league_id, config in LEAGUES.items():\n",
    "                if config.get('active', False):\n",
    "                    league_data = {\n",
    "                        'league_id': int(league_id),\n",
    "                        'name': config['name'],\n",
    "                        'name_en': config.get('name_en', ''),\n",
    "                        'country': config.get('country', 'Israel'),\n",
    "                        'season': config.get('season', ''),\n",
    "                        'url': config['url'],\n",
    "                        'is_active': True\n",
    "                    }\n",
    "                    self.uploader.upsert_league(league_data)\n",
    "                    self.stats['leagues'] += 1\n",
    "        else:\n",
    "            df = pd.read_csv(leagues_file, encoding='utf-8-sig')\n",
    "            for _, row in df.iterrows():\n",
    "                league_data = {\n",
    "                    'league_id': int(row['league_id']),\n",
    "                    'name': row['name'],\n",
    "                    'name_en': row.get('name_en', ''),\n",
    "                    'country': row.get('country', 'Israel'),\n",
    "                    'season': row.get('season', ''),\n",
    "                    'url': row.get('url', ''),\n",
    "                    'is_active': bool(row.get('is_active', False))\n",
    "                }\n",
    "                self.uploader.upsert_league(league_data)\n",
    "                self.stats['leagues'] += 1\n",
    "        \n",
    "        log_message(f\"✅ Migrated {self.stats['leagues']} leagues\")\n",
    "    \n",
    "    def _migrate_global_teams(self):\n",
    "        \"\"\"העברת טבלת teams\"\"\"\n",
    "        log_message(\"🏀 Migrating teams...\")\n",
    "        \n",
    "        teams_file = 'data/teams.csv'\n",
    "        if not os.path.exists(teams_file):\n",
    "            log_message(\"⚠️  teams.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(teams_file, encoding='utf-8-sig')\n",
    "        teams_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Skip invalid teams\n",
    "            if pd.isna(row['Team_ID']) or pd.isna(row['League_ID']):\n",
    "                continue\n",
    "            if int(row['League_ID']) == 0:\n",
    "                continue\n",
    "            \n",
    "            teams_data.append({\n",
    "                'team_id': int(row['Team_ID']),\n",
    "                'league_id': int(row['League_ID']),\n",
    "                'team_name': row['Team_Name'],\n",
    "                'short_name': row.get('short_name', row['Team_Name']),\n",
    "                'bg_color': row.get('bg_color', '#000000'),\n",
    "                'text_color': row.get('text_color', '#FFFFFF'),\n",
    "                'name_variations': row.get('name_variations', '')\n",
    "            })\n",
    "        \n",
    "        if teams_data:\n",
    "            self.uploader.upsert_teams(teams_data)\n",
    "            self.stats['teams'] = len(teams_data)\n",
    "        \n",
    "        log_message(f\"✅ Migrated {self.stats['teams']} teams\")\n",
    "    \n",
    "    def _migrate_global_players(self):\n",
    "        \"\"\"העברת טבלת players\"\"\"\n",
    "        log_message(\"👤 Migrating players...\")\n",
    "        \n",
    "        players_file = 'data/players.csv'\n",
    "        if not os.path.exists(players_file):\n",
    "            log_message(\"⚠️  players.csv not found, skipping\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(players_file, encoding='utf-8-sig')\n",
    "        players_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            if pd.isna(row['player_id']):\n",
    "                continue\n",
    "            \n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['name'],\n",
    "                'current_team_id': int(row['current_team_id']) if pd.notna(row['current_team_id']) else None,\n",
    "                'league_id': int(row['league_id']),\n",
    "                'date_of_birth': row['date_of_birth'] if pd.notna(row['date_of_birth']) else None,\n",
    "                'height': float(row['height']) if pd.notna(row['height']) else None,\n",
    "                'jersey_number': int(row['jersey_number']) if pd.notna(row['jersey_number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            self.stats['players'] = len(players_data)\n",
    "        \n",
    "        log_message(f\"✅ Migrated {self.stats['players']} players\")\n",
    "    \n",
    "    def _migrate_league_data(self, league_id, config):\n",
    "        \"\"\"העברת נתוני ליגה ספציפית\"\"\"\n",
    "        league_code = config['code']\n",
    "        data_folder = config['data_folder']\n",
    "        games_folder = config['games_folder']\n",
    "        \n",
    "        # Player details (if not in global file)\n",
    "        self._migrate_player_details(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Player history\n",
    "        self._migrate_player_history(league_id, league_code, data_folder)\n",
    "        \n",
    "        # Games\n",
    "        self._migrate_games(league_id, games_folder)\n",
    "        \n",
    "        # Game quarters\n",
    "        self._migrate_game_quarters(league_id, games_folder)\n",
    "        \n",
    "        # Game player stats\n",
    "        self._migrate_game_player_stats(league_id, games_folder)\n",
    "        \n",
    "        # Game team stats\n",
    "        self._migrate_game_team_stats(league_id, games_folder)\n",
    "        \n",
    "        # Averages\n",
    "        self._migrate_player_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_team_averages(league_id, league_code, data_folder)\n",
    "        self._migrate_opponent_averages(league_id, league_code, data_folder)\n",
    "    \n",
    "    def _migrate_player_details(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת פרטי שחקנים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_details.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📝 Player details...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        players_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            players_data.append({\n",
    "                'player_id': str(row['player_id']),\n",
    "                'name': row['Name'],\n",
    "                'current_team_id': int(row['team_id']) if pd.notna(row['team_id']) else None,\n",
    "                'league_id': int(league_id),\n",
    "                'date_of_birth': row['Date Of Birth'] if pd.notna(row['Date Of Birth']) else None,\n",
    "                'height': float(row['Height']) if pd.notna(row['Height']) else None,\n",
    "                'jersey_number': int(row['Number']) if pd.notna(row['Number']) else None\n",
    "            })\n",
    "        \n",
    "        if players_data:\n",
    "            self.uploader.upsert_players(players_data)\n",
    "            log_message(f\"  ✅ {len(players_data)} players\")\n",
    "    \n",
    "    def _migrate_player_history(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת היסטוריית שחקנים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_history.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📚 Player history...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        history_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            player_id = str(row['player_id'])\n",
    "            \n",
    "            # Extract seasons from columns\n",
    "            for col in df.columns:\n",
    "                if col not in ['player_id', 'Name', 'Current Team', 'team_id', 'league_id',\n",
    "                               'Date Of Birth', 'Height', 'Number']:\n",
    "                    season = col\n",
    "                    if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                        team_league = str(row[col])\n",
    "                        team_name = team_league.split('(')[0].strip() if '(' in team_league else team_league\n",
    "                        league_name = team_league.split('(')[1].replace(')', '').strip() if '(' in team_league else ''\n",
    "                        \n",
    "                        history_data.append({\n",
    "                            'player_id': player_id,\n",
    "                            'season': season,\n",
    "                            'team_name': team_name,\n",
    "                            'league_name': league_name,\n",
    "                            'league_id': int(league_id)\n",
    "                        })\n",
    "        \n",
    "        if history_data:\n",
    "            self.uploader.upsert_player_history(history_data)\n",
    "            self.stats['history'] += len(history_data)\n",
    "            log_message(f\"  ✅ {len(history_data)} history records\")\n",
    "    \n",
    "    def _migrate_games(self, league_id, games_folder):\n",
    "        \"\"\"העברת משחקים\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'games_schedule.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  🏆 Games...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        games_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            games_data.append({\n",
    "                'game_id': str(row['gameid']),\n",
    "                'league_id': int(league_id),\n",
    "                'code': str(row['Code']) if pd.notna(row.get('Code')) else None,\n",
    "                'week_day': row.get('Week Day', ''),\n",
    "                'date': row['Date'] if pd.notna(row['Date']) else None,\n",
    "                'round': str(row['Round']) if pd.notna(row.get('Round')) else None,\n",
    "                'time': row.get('Time', ''),\n",
    "                'home_team': row['Home Team'],\n",
    "                'home_team_code': row.get('Home Team Code', ''),\n",
    "                'home_team_id': int(row['home_team_id']) if pd.notna(row.get('home_team_id')) else None,\n",
    "                'away_team': row['Away Team'],\n",
    "                'away_team_code': row.get('Away Team Code', ''),\n",
    "                'away_team_id': int(row['away_team_id']) if pd.notna(row.get('away_team_id')) else None,\n",
    "                'venue': row.get('Venue', ''),\n",
    "                'home_score': int(row['Home Score']) if pd.notna(row.get('Home Score')) and str(row['Home Score']).replace('.','').isdigit() else None,\n",
    "                'away_score': int(row['Away Score']) if pd.notna(row.get('Away Score')) and str(row['Away Score']).replace('.','').isdigit() else None,\n",
    "                'arena': row.get('Arena', ''),\n",
    "                'status': 'completed' if pd.notna(row.get('Home Score')) and str(row.get('Home Score')).strip() != '' else 'scheduled'\n",
    "            })\n",
    "        \n",
    "        if games_data:\n",
    "            self.uploader.upsert_games(games_data)\n",
    "            self.stats['games'] += len(games_data)\n",
    "            log_message(f\"  ✅ {len(games_data)} games\")\n",
    "    \n",
    "    def _migrate_game_quarters(self, league_id, games_folder):\n",
    "        \"\"\"העברת רבעי משחק\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_quarters.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  🔢 Quarters...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        quarters_data = df.to_dict('records')\n",
    "        \n",
    "        if quarters_data:\n",
    "            self.uploader.upsert_game_quarters(quarters_data)\n",
    "            self.stats['quarters'] += len(quarters_data)\n",
    "            log_message(f\"  ✅ {len(quarters_data)} quarters\")\n",
    "    \n",
    "    def _migrate_game_player_stats(self, league_id, games_folder):\n",
    "        \"\"\"העברת סטטיסטיקות שחקן במשחק\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_player_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📊 Player stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_player_stats(stats_data)\n",
    "            self.stats['player_stats'] += len(stats_data)\n",
    "            log_message(f\"  ✅ {len(stats_data)} player stats\")\n",
    "    \n",
    "    def _migrate_game_team_stats(self, league_id, games_folder):\n",
    "        \"\"\"העברת סטטיסטיקות קבוצה במשחק\"\"\"\n",
    "        file_path = os.path.join(games_folder, 'game_team_stats.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📈 Team stats...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        stats_data = df.to_dict('records')\n",
    "        \n",
    "        if stats_data:\n",
    "            self.uploader.upsert_game_team_stats(stats_data)\n",
    "            self.stats['team_stats'] += len(stats_data)\n",
    "            log_message(f\"  ✅ {len(stats_data)} team stats\")\n",
    "    \n",
    "    def _migrate_player_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת ממוצעי שחקנים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_player_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📉 Player averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_player_averages(avg_data)\n",
    "            self.stats['player_averages'] += len(avg_data)\n",
    "            log_message(f\"  ✅ {len(avg_data)} player averages\")\n",
    "    \n",
    "    def _migrate_team_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת ממוצעי קבוצות\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_team_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  📊 Team averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_team_averages(avg_data)\n",
    "            self.stats['team_averages'] += len(avg_data)\n",
    "            log_message(f\"  ✅ {len(avg_data)} team averages\")\n",
    "    \n",
    "    def _migrate_opponent_averages(self, league_id, league_code, data_folder):\n",
    "        \"\"\"העברת ממוצעי יריבים\"\"\"\n",
    "        file_path = os.path.join(data_folder, f\"{league_code}_opponent_averages.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return\n",
    "        \n",
    "        log_message(f\"  🛡️ Opponent averages...\")\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        avg_data = df.to_dict('records')\n",
    "        \n",
    "        if avg_data:\n",
    "            self.uploader.upsert_opponent_averages(avg_data)\n",
    "            self.stats['opponent_averages'] += len(avg_data)\n",
    "            log_message(f\"  ✅ {len(avg_data)} opponent averages\")\n",
    "    \n",
    "    def _print_stats(self):\n",
    "        \"\"\"הדפסת סטטיסטיקות\"\"\"\n",
    "        log_message(\"\")\n",
    "        log_message(\"=\"*60)\n",
    "        log_message(\"📊 MIGRATION STATISTICS\")\n",
    "        log_message(\"=\"*60)\n",
    "        \n",
    "        for key, value in self.stats.items():\n",
    "            if value > 0:\n",
    "                log_message(f\"  {key.replace('_', ' ').title()}: {value:,}\")\n",
    "        \n",
    "        total = sum(self.stats.values())\n",
    "        log_message(\"\")\n",
    "        log_message(f\"  TOTAL RECORDS: {total:,}\")\n",
    "        log_message(\"=\"*60)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Migrate CSV data to Supabase')\n",
    "    parser.add_argument('--league', type=str, help='Specific league ID to migrate')\n",
    "    parser.add_argument('--all', action='store_true', help='Migrate all active leagues')\n",
    "    parser.add_argument('--test', action='store_true', help='Test connection only')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    migrator = DataMigration()\n",
    "    \n",
    "    if args.test:\n",
    "        # Test connection\n",
    "        log_message(\"🧪 Testing Supabase connection...\")\n",
    "        if migrator.uploader.test_connection():\n",
    "            log_message(\"✅ Connection successful!\")\n",
    "        else:\n",
    "            log_message(\"❌ Connection failed!\")\n",
    "    \n",
    "    elif args.all or args.league:\n",
    "        # Migrate data\n",
    "        league_ids = [args.league] if args.league else None\n",
    "        migrator.migrate_all(league_ids)\n",
    "    \n",
    "    else:\n",
    "        # Show help\n",
    "        parser.print_help()\n",
    "        print(\"\\nExamples:\")\n",
    "        print(\"  python migrate_to_supabase.py --test              # Test connection\")\n",
    "        print(\"  python migrate_to_supabase.py --all               # Migrate all leagues\")\n",
    "        print(\"  python migrate_to_supabase.py --league 1          # Migrate league 1 only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe7cb82-7e89-4803-96bb-f10f18190e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ URL: https://whaogopjlqzaricskrrq.supabase.co\n",
      "✅ Key מתחיל ב: eyJhbGciOiJIUzI1NiIs...\n",
      "✅ Connected!\n",
      "✅ יש 0 ליגות\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import ספציפי\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Cell 2: טען credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = os.getenv('SUPABASE_URL')\n",
    "key = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "# Cell 3: בדוק שהם קיימים\n",
    "if not url or not key:\n",
    "    print(\"❌ חסר URL או KEY!\")\n",
    "else:\n",
    "    print(f\"✅ URL: {url}\")\n",
    "    print(f\"✅ Key מתחיל ב: {key[:20]}...\")\n",
    "\n",
    "# Cell 4: חיבור עם try-except\n",
    "try:\n",
    "    supabase: Client = create_client(url, key)\n",
    "    print(\"✅ Connected!\")\n",
    "    \n",
    "    # בדיקה\n",
    "    result = supabase.table('leagues').select(\"count\", count=\"exact\").execute()\n",
    "    print(f\"✅ יש {result.count} ליגות\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
